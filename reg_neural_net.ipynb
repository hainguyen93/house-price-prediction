{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reg_neural_net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEnBtiXhWhWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "86a329d1-82cd-461f-d728-215c2595410d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.models import Model, Sequential\n",
        "from keras.metrics import mean_absolute_error, mae, mse\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.regularizers import l2, l1_l2\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRxmx9xVW0Rg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "83482af5-5385-47be-92f8-e1558c6e9aa6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-nntaLcW2EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(url, columns=[1, 2, 4, 6, 11]):\n",
        "  \"\"\" Load the dataset, change the column names, \n",
        "      and replace the categorical data by numeric values\n",
        "  \"\"\"\n",
        "  # load data\n",
        "  df = pd.read_csv(url, header=None, usecols=columns)\n",
        "  print('Original Data shape: ', np.shape(df))\n",
        "\n",
        "  # re-name all columns\n",
        "  column_names = ['Price', 'PurchaseDate', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  df.columns = column_names\n",
        "  \n",
        "  # resplace column values\n",
        "  df['PropertyType'] = df['PropertyType'].replace({'F':0, 'D':1, 'S':2, 'T':3, 'O':4})\n",
        "  df['LeaseDuration'] = df['LeaseDuration'].replace({'L':0, 'F':1, 'U':2})\n",
        "  df.loc[df['City']=='LONDON', 'City'] = 0\n",
        "  df.loc[df['City'] != 0, 'City'] = 1\n",
        "\n",
        "  # convert column values to appropriate dtype (to save memory)\n",
        "  df['PurchaseDate'] = pd.to_datetime(df['PurchaseDate'])\n",
        "  df['Price'] = pd.to_numeric(df[\"Price\"], downcast=\"integer\")\n",
        "  df['PropertyType'] = pd.to_numeric(df['PropertyType'], downcast='integer')\n",
        "  df['LeaseDuration'] = pd.to_numeric(df[\"LeaseDuration\"], downcast=\"integer\")\n",
        "  df['City'] = pd.to_numeric(df[\"City\"], downcast=\"integer\")\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C_FNi6BW4IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(df):\n",
        "  \"\"\" Split the data into training and test dataset \n",
        "  \"\"\"\n",
        "  # purchases prior to 1/1/2016 as training \n",
        "  cutoff = datetime(2016, 1, 1)\n",
        "  column_sels = ['Price', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  train_df = df.loc[df['PurchaseDate'] <= cutoff][column_sels]\n",
        "  test_df = df.loc[df['PurchaseDate'] > cutoff][column_sels] \n",
        "  \n",
        "  # remove duplicates\n",
        "  train_df.drop_duplicates(keep='first', inplace=True)\n",
        "  test_df.drop_duplicates(keep='first', inplace=True)\n",
        "  print(\"Train (and val) shape: \", train_df.shape)\n",
        "  print(\"Test shape: \", test_df.shape)  \n",
        "  return train_df, test_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNm3YmcrW6Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_val(train_df, perc=0.2):\n",
        "  \"\"\" Split training into train and validation (perc %) dataset\n",
        "  \"\"\" \n",
        "  train_df, val_df = train_test_split(train_df, test_size=perc, random_state=2019)\n",
        "  print(\"Partial Train shape : \", train_df.shape)\n",
        "  print(\"Val shape : \", val_df.shape)  \n",
        "  return train_df, val_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DMSV3QyW7_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(train_df, val_df, test_df):\n",
        "  \"\"\" Prepare inputs/targets pair for training, val, and testing\n",
        "      using one-hot encoding (for categorical data), and \n",
        "      down-scale the target values (prices) \n",
        "  \"\"\"\n",
        "  # training \n",
        "  train_X = train_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  train_y = train_df['Price']\n",
        "\n",
        "  # validation\n",
        "  val_X = val_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  val_y = val_df['Price']\n",
        "\n",
        "  # testing \n",
        "  test_X = test_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  test_y = test_df['Price']\n",
        "\n",
        "  # one-hot encoding the inputs\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(train_X)\n",
        "  train_X = ohc.transform(train_X)\n",
        "  val_X = ohc.transform(val_X)\n",
        "  test_X = ohc.transform(test_X)\n",
        "\n",
        "  # convert the targets to smaller range\n",
        "  train_y = np.log1p(train_y * 1e-3)\n",
        "  val_y = np.log1p(val_y * 1e-3)\n",
        "  test_y = np.log1p(test_y * 1e-3)\n",
        "  return (train_X, train_y), (val_X, val_y), (test_X, test_y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNEV_u4OW-MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_size):\n",
        "  \"\"\" Build and compile a fully-connected neural network \n",
        "  \"\"\"\n",
        "  inp = Input(shape=(input_size,))\n",
        "  fc1 = Dense(100, activation='relu', kernel_regularizer=l2(0.001))(inp)\n",
        "  do1 = Dropout(0.5)(fc1)\n",
        "  fc2 = Dense(200, activation='relu', kernel_regularizer=l2(0.001))(do1)\n",
        "  do2 = Dropout(0.5)(fc2)\n",
        "  fc3 = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(do2)\n",
        "  do3 = Dropout(0.5)(fc3)\n",
        "  fc4 = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(do3)\n",
        "  out = Dense(1)(fc4)\n",
        "  model = Model(inputs=inp, outputs=out)\n",
        "  print(model.summary())\n",
        "  model.compile(optimizer=RMSprop(lr=1e-3), loss=mse, metrics=[mae])  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnKG-_ySXAMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, epochs=10, batch_size=10000):\n",
        "  \"\"\" Train a compiled model\n",
        "  \"\"\"\n",
        "  history = model.fit(train_X, train_y, batch_size=batch_size, verbose=1,\n",
        "                      epochs=epochs, validation_data=(val_X, val_y))  \n",
        "  train_mae = history.history['mean_absolute_error']\n",
        "  val_mae = history.history['val_mean_absolute_error']  \n",
        "  return train_mae, val_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dlkwMkvXB6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_with_cv(train_df, val_df, epochs=500, batch_size=512, kfold=10):\n",
        "  \"\"\" Train with k-fold cross-validation\n",
        "  \"\"\"\n",
        "  num_samples = np.shape(train_df)[0] // kfold\n",
        "  train_errs = []\n",
        "  val_errs = []\n",
        "\n",
        "  for i in range(kfold):\n",
        "    print('Processing fold {0}'.format(i))\n",
        "\n",
        "    # prepare train and val data\n",
        "    val_data = train_df.iloc[i*num_samples:(i+1)*num_samples]\n",
        "    train_data = pd.concat([train_df.iloc[:i*num_samples], train_df.iloc[(i+1)*num_samples:]])\n",
        "    (train_X, train_y), (val_X, val_y), (test_X, test_y) = prep(train_data, val_data, test_df)\n",
        "\n",
        "    # build a new model\n",
        "    model = build_model(np.shape(train_X)[1])\n",
        "    train_mae, val_mae = train_model(model, epochs, batch_size)\n",
        "\n",
        "    # append train/val errs to error lists\n",
        "    train_errs.append(train_mae)\n",
        "    val_errs.append(val_mae)\n",
        "\n",
        "  return train_errs, val_errs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ8hOjt-XDk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_val_error(train_errs, val_errs):\n",
        "  \"\"\" Plot the train/val loss over epochs\n",
        "  \"\"\"\n",
        "  epochs = len(train_errs)\n",
        "  plt.plot(range(1, epochs+1), train_errs, label='Training Loss')\n",
        "  plt.plot(range(1, epochs+1), val_errs, label='Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOuj27tDlADu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# smooth the val curve in the above plot\n",
        "def smooth_curve(points, factor=0.9):\n",
        "  \"\"\" Smooth with an exponential moving average\n",
        "  \"\"\"\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous*factor + point*(1-factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4_fmMipXFTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4865cfcb-4f4a-44b3-9f9b-afd4c0fb0dd7"
      },
      "source": [
        "# load the data\n",
        "url = '/content/drive/My Drive/pp-complete.csv'\n",
        "df = load_data(url)\n",
        "train_df, test_df = split_train_test(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Data shape:  (24852949, 5)\n",
            "Train (and val) shape:  (317952, 4)\n",
            "Test shape:  (144006, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRcCfIDaXHJs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "028810bd-92f8-4912-9fec-25926de90db0"
      },
      "source": [
        "train_df, val_df = split_train_val(train_df)\n",
        "(train_X, train_y), (val_X, val_y), (test_X, test_y)  = prep(train_df, val_df, test_df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Partial Train shape :  (254361, 4)\n",
            "Val shape :  (63591, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdYPsv1vXIza",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d2ccfa10-a15f-411f-8464-29eeec61002c"
      },
      "source": [
        "# fitting the model using cross validation with k folds\n",
        "train_errs, val_errs = train_model_with_cv(train_df, val_df, epochs=50)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing fold 0\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 4s 17us/step - loss: 3.4357 - mean_absolute_error: 1.0607 - val_loss: 2.8599 - val_mean_absolute_error: 1.1276\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.8070 - mean_absolute_error: 0.8542 - val_loss: 1.3551 - val_mean_absolute_error: 0.7869\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.3509 - mean_absolute_error: 0.8185 - val_loss: 1.2294 - val_mean_absolute_error: 0.7894\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.2346 - mean_absolute_error: 0.8084 - val_loss: 1.1475 - val_mean_absolute_error: 0.7840\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1880 - mean_absolute_error: 0.8014 - val_loss: 1.1377 - val_mean_absolute_error: 0.7843\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1822 - mean_absolute_error: 0.8008 - val_loss: 1.1473 - val_mean_absolute_error: 0.7894\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1781 - mean_absolute_error: 0.8002 - val_loss: 1.1372 - val_mean_absolute_error: 0.7851\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1770 - mean_absolute_error: 0.8006 - val_loss: 1.1323 - val_mean_absolute_error: 0.7845\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1761 - mean_absolute_error: 0.8003 - val_loss: 1.1456 - val_mean_absolute_error: 0.7892\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1762 - mean_absolute_error: 0.8007 - val_loss: 1.1344 - val_mean_absolute_error: 0.7846\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1726 - mean_absolute_error: 0.8003 - val_loss: 1.1335 - val_mean_absolute_error: 0.7874\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1731 - mean_absolute_error: 0.8003 - val_loss: 1.1449 - val_mean_absolute_error: 0.7874\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1714 - mean_absolute_error: 0.8000 - val_loss: 1.1439 - val_mean_absolute_error: 0.7886\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1705 - mean_absolute_error: 0.7997 - val_loss: 1.1358 - val_mean_absolute_error: 0.7862\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1693 - mean_absolute_error: 0.7996 - val_loss: 1.1343 - val_mean_absolute_error: 0.7857\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1700 - mean_absolute_error: 0.8000 - val_loss: 1.1324 - val_mean_absolute_error: 0.7855\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1703 - mean_absolute_error: 0.8000 - val_loss: 1.1343 - val_mean_absolute_error: 0.7867\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1685 - mean_absolute_error: 0.7995 - val_loss: 1.1374 - val_mean_absolute_error: 0.7856\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1699 - mean_absolute_error: 0.8001 - val_loss: 1.1294 - val_mean_absolute_error: 0.7878\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1684 - mean_absolute_error: 0.7996 - val_loss: 1.1266 - val_mean_absolute_error: 0.7855\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1672 - mean_absolute_error: 0.7997 - val_loss: 1.1286 - val_mean_absolute_error: 0.7860\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1671 - mean_absolute_error: 0.7994 - val_loss: 1.1290 - val_mean_absolute_error: 0.7854\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1667 - mean_absolute_error: 0.7997 - val_loss: 1.1354 - val_mean_absolute_error: 0.7885\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1651 - mean_absolute_error: 0.7995 - val_loss: 1.1451 - val_mean_absolute_error: 0.7927\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1682 - mean_absolute_error: 0.7997 - val_loss: 1.1331 - val_mean_absolute_error: 0.7876\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7992 - val_loss: 1.1330 - val_mean_absolute_error: 0.7872\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1660 - mean_absolute_error: 0.7994 - val_loss: 1.1295 - val_mean_absolute_error: 0.7870\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1648 - mean_absolute_error: 0.7990 - val_loss: 1.1336 - val_mean_absolute_error: 0.7880\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1647 - mean_absolute_error: 0.7991 - val_loss: 1.1268 - val_mean_absolute_error: 0.7854\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1650 - mean_absolute_error: 0.7994 - val_loss: 1.1375 - val_mean_absolute_error: 0.7867\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1646 - mean_absolute_error: 0.7995 - val_loss: 1.1363 - val_mean_absolute_error: 0.7900\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1641 - mean_absolute_error: 0.7989 - val_loss: 1.1317 - val_mean_absolute_error: 0.7860\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1631 - mean_absolute_error: 0.7989 - val_loss: 1.1351 - val_mean_absolute_error: 0.7897\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1631 - mean_absolute_error: 0.7995 - val_loss: 1.1339 - val_mean_absolute_error: 0.7857\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1647 - mean_absolute_error: 0.7994 - val_loss: 1.1282 - val_mean_absolute_error: 0.7853\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1642 - mean_absolute_error: 0.7994 - val_loss: 1.1285 - val_mean_absolute_error: 0.7900\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1631 - mean_absolute_error: 0.7992 - val_loss: 1.1706 - val_mean_absolute_error: 0.7981\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1618 - mean_absolute_error: 0.7988 - val_loss: 1.1290 - val_mean_absolute_error: 0.7861\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1622 - mean_absolute_error: 0.7987 - val_loss: 1.1283 - val_mean_absolute_error: 0.7849\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1612 - mean_absolute_error: 0.7986 - val_loss: 1.1281 - val_mean_absolute_error: 0.7859\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1614 - mean_absolute_error: 0.7987 - val_loss: 1.1252 - val_mean_absolute_error: 0.7856\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1613 - mean_absolute_error: 0.7988 - val_loss: 1.1265 - val_mean_absolute_error: 0.7853\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1620 - mean_absolute_error: 0.7989 - val_loss: 1.1236 - val_mean_absolute_error: 0.7860\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1618 - mean_absolute_error: 0.7988 - val_loss: 1.1362 - val_mean_absolute_error: 0.7876\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1610 - mean_absolute_error: 0.7987 - val_loss: 1.1242 - val_mean_absolute_error: 0.7870\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1619 - mean_absolute_error: 0.7988 - val_loss: 1.1292 - val_mean_absolute_error: 0.7868\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1610 - mean_absolute_error: 0.7989 - val_loss: 1.1242 - val_mean_absolute_error: 0.7857\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1612 - mean_absolute_error: 0.7990 - val_loss: 1.1272 - val_mean_absolute_error: 0.7864\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1609 - mean_absolute_error: 0.7991 - val_loss: 1.1284 - val_mean_absolute_error: 0.7867\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1594 - mean_absolute_error: 0.7984 - val_loss: 1.1293 - val_mean_absolute_error: 0.7862\n",
            "Processing fold 1\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 4s 17us/step - loss: 3.4224 - mean_absolute_error: 1.0467 - val_loss: 2.7733 - val_mean_absolute_error: 1.0883\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.8155 - mean_absolute_error: 0.8501 - val_loss: 1.3646 - val_mean_absolute_error: 0.7878\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3476 - mean_absolute_error: 0.8176 - val_loss: 1.1998 - val_mean_absolute_error: 0.7853\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2248 - mean_absolute_error: 0.8062 - val_loss: 1.1570 - val_mean_absolute_error: 0.7906\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1872 - mean_absolute_error: 0.8011 - val_loss: 1.1393 - val_mean_absolute_error: 0.7854\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1810 - mean_absolute_error: 0.8011 - val_loss: 1.1389 - val_mean_absolute_error: 0.7859\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1788 - mean_absolute_error: 0.8011 - val_loss: 1.1340 - val_mean_absolute_error: 0.7839\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1764 - mean_absolute_error: 0.8003 - val_loss: 1.1421 - val_mean_absolute_error: 0.7892\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1750 - mean_absolute_error: 0.8003 - val_loss: 1.1342 - val_mean_absolute_error: 0.7870\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1735 - mean_absolute_error: 0.7998 - val_loss: 1.1631 - val_mean_absolute_error: 0.7982\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1746 - mean_absolute_error: 0.8006 - val_loss: 1.1330 - val_mean_absolute_error: 0.7850\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1731 - mean_absolute_error: 0.8004 - val_loss: 1.1319 - val_mean_absolute_error: 0.7848\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1714 - mean_absolute_error: 0.7999 - val_loss: 1.1513 - val_mean_absolute_error: 0.7974\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1708 - mean_absolute_error: 0.7998 - val_loss: 1.1386 - val_mean_absolute_error: 0.7890\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1699 - mean_absolute_error: 0.7997 - val_loss: 1.1292 - val_mean_absolute_error: 0.7850\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1675 - mean_absolute_error: 0.7995 - val_loss: 1.1309 - val_mean_absolute_error: 0.7855\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1679 - mean_absolute_error: 0.7998 - val_loss: 1.1308 - val_mean_absolute_error: 0.7861\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1686 - mean_absolute_error: 0.7993 - val_loss: 1.1288 - val_mean_absolute_error: 0.7854\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1682 - mean_absolute_error: 0.7997 - val_loss: 1.1271 - val_mean_absolute_error: 0.7853\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1671 - mean_absolute_error: 0.7995 - val_loss: 1.1335 - val_mean_absolute_error: 0.7875\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1670 - mean_absolute_error: 0.7995 - val_loss: 1.1337 - val_mean_absolute_error: 0.7872\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1669 - mean_absolute_error: 0.7995 - val_loss: 1.1316 - val_mean_absolute_error: 0.7850\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1659 - mean_absolute_error: 0.7993 - val_loss: 1.1345 - val_mean_absolute_error: 0.7862\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1663 - mean_absolute_error: 0.7994 - val_loss: 1.1301 - val_mean_absolute_error: 0.7863\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7995 - val_loss: 1.1250 - val_mean_absolute_error: 0.7865\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1646 - mean_absolute_error: 0.7990 - val_loss: 1.1296 - val_mean_absolute_error: 0.7853\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1655 - mean_absolute_error: 0.7993 - val_loss: 1.1244 - val_mean_absolute_error: 0.7861\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1645 - mean_absolute_error: 0.7992 - val_loss: 1.1266 - val_mean_absolute_error: 0.7868\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1648 - mean_absolute_error: 0.7992 - val_loss: 1.1352 - val_mean_absolute_error: 0.7868\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1656 - mean_absolute_error: 0.7995 - val_loss: 1.1286 - val_mean_absolute_error: 0.7847\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1633 - mean_absolute_error: 0.7990 - val_loss: 1.1261 - val_mean_absolute_error: 0.7846\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1652 - mean_absolute_error: 0.7997 - val_loss: 1.1294 - val_mean_absolute_error: 0.7847\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1645 - mean_absolute_error: 0.7990 - val_loss: 1.1260 - val_mean_absolute_error: 0.7869\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1632 - mean_absolute_error: 0.7989 - val_loss: 1.1275 - val_mean_absolute_error: 0.7841\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1639 - mean_absolute_error: 0.7993 - val_loss: 1.1262 - val_mean_absolute_error: 0.7863\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1611 - mean_absolute_error: 0.7988 - val_loss: 1.1346 - val_mean_absolute_error: 0.7863\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1627 - mean_absolute_error: 0.7991 - val_loss: 1.1409 - val_mean_absolute_error: 0.7904\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1622 - mean_absolute_error: 0.7990 - val_loss: 1.1281 - val_mean_absolute_error: 0.7849\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7988 - val_loss: 1.1271 - val_mean_absolute_error: 0.7864\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1621 - mean_absolute_error: 0.7991 - val_loss: 1.1295 - val_mean_absolute_error: 0.7851\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1608 - mean_absolute_error: 0.7989 - val_loss: 1.1226 - val_mean_absolute_error: 0.7850\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1615 - mean_absolute_error: 0.7987 - val_loss: 1.1249 - val_mean_absolute_error: 0.7861\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1621 - mean_absolute_error: 0.7993 - val_loss: 1.1252 - val_mean_absolute_error: 0.7862\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1619 - mean_absolute_error: 0.7991 - val_loss: 1.1325 - val_mean_absolute_error: 0.7858\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1614 - mean_absolute_error: 0.7986 - val_loss: 1.1233 - val_mean_absolute_error: 0.7846\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1612 - mean_absolute_error: 0.7991 - val_loss: 1.1388 - val_mean_absolute_error: 0.7931\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1618 - mean_absolute_error: 0.7992 - val_loss: 1.1330 - val_mean_absolute_error: 0.7861\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1601 - mean_absolute_error: 0.7989 - val_loss: 1.1235 - val_mean_absolute_error: 0.7854\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1597 - mean_absolute_error: 0.7985 - val_loss: 1.1241 - val_mean_absolute_error: 0.7872\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1592 - mean_absolute_error: 0.7982 - val_loss: 1.1220 - val_mean_absolute_error: 0.7864\n",
            "Processing fold 2\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 4s 17us/step - loss: 3.4115 - mean_absolute_error: 1.0439 - val_loss: 2.4870 - val_mean_absolute_error: 0.9825\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.8331 - mean_absolute_error: 0.8525 - val_loss: 1.3930 - val_mean_absolute_error: 0.7953\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3632 - mean_absolute_error: 0.8184 - val_loss: 1.2101 - val_mean_absolute_error: 0.7858\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2343 - mean_absolute_error: 0.8074 - val_loss: 1.1477 - val_mean_absolute_error: 0.7859\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1915 - mean_absolute_error: 0.8024 - val_loss: 1.1391 - val_mean_absolute_error: 0.7862\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1813 - mean_absolute_error: 0.8008 - val_loss: 1.1549 - val_mean_absolute_error: 0.7930\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1788 - mean_absolute_error: 0.8008 - val_loss: 1.1351 - val_mean_absolute_error: 0.7849\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1763 - mean_absolute_error: 0.8002 - val_loss: 1.1382 - val_mean_absolute_error: 0.7891\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1749 - mean_absolute_error: 0.7999 - val_loss: 1.1362 - val_mean_absolute_error: 0.7855\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1731 - mean_absolute_error: 0.8002 - val_loss: 1.1313 - val_mean_absolute_error: 0.7843\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1740 - mean_absolute_error: 0.8003 - val_loss: 1.1373 - val_mean_absolute_error: 0.7871\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1731 - mean_absolute_error: 0.8002 - val_loss: 1.1319 - val_mean_absolute_error: 0.7860\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1701 - mean_absolute_error: 0.7994 - val_loss: 1.1495 - val_mean_absolute_error: 0.7909\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1698 - mean_absolute_error: 0.7999 - val_loss: 1.1346 - val_mean_absolute_error: 0.7858\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1709 - mean_absolute_error: 0.8001 - val_loss: 1.1322 - val_mean_absolute_error: 0.7854\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1706 - mean_absolute_error: 0.7997 - val_loss: 1.1759 - val_mean_absolute_error: 0.8039\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1678 - mean_absolute_error: 0.7996 - val_loss: 1.1284 - val_mean_absolute_error: 0.7847\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1683 - mean_absolute_error: 0.7997 - val_loss: 1.1289 - val_mean_absolute_error: 0.7846\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1681 - mean_absolute_error: 0.7997 - val_loss: 1.1355 - val_mean_absolute_error: 0.7863\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1679 - mean_absolute_error: 0.7997 - val_loss: 1.1334 - val_mean_absolute_error: 0.7862\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1671 - mean_absolute_error: 0.7995 - val_loss: 1.1301 - val_mean_absolute_error: 0.7842\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1672 - mean_absolute_error: 0.7998 - val_loss: 1.1386 - val_mean_absolute_error: 0.7894\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1666 - mean_absolute_error: 0.7995 - val_loss: 1.1331 - val_mean_absolute_error: 0.7903\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1660 - mean_absolute_error: 0.7995 - val_loss: 1.1265 - val_mean_absolute_error: 0.7851\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7994 - val_loss: 1.1335 - val_mean_absolute_error: 0.7852\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7995 - val_loss: 1.1273 - val_mean_absolute_error: 0.7848\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1648 - mean_absolute_error: 0.7992 - val_loss: 1.1349 - val_mean_absolute_error: 0.7868\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1653 - mean_absolute_error: 0.7996 - val_loss: 1.1277 - val_mean_absolute_error: 0.7857\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1637 - mean_absolute_error: 0.7986 - val_loss: 1.1328 - val_mean_absolute_error: 0.7865\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1644 - mean_absolute_error: 0.7993 - val_loss: 1.1275 - val_mean_absolute_error: 0.7859\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1639 - mean_absolute_error: 0.7992 - val_loss: 1.1258 - val_mean_absolute_error: 0.7851\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1637 - mean_absolute_error: 0.7993 - val_loss: 1.1279 - val_mean_absolute_error: 0.7864\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7988 - val_loss: 1.1305 - val_mean_absolute_error: 0.7873\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1639 - mean_absolute_error: 0.7994 - val_loss: 1.1267 - val_mean_absolute_error: 0.7854\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1641 - mean_absolute_error: 0.7992 - val_loss: 1.1264 - val_mean_absolute_error: 0.7850\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7991 - val_loss: 1.1400 - val_mean_absolute_error: 0.7912\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1618 - mean_absolute_error: 0.7988 - val_loss: 1.1301 - val_mean_absolute_error: 0.7874\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7990 - val_loss: 1.1260 - val_mean_absolute_error: 0.7863\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1621 - mean_absolute_error: 0.7991 - val_loss: 1.1248 - val_mean_absolute_error: 0.7839\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1615 - mean_absolute_error: 0.7986 - val_loss: 1.1224 - val_mean_absolute_error: 0.7846\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1613 - mean_absolute_error: 0.7986 - val_loss: 1.1277 - val_mean_absolute_error: 0.7865\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1619 - mean_absolute_error: 0.7988 - val_loss: 1.1295 - val_mean_absolute_error: 0.7852\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7994 - val_loss: 1.1359 - val_mean_absolute_error: 0.7886\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1614 - mean_absolute_error: 0.7990 - val_loss: 1.1307 - val_mean_absolute_error: 0.7855\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1609 - mean_absolute_error: 0.7988 - val_loss: 1.1290 - val_mean_absolute_error: 0.7848\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1601 - mean_absolute_error: 0.7990 - val_loss: 1.1305 - val_mean_absolute_error: 0.7869\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1606 - mean_absolute_error: 0.7985 - val_loss: 1.1331 - val_mean_absolute_error: 0.7899\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1611 - mean_absolute_error: 0.7990 - val_loss: 1.1467 - val_mean_absolute_error: 0.7946\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1588 - mean_absolute_error: 0.7985 - val_loss: 1.1364 - val_mean_absolute_error: 0.7920\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1590 - mean_absolute_error: 0.7986 - val_loss: 1.1277 - val_mean_absolute_error: 0.7853\n",
            "Processing fold 3\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 19us/step - loss: 3.3653 - mean_absolute_error: 1.0491 - val_loss: 1.9970 - val_mean_absolute_error: 0.8305\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.7189 - mean_absolute_error: 0.8494 - val_loss: 1.5094 - val_mean_absolute_error: 0.8680\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3276 - mean_absolute_error: 0.8183 - val_loss: 1.2067 - val_mean_absolute_error: 0.7881\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2244 - mean_absolute_error: 0.8072 - val_loss: 1.1510 - val_mean_absolute_error: 0.7838\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1882 - mean_absolute_error: 0.8011 - val_loss: 1.1385 - val_mean_absolute_error: 0.7864\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1805 - mean_absolute_error: 0.8006 - val_loss: 1.1392 - val_mean_absolute_error: 0.7843\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1767 - mean_absolute_error: 0.7997 - val_loss: 1.1374 - val_mean_absolute_error: 0.7841\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1753 - mean_absolute_error: 0.7997 - val_loss: 1.1328 - val_mean_absolute_error: 0.7851\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1743 - mean_absolute_error: 0.7999 - val_loss: 1.1352 - val_mean_absolute_error: 0.7849\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1731 - mean_absolute_error: 0.8003 - val_loss: 1.1328 - val_mean_absolute_error: 0.7847\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1738 - mean_absolute_error: 0.8005 - val_loss: 1.1328 - val_mean_absolute_error: 0.7846\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1723 - mean_absolute_error: 0.8003 - val_loss: 1.1398 - val_mean_absolute_error: 0.7910\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1709 - mean_absolute_error: 0.8000 - val_loss: 1.1329 - val_mean_absolute_error: 0.7861\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1700 - mean_absolute_error: 0.7993 - val_loss: 1.1321 - val_mean_absolute_error: 0.7849\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1693 - mean_absolute_error: 0.7995 - val_loss: 1.1270 - val_mean_absolute_error: 0.7856\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1688 - mean_absolute_error: 0.7995 - val_loss: 1.1431 - val_mean_absolute_error: 0.7889\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1684 - mean_absolute_error: 0.7997 - val_loss: 1.1292 - val_mean_absolute_error: 0.7856\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1670 - mean_absolute_error: 0.7992 - val_loss: 1.1321 - val_mean_absolute_error: 0.7868\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1683 - mean_absolute_error: 0.7997 - val_loss: 1.1328 - val_mean_absolute_error: 0.7847\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1685 - mean_absolute_error: 0.7993 - val_loss: 1.1389 - val_mean_absolute_error: 0.7893\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1658 - mean_absolute_error: 0.7989 - val_loss: 1.1287 - val_mean_absolute_error: 0.7874\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1666 - mean_absolute_error: 0.7994 - val_loss: 1.1421 - val_mean_absolute_error: 0.7884\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1654 - mean_absolute_error: 0.7988 - val_loss: 1.1369 - val_mean_absolute_error: 0.7909\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1651 - mean_absolute_error: 0.7990 - val_loss: 1.1370 - val_mean_absolute_error: 0.7895\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1646 - mean_absolute_error: 0.7988 - val_loss: 1.1268 - val_mean_absolute_error: 0.7856\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1657 - mean_absolute_error: 0.7992 - val_loss: 1.1341 - val_mean_absolute_error: 0.7859\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1642 - mean_absolute_error: 0.7991 - val_loss: 1.1290 - val_mean_absolute_error: 0.7852\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1646 - mean_absolute_error: 0.7993 - val_loss: 1.1293 - val_mean_absolute_error: 0.7875\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1635 - mean_absolute_error: 0.7990 - val_loss: 1.1340 - val_mean_absolute_error: 0.7885\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1638 - mean_absolute_error: 0.7991 - val_loss: 1.1272 - val_mean_absolute_error: 0.7856\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1638 - mean_absolute_error: 0.7994 - val_loss: 1.1267 - val_mean_absolute_error: 0.7855\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1640 - mean_absolute_error: 0.7991 - val_loss: 1.1258 - val_mean_absolute_error: 0.7852\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7990 - val_loss: 1.1482 - val_mean_absolute_error: 0.7931\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1624 - mean_absolute_error: 0.7989 - val_loss: 1.1532 - val_mean_absolute_error: 0.7998\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1626 - mean_absolute_error: 0.7991 - val_loss: 1.1245 - val_mean_absolute_error: 0.7862\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1621 - mean_absolute_error: 0.7989 - val_loss: 1.1295 - val_mean_absolute_error: 0.7873\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1621 - mean_absolute_error: 0.7990 - val_loss: 1.1314 - val_mean_absolute_error: 0.7861\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1609 - mean_absolute_error: 0.7987 - val_loss: 1.1242 - val_mean_absolute_error: 0.7851\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1623 - mean_absolute_error: 0.7992 - val_loss: 1.1380 - val_mean_absolute_error: 0.7899\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1608 - mean_absolute_error: 0.7985 - val_loss: 1.1215 - val_mean_absolute_error: 0.7863\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1613 - mean_absolute_error: 0.7990 - val_loss: 1.1239 - val_mean_absolute_error: 0.7855\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1602 - mean_absolute_error: 0.7985 - val_loss: 1.1319 - val_mean_absolute_error: 0.7877\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1610 - mean_absolute_error: 0.7986 - val_loss: 1.1232 - val_mean_absolute_error: 0.7871\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1610 - mean_absolute_error: 0.7987 - val_loss: 1.1255 - val_mean_absolute_error: 0.7864\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1603 - mean_absolute_error: 0.7986 - val_loss: 1.1521 - val_mean_absolute_error: 0.7989\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1605 - mean_absolute_error: 0.7989 - val_loss: 1.1288 - val_mean_absolute_error: 0.7861\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1596 - mean_absolute_error: 0.7985 - val_loss: 1.1287 - val_mean_absolute_error: 0.7848\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1599 - mean_absolute_error: 0.7985 - val_loss: 1.1231 - val_mean_absolute_error: 0.7847\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1606 - mean_absolute_error: 0.7990 - val_loss: 1.1287 - val_mean_absolute_error: 0.7877\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1600 - mean_absolute_error: 0.7988 - val_loss: 1.1229 - val_mean_absolute_error: 0.7869\n",
            "Processing fold 4\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 18us/step - loss: 3.4278 - mean_absolute_error: 1.0653 - val_loss: 2.0892 - val_mean_absolute_error: 0.8617\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.7249 - mean_absolute_error: 0.8514 - val_loss: 1.3174 - val_mean_absolute_error: 0.7845\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3266 - mean_absolute_error: 0.8178 - val_loss: 1.1992 - val_mean_absolute_error: 0.7848\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2281 - mean_absolute_error: 0.8083 - val_loss: 1.1680 - val_mean_absolute_error: 0.7877\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1892 - mean_absolute_error: 0.8015 - val_loss: 1.1375 - val_mean_absolute_error: 0.7846\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1805 - mean_absolute_error: 0.8006 - val_loss: 1.1445 - val_mean_absolute_error: 0.7871\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1791 - mean_absolute_error: 0.8001 - val_loss: 1.1403 - val_mean_absolute_error: 0.7851\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1766 - mean_absolute_error: 0.8000 - val_loss: 1.1336 - val_mean_absolute_error: 0.7848\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1760 - mean_absolute_error: 0.8004 - val_loss: 1.1347 - val_mean_absolute_error: 0.7840\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1742 - mean_absolute_error: 0.8000 - val_loss: 1.1338 - val_mean_absolute_error: 0.7863\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1714 - mean_absolute_error: 0.7998 - val_loss: 1.1335 - val_mean_absolute_error: 0.7850\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1706 - mean_absolute_error: 0.7995 - val_loss: 1.1381 - val_mean_absolute_error: 0.7914\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1716 - mean_absolute_error: 0.7999 - val_loss: 1.1401 - val_mean_absolute_error: 0.7882\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1689 - mean_absolute_error: 0.7991 - val_loss: 1.1304 - val_mean_absolute_error: 0.7851\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1708 - mean_absolute_error: 0.7995 - val_loss: 1.1488 - val_mean_absolute_error: 0.7880\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1681 - mean_absolute_error: 0.7993 - val_loss: 1.1306 - val_mean_absolute_error: 0.7849\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1701 - mean_absolute_error: 0.7993 - val_loss: 1.1414 - val_mean_absolute_error: 0.7898\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1674 - mean_absolute_error: 0.7991 - val_loss: 1.1366 - val_mean_absolute_error: 0.7876\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1665 - mean_absolute_error: 0.7993 - val_loss: 1.1294 - val_mean_absolute_error: 0.7865\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1672 - mean_absolute_error: 0.7997 - val_loss: 1.1280 - val_mean_absolute_error: 0.7861\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1675 - mean_absolute_error: 0.7992 - val_loss: 1.1317 - val_mean_absolute_error: 0.7862\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1662 - mean_absolute_error: 0.7991 - val_loss: 1.1308 - val_mean_absolute_error: 0.7858\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1656 - mean_absolute_error: 0.7993 - val_loss: 1.1287 - val_mean_absolute_error: 0.7846\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1647 - mean_absolute_error: 0.7992 - val_loss: 1.1364 - val_mean_absolute_error: 0.7862\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1643 - mean_absolute_error: 0.7990 - val_loss: 1.1397 - val_mean_absolute_error: 0.7900\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1643 - mean_absolute_error: 0.7988 - val_loss: 1.1277 - val_mean_absolute_error: 0.7855\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1654 - mean_absolute_error: 0.7992 - val_loss: 1.1272 - val_mean_absolute_error: 0.7851\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1649 - mean_absolute_error: 0.7992 - val_loss: 1.1261 - val_mean_absolute_error: 0.7857\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1642 - mean_absolute_error: 0.7993 - val_loss: 1.1361 - val_mean_absolute_error: 0.7888\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1646 - mean_absolute_error: 0.7992 - val_loss: 1.1259 - val_mean_absolute_error: 0.7854\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1646 - mean_absolute_error: 0.7991 - val_loss: 1.1285 - val_mean_absolute_error: 0.7858\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1640 - mean_absolute_error: 0.7994 - val_loss: 1.1275 - val_mean_absolute_error: 0.7872\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1635 - mean_absolute_error: 0.7990 - val_loss: 1.1361 - val_mean_absolute_error: 0.7859\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7989 - val_loss: 1.1277 - val_mean_absolute_error: 0.7847\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1637 - mean_absolute_error: 0.7992 - val_loss: 1.1299 - val_mean_absolute_error: 0.7864\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1612 - mean_absolute_error: 0.7985 - val_loss: 1.1241 - val_mean_absolute_error: 0.7851\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1620 - mean_absolute_error: 0.7987 - val_loss: 1.1321 - val_mean_absolute_error: 0.7856\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1619 - mean_absolute_error: 0.7988 - val_loss: 1.1262 - val_mean_absolute_error: 0.7856\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1606 - mean_absolute_error: 0.7985 - val_loss: 1.1247 - val_mean_absolute_error: 0.7858\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1624 - mean_absolute_error: 0.7987 - val_loss: 1.1306 - val_mean_absolute_error: 0.7882\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7990 - val_loss: 1.1261 - val_mean_absolute_error: 0.7871\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1609 - mean_absolute_error: 0.7985 - val_loss: 1.1317 - val_mean_absolute_error: 0.7869\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1610 - mean_absolute_error: 0.7987 - val_loss: 1.1492 - val_mean_absolute_error: 0.7940\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1621 - mean_absolute_error: 0.7991 - val_loss: 1.1333 - val_mean_absolute_error: 0.7875\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1617 - mean_absolute_error: 0.7989 - val_loss: 1.1322 - val_mean_absolute_error: 0.7888\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1612 - mean_absolute_error: 0.7989 - val_loss: 1.1245 - val_mean_absolute_error: 0.7857\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1593 - mean_absolute_error: 0.7983 - val_loss: 1.1226 - val_mean_absolute_error: 0.7855\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1597 - mean_absolute_error: 0.7987 - val_loss: 1.1229 - val_mean_absolute_error: 0.7859\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1594 - mean_absolute_error: 0.7986 - val_loss: 1.1446 - val_mean_absolute_error: 0.7948\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1595 - mean_absolute_error: 0.7984 - val_loss: 1.1364 - val_mean_absolute_error: 0.7915\n",
            "Processing fold 5\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 19us/step - loss: 3.3358 - mean_absolute_error: 1.0514 - val_loss: 3.1834 - val_mean_absolute_error: 1.2581\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.7904 - mean_absolute_error: 0.8523 - val_loss: 1.3714 - val_mean_absolute_error: 0.7903\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3584 - mean_absolute_error: 0.8173 - val_loss: 1.3748 - val_mean_absolute_error: 0.8587\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2357 - mean_absolute_error: 0.8078 - val_loss: 1.1711 - val_mean_absolute_error: 0.7926\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1903 - mean_absolute_error: 0.8018 - val_loss: 1.1446 - val_mean_absolute_error: 0.7843\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1835 - mean_absolute_error: 0.8008 - val_loss: 1.1423 - val_mean_absolute_error: 0.7845\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1795 - mean_absolute_error: 0.8004 - val_loss: 1.1348 - val_mean_absolute_error: 0.7838\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1768 - mean_absolute_error: 0.7999 - val_loss: 1.1577 - val_mean_absolute_error: 0.7921\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1765 - mean_absolute_error: 0.8001 - val_loss: 1.1593 - val_mean_absolute_error: 0.7945\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1744 - mean_absolute_error: 0.8001 - val_loss: 1.1433 - val_mean_absolute_error: 0.7896\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1737 - mean_absolute_error: 0.8004 - val_loss: 1.1382 - val_mean_absolute_error: 0.7859\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1717 - mean_absolute_error: 0.8003 - val_loss: 1.1383 - val_mean_absolute_error: 0.7866\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1727 - mean_absolute_error: 0.8003 - val_loss: 1.1359 - val_mean_absolute_error: 0.7851\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1712 - mean_absolute_error: 0.7999 - val_loss: 1.1367 - val_mean_absolute_error: 0.7850\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1718 - mean_absolute_error: 0.8006 - val_loss: 1.1322 - val_mean_absolute_error: 0.7857\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1694 - mean_absolute_error: 0.7995 - val_loss: 1.1582 - val_mean_absolute_error: 0.7972\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1690 - mean_absolute_error: 0.7996 - val_loss: 1.1306 - val_mean_absolute_error: 0.7851\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1674 - mean_absolute_error: 0.7993 - val_loss: 1.1266 - val_mean_absolute_error: 0.7849\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1650 - mean_absolute_error: 0.7989 - val_loss: 1.1390 - val_mean_absolute_error: 0.7891\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1678 - mean_absolute_error: 0.7998 - val_loss: 1.1329 - val_mean_absolute_error: 0.7874\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1677 - mean_absolute_error: 0.7994 - val_loss: 1.1273 - val_mean_absolute_error: 0.7842\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1677 - mean_absolute_error: 0.7996 - val_loss: 1.1381 - val_mean_absolute_error: 0.7886\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1656 - mean_absolute_error: 0.7993 - val_loss: 1.1288 - val_mean_absolute_error: 0.7860\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1664 - mean_absolute_error: 0.7996 - val_loss: 1.1297 - val_mean_absolute_error: 0.7875\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1651 - mean_absolute_error: 0.7990 - val_loss: 1.1346 - val_mean_absolute_error: 0.7898\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1660 - mean_absolute_error: 0.7993 - val_loss: 1.1307 - val_mean_absolute_error: 0.7856\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1638 - mean_absolute_error: 0.7990 - val_loss: 1.1245 - val_mean_absolute_error: 0.7857\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1654 - mean_absolute_error: 0.7996 - val_loss: 1.1317 - val_mean_absolute_error: 0.7861\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1645 - mean_absolute_error: 0.7991 - val_loss: 1.1397 - val_mean_absolute_error: 0.7884\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1642 - mean_absolute_error: 0.7994 - val_loss: 1.1259 - val_mean_absolute_error: 0.7844\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1645 - mean_absolute_error: 0.7994 - val_loss: 1.1264 - val_mean_absolute_error: 0.7851\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1631 - mean_absolute_error: 0.7989 - val_loss: 1.1307 - val_mean_absolute_error: 0.7904\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1643 - mean_absolute_error: 0.7996 - val_loss: 1.1290 - val_mean_absolute_error: 0.7851\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1624 - mean_absolute_error: 0.7986 - val_loss: 1.1308 - val_mean_absolute_error: 0.7880\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1627 - mean_absolute_error: 0.7991 - val_loss: 1.1277 - val_mean_absolute_error: 0.7874\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1615 - mean_absolute_error: 0.7987 - val_loss: 1.1270 - val_mean_absolute_error: 0.7866\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1622 - mean_absolute_error: 0.7989 - val_loss: 1.1265 - val_mean_absolute_error: 0.7849\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1623 - mean_absolute_error: 0.7985 - val_loss: 1.1296 - val_mean_absolute_error: 0.7869\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1605 - mean_absolute_error: 0.7986 - val_loss: 1.1297 - val_mean_absolute_error: 0.7866\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1631 - mean_absolute_error: 0.7993 - val_loss: 1.1279 - val_mean_absolute_error: 0.7852\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1617 - mean_absolute_error: 0.7992 - val_loss: 1.1253 - val_mean_absolute_error: 0.7871\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1612 - mean_absolute_error: 0.7987 - val_loss: 1.1412 - val_mean_absolute_error: 0.7909\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1610 - mean_absolute_error: 0.7985 - val_loss: 1.1258 - val_mean_absolute_error: 0.7852\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1615 - mean_absolute_error: 0.7987 - val_loss: 1.1233 - val_mean_absolute_error: 0.7858\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1613 - mean_absolute_error: 0.7989 - val_loss: 1.1335 - val_mean_absolute_error: 0.7893\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1608 - mean_absolute_error: 0.7988 - val_loss: 1.1218 - val_mean_absolute_error: 0.7861\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1610 - mean_absolute_error: 0.7988 - val_loss: 1.1275 - val_mean_absolute_error: 0.7879\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1600 - mean_absolute_error: 0.7986 - val_loss: 1.1313 - val_mean_absolute_error: 0.7859\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1605 - mean_absolute_error: 0.7990 - val_loss: 1.1235 - val_mean_absolute_error: 0.7874\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1592 - mean_absolute_error: 0.7983 - val_loss: 1.1251 - val_mean_absolute_error: 0.7858\n",
            "Processing fold 6\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 19us/step - loss: 3.3582 - mean_absolute_error: 1.0368 - val_loss: 2.1221 - val_mean_absolute_error: 0.8489\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.7800 - mean_absolute_error: 0.8480 - val_loss: 1.3566 - val_mean_absolute_error: 0.7845\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3404 - mean_absolute_error: 0.8168 - val_loss: 1.2160 - val_mean_absolute_error: 0.7888\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2198 - mean_absolute_error: 0.8065 - val_loss: 1.1467 - val_mean_absolute_error: 0.7839\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1860 - mean_absolute_error: 0.8015 - val_loss: 1.1455 - val_mean_absolute_error: 0.7864\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1797 - mean_absolute_error: 0.8009 - val_loss: 1.1364 - val_mean_absolute_error: 0.7848\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1783 - mean_absolute_error: 0.8007 - val_loss: 1.1437 - val_mean_absolute_error: 0.7868\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1760 - mean_absolute_error: 0.8000 - val_loss: 1.1440 - val_mean_absolute_error: 0.7887\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1766 - mean_absolute_error: 0.8002 - val_loss: 1.1362 - val_mean_absolute_error: 0.7845\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1745 - mean_absolute_error: 0.7997 - val_loss: 1.1336 - val_mean_absolute_error: 0.7844\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1733 - mean_absolute_error: 0.8003 - val_loss: 1.1349 - val_mean_absolute_error: 0.7851\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1730 - mean_absolute_error: 0.8000 - val_loss: 1.1378 - val_mean_absolute_error: 0.7863\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1729 - mean_absolute_error: 0.8003 - val_loss: 1.1327 - val_mean_absolute_error: 0.7860\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1716 - mean_absolute_error: 0.7997 - val_loss: 1.1332 - val_mean_absolute_error: 0.7848\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1711 - mean_absolute_error: 0.7996 - val_loss: 1.1331 - val_mean_absolute_error: 0.7875\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1698 - mean_absolute_error: 0.7996 - val_loss: 1.1331 - val_mean_absolute_error: 0.7853\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1693 - mean_absolute_error: 0.7998 - val_loss: 1.1461 - val_mean_absolute_error: 0.7882\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1688 - mean_absolute_error: 0.7997 - val_loss: 1.1295 - val_mean_absolute_error: 0.7859\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1683 - mean_absolute_error: 0.7996 - val_loss: 1.1366 - val_mean_absolute_error: 0.7892\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1680 - mean_absolute_error: 0.7997 - val_loss: 1.1485 - val_mean_absolute_error: 0.7932\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1680 - mean_absolute_error: 0.7997 - val_loss: 1.1296 - val_mean_absolute_error: 0.7859\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1668 - mean_absolute_error: 0.7993 - val_loss: 1.1305 - val_mean_absolute_error: 0.7861\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1657 - mean_absolute_error: 0.7993 - val_loss: 1.1293 - val_mean_absolute_error: 0.7882\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1672 - mean_absolute_error: 0.7997 - val_loss: 1.1290 - val_mean_absolute_error: 0.7852\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1664 - mean_absolute_error: 0.7996 - val_loss: 1.1332 - val_mean_absolute_error: 0.7883\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1668 - mean_absolute_error: 0.7997 - val_loss: 1.1279 - val_mean_absolute_error: 0.7852\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1641 - mean_absolute_error: 0.7989 - val_loss: 1.1248 - val_mean_absolute_error: 0.7860\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1659 - mean_absolute_error: 0.7996 - val_loss: 1.1294 - val_mean_absolute_error: 0.7847\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1647 - mean_absolute_error: 0.7992 - val_loss: 1.1285 - val_mean_absolute_error: 0.7849\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1648 - mean_absolute_error: 0.7994 - val_loss: 1.1264 - val_mean_absolute_error: 0.7855\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1655 - mean_absolute_error: 0.7993 - val_loss: 1.1271 - val_mean_absolute_error: 0.7840\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1642 - mean_absolute_error: 0.7991 - val_loss: 1.1275 - val_mean_absolute_error: 0.7877\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1663 - mean_absolute_error: 0.7997 - val_loss: 1.1296 - val_mean_absolute_error: 0.7844\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1632 - mean_absolute_error: 0.7993 - val_loss: 1.1306 - val_mean_absolute_error: 0.7902\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1617 - mean_absolute_error: 0.7982 - val_loss: 1.1412 - val_mean_absolute_error: 0.7955\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7990 - val_loss: 1.1407 - val_mean_absolute_error: 0.7940\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1636 - mean_absolute_error: 0.7990 - val_loss: 1.1369 - val_mean_absolute_error: 0.7882\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1631 - mean_absolute_error: 0.7990 - val_loss: 1.1259 - val_mean_absolute_error: 0.7864\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1635 - mean_absolute_error: 0.7993 - val_loss: 1.1283 - val_mean_absolute_error: 0.7862\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1618 - mean_absolute_error: 0.7986 - val_loss: 1.1339 - val_mean_absolute_error: 0.7864\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1626 - mean_absolute_error: 0.7992 - val_loss: 1.1404 - val_mean_absolute_error: 0.7887\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7989 - val_loss: 1.1363 - val_mean_absolute_error: 0.7896\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1618 - mean_absolute_error: 0.7989 - val_loss: 1.1322 - val_mean_absolute_error: 0.7919\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1611 - mean_absolute_error: 0.7987 - val_loss: 1.1420 - val_mean_absolute_error: 0.7895\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1609 - mean_absolute_error: 0.7989 - val_loss: 1.1308 - val_mean_absolute_error: 0.7879\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1614 - mean_absolute_error: 0.7989 - val_loss: 1.1316 - val_mean_absolute_error: 0.7897\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1603 - mean_absolute_error: 0.7987 - val_loss: 1.1258 - val_mean_absolute_error: 0.7859\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1608 - mean_absolute_error: 0.7991 - val_loss: 1.1252 - val_mean_absolute_error: 0.7859\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1602 - mean_absolute_error: 0.7988 - val_loss: 1.1386 - val_mean_absolute_error: 0.7905\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1596 - mean_absolute_error: 0.7986 - val_loss: 1.1403 - val_mean_absolute_error: 0.7928\n",
            "Processing fold 7\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 20us/step - loss: 3.4760 - mean_absolute_error: 1.0564 - val_loss: 3.1490 - val_mean_absolute_error: 1.2129\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.8380 - mean_absolute_error: 0.8562 - val_loss: 1.4071 - val_mean_absolute_error: 0.8097\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3490 - mean_absolute_error: 0.8188 - val_loss: 1.1972 - val_mean_absolute_error: 0.7867\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2257 - mean_absolute_error: 0.8076 - val_loss: 1.1633 - val_mean_absolute_error: 0.7929\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1881 - mean_absolute_error: 0.8016 - val_loss: 1.1374 - val_mean_absolute_error: 0.7849\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1805 - mean_absolute_error: 0.8008 - val_loss: 1.1430 - val_mean_absolute_error: 0.7871\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1786 - mean_absolute_error: 0.8004 - val_loss: 1.1410 - val_mean_absolute_error: 0.7875\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1772 - mean_absolute_error: 0.8000 - val_loss: 1.1358 - val_mean_absolute_error: 0.7852\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1736 - mean_absolute_error: 0.7996 - val_loss: 1.1352 - val_mean_absolute_error: 0.7858\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1741 - mean_absolute_error: 0.8000 - val_loss: 1.1363 - val_mean_absolute_error: 0.7886\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1723 - mean_absolute_error: 0.8002 - val_loss: 1.1457 - val_mean_absolute_error: 0.7919\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1700 - mean_absolute_error: 0.7994 - val_loss: 1.1345 - val_mean_absolute_error: 0.7857\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1701 - mean_absolute_error: 0.7997 - val_loss: 1.1366 - val_mean_absolute_error: 0.7889\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1696 - mean_absolute_error: 0.8000 - val_loss: 1.1299 - val_mean_absolute_error: 0.7852\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1701 - mean_absolute_error: 0.7996 - val_loss: 1.1344 - val_mean_absolute_error: 0.7879\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1686 - mean_absolute_error: 0.7998 - val_loss: 1.1441 - val_mean_absolute_error: 0.7900\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1682 - mean_absolute_error: 0.7995 - val_loss: 1.1280 - val_mean_absolute_error: 0.7863\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1686 - mean_absolute_error: 0.7996 - val_loss: 1.1278 - val_mean_absolute_error: 0.7850\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1672 - mean_absolute_error: 0.8000 - val_loss: 1.1303 - val_mean_absolute_error: 0.7847\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7996 - val_loss: 1.1495 - val_mean_absolute_error: 0.7942\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1668 - mean_absolute_error: 0.7996 - val_loss: 1.1273 - val_mean_absolute_error: 0.7848\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1676 - mean_absolute_error: 0.8001 - val_loss: 1.1283 - val_mean_absolute_error: 0.7849\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1643 - mean_absolute_error: 0.7993 - val_loss: 1.1410 - val_mean_absolute_error: 0.7926\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1652 - mean_absolute_error: 0.7994 - val_loss: 1.1287 - val_mean_absolute_error: 0.7873\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1646 - mean_absolute_error: 0.7990 - val_loss: 1.1357 - val_mean_absolute_error: 0.7875\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1645 - mean_absolute_error: 0.7987 - val_loss: 1.1419 - val_mean_absolute_error: 0.7917\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1636 - mean_absolute_error: 0.7990 - val_loss: 1.1334 - val_mean_absolute_error: 0.7861\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1639 - mean_absolute_error: 0.7991 - val_loss: 1.1296 - val_mean_absolute_error: 0.7855\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1640 - mean_absolute_error: 0.7992 - val_loss: 1.1292 - val_mean_absolute_error: 0.7858\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7989 - val_loss: 1.1266 - val_mean_absolute_error: 0.7848\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1620 - mean_absolute_error: 0.7987 - val_loss: 1.1325 - val_mean_absolute_error: 0.7860\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1619 - mean_absolute_error: 0.7989 - val_loss: 1.1247 - val_mean_absolute_error: 0.7851\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1647 - mean_absolute_error: 0.7998 - val_loss: 1.1269 - val_mean_absolute_error: 0.7858\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1626 - mean_absolute_error: 0.7994 - val_loss: 1.1332 - val_mean_absolute_error: 0.7908\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1619 - mean_absolute_error: 0.7986 - val_loss: 1.1238 - val_mean_absolute_error: 0.7855\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1606 - mean_absolute_error: 0.7984 - val_loss: 1.1259 - val_mean_absolute_error: 0.7870\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1627 - mean_absolute_error: 0.7993 - val_loss: 1.1245 - val_mean_absolute_error: 0.7854\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1607 - mean_absolute_error: 0.7983 - val_loss: 1.1270 - val_mean_absolute_error: 0.7867\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1628 - mean_absolute_error: 0.7994 - val_loss: 1.1256 - val_mean_absolute_error: 0.7840\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1617 - mean_absolute_error: 0.7988 - val_loss: 1.1245 - val_mean_absolute_error: 0.7849\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1613 - mean_absolute_error: 0.7987 - val_loss: 1.1352 - val_mean_absolute_error: 0.7902\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1598 - mean_absolute_error: 0.7986 - val_loss: 1.1264 - val_mean_absolute_error: 0.7855\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1598 - mean_absolute_error: 0.7985 - val_loss: 1.1264 - val_mean_absolute_error: 0.7867\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1614 - mean_absolute_error: 0.7992 - val_loss: 1.1260 - val_mean_absolute_error: 0.7864\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1603 - mean_absolute_error: 0.7991 - val_loss: 1.1218 - val_mean_absolute_error: 0.7852\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1605 - mean_absolute_error: 0.7991 - val_loss: 1.1276 - val_mean_absolute_error: 0.7888\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1593 - mean_absolute_error: 0.7985 - val_loss: 1.1223 - val_mean_absolute_error: 0.7844\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1600 - mean_absolute_error: 0.7988 - val_loss: 1.1266 - val_mean_absolute_error: 0.7864\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1607 - mean_absolute_error: 0.7990 - val_loss: 1.1359 - val_mean_absolute_error: 0.7885\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1604 - mean_absolute_error: 0.7990 - val_loss: 1.1332 - val_mean_absolute_error: 0.7904\n",
            "Processing fold 8\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 20us/step - loss: 3.3921 - mean_absolute_error: 1.0694 - val_loss: 2.5815 - val_mean_absolute_error: 1.0607\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.7661 - mean_absolute_error: 0.8563 - val_loss: 1.3325 - val_mean_absolute_error: 0.7842\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3340 - mean_absolute_error: 0.8179 - val_loss: 1.2705 - val_mean_absolute_error: 0.8060\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.2287 - mean_absolute_error: 0.8069 - val_loss: 1.1872 - val_mean_absolute_error: 0.8029\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1890 - mean_absolute_error: 0.8017 - val_loss: 1.1421 - val_mean_absolute_error: 0.7841\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1817 - mean_absolute_error: 0.8006 - val_loss: 1.1387 - val_mean_absolute_error: 0.7855\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1767 - mean_absolute_error: 0.7999 - val_loss: 1.1457 - val_mean_absolute_error: 0.7867\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1757 - mean_absolute_error: 0.8001 - val_loss: 1.1464 - val_mean_absolute_error: 0.7893\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1743 - mean_absolute_error: 0.7997 - val_loss: 1.1376 - val_mean_absolute_error: 0.7851\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1731 - mean_absolute_error: 0.7997 - val_loss: 1.1398 - val_mean_absolute_error: 0.7862\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1719 - mean_absolute_error: 0.7997 - val_loss: 1.1354 - val_mean_absolute_error: 0.7848\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1712 - mean_absolute_error: 0.7996 - val_loss: 1.1312 - val_mean_absolute_error: 0.7847\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1699 - mean_absolute_error: 0.7997 - val_loss: 1.1478 - val_mean_absolute_error: 0.7899\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1677 - mean_absolute_error: 0.7988 - val_loss: 1.1360 - val_mean_absolute_error: 0.7859\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1697 - mean_absolute_error: 0.7994 - val_loss: 1.1356 - val_mean_absolute_error: 0.7862\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1691 - mean_absolute_error: 0.7991 - val_loss: 1.1332 - val_mean_absolute_error: 0.7866\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1683 - mean_absolute_error: 0.7999 - val_loss: 1.1379 - val_mean_absolute_error: 0.7859\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1666 - mean_absolute_error: 0.7990 - val_loss: 1.1564 - val_mean_absolute_error: 0.7934\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1668 - mean_absolute_error: 0.7992 - val_loss: 1.1385 - val_mean_absolute_error: 0.7892\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7993 - val_loss: 1.1447 - val_mean_absolute_error: 0.7921\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1656 - mean_absolute_error: 0.7988 - val_loss: 1.1338 - val_mean_absolute_error: 0.7891\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1649 - mean_absolute_error: 0.7990 - val_loss: 1.1373 - val_mean_absolute_error: 0.7887\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1655 - mean_absolute_error: 0.7992 - val_loss: 1.1299 - val_mean_absolute_error: 0.7854\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1650 - mean_absolute_error: 0.7992 - val_loss: 1.1279 - val_mean_absolute_error: 0.7857\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1644 - mean_absolute_error: 0.7992 - val_loss: 1.1367 - val_mean_absolute_error: 0.7876\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1641 - mean_absolute_error: 0.7990 - val_loss: 1.1252 - val_mean_absolute_error: 0.7858\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1644 - mean_absolute_error: 0.7991 - val_loss: 1.1312 - val_mean_absolute_error: 0.7896\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1641 - mean_absolute_error: 0.7992 - val_loss: 1.1273 - val_mean_absolute_error: 0.7860\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1655 - mean_absolute_error: 0.7993 - val_loss: 1.1362 - val_mean_absolute_error: 0.7865\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1635 - mean_absolute_error: 0.7990 - val_loss: 1.1283 - val_mean_absolute_error: 0.7857\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7985 - val_loss: 1.1241 - val_mean_absolute_error: 0.7848\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7991 - val_loss: 1.1289 - val_mean_absolute_error: 0.7843\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1624 - mean_absolute_error: 0.7986 - val_loss: 1.1365 - val_mean_absolute_error: 0.7877\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1629 - mean_absolute_error: 0.7990 - val_loss: 1.1250 - val_mean_absolute_error: 0.7846\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1626 - mean_absolute_error: 0.7988 - val_loss: 1.1328 - val_mean_absolute_error: 0.7903\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1625 - mean_absolute_error: 0.7994 - val_loss: 1.1390 - val_mean_absolute_error: 0.7919\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1624 - mean_absolute_error: 0.7991 - val_loss: 1.1271 - val_mean_absolute_error: 0.7875\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1612 - mean_absolute_error: 0.7988 - val_loss: 1.1275 - val_mean_absolute_error: 0.7854\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1610 - mean_absolute_error: 0.7988 - val_loss: 1.1357 - val_mean_absolute_error: 0.7911\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1619 - mean_absolute_error: 0.7992 - val_loss: 1.1351 - val_mean_absolute_error: 0.7882\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1605 - mean_absolute_error: 0.7983 - val_loss: 1.1244 - val_mean_absolute_error: 0.7886\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1603 - mean_absolute_error: 0.7983 - val_loss: 1.1289 - val_mean_absolute_error: 0.7855\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1583 - mean_absolute_error: 0.7980 - val_loss: 1.1245 - val_mean_absolute_error: 0.7844\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1612 - mean_absolute_error: 0.7985 - val_loss: 1.1367 - val_mean_absolute_error: 0.7895\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1593 - mean_absolute_error: 0.7983 - val_loss: 1.1282 - val_mean_absolute_error: 0.7848\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1611 - mean_absolute_error: 0.7986 - val_loss: 1.1280 - val_mean_absolute_error: 0.7859\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1616 - mean_absolute_error: 0.7991 - val_loss: 1.1253 - val_mean_absolute_error: 0.7883\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1600 - mean_absolute_error: 0.7986 - val_loss: 1.1330 - val_mean_absolute_error: 0.7893\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1588 - mean_absolute_error: 0.7982 - val_loss: 1.1252 - val_mean_absolute_error: 0.7843\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1600 - mean_absolute_error: 0.7982 - val_loss: 1.1247 - val_mean_absolute_error: 0.7852\n",
            "Processing fold 9\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254361 samples, validate on 63591 samples\n",
            "Epoch 1/50\n",
            "254361/254361 [==============================] - 5s 21us/step - loss: 3.4089 - mean_absolute_error: 1.0747 - val_loss: 2.3677 - val_mean_absolute_error: 0.9881\n",
            "Epoch 2/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.8025 - mean_absolute_error: 0.8677 - val_loss: 1.3643 - val_mean_absolute_error: 0.7862\n",
            "Epoch 3/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.3693 - mean_absolute_error: 0.8239 - val_loss: 1.2154 - val_mean_absolute_error: 0.7849\n",
            "Epoch 4/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.2532 - mean_absolute_error: 0.8121 - val_loss: 1.1597 - val_mean_absolute_error: 0.7846\n",
            "Epoch 5/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.2012 - mean_absolute_error: 0.8036 - val_loss: 1.1441 - val_mean_absolute_error: 0.7862\n",
            "Epoch 6/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1846 - mean_absolute_error: 0.8012 - val_loss: 1.1390 - val_mean_absolute_error: 0.7839\n",
            "Epoch 7/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1814 - mean_absolute_error: 0.8008 - val_loss: 1.1347 - val_mean_absolute_error: 0.7851\n",
            "Epoch 8/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1780 - mean_absolute_error: 0.8003 - val_loss: 1.1350 - val_mean_absolute_error: 0.7849\n",
            "Epoch 9/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1762 - mean_absolute_error: 0.8003 - val_loss: 1.1484 - val_mean_absolute_error: 0.7930\n",
            "Epoch 10/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1739 - mean_absolute_error: 0.7998 - val_loss: 1.1317 - val_mean_absolute_error: 0.7845\n",
            "Epoch 11/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1740 - mean_absolute_error: 0.8001 - val_loss: 1.1336 - val_mean_absolute_error: 0.7873\n",
            "Epoch 12/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1735 - mean_absolute_error: 0.7998 - val_loss: 1.1346 - val_mean_absolute_error: 0.7856\n",
            "Epoch 13/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1719 - mean_absolute_error: 0.7997 - val_loss: 1.1419 - val_mean_absolute_error: 0.7909\n",
            "Epoch 14/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1710 - mean_absolute_error: 0.7998 - val_loss: 1.1476 - val_mean_absolute_error: 0.7934\n",
            "Epoch 15/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1695 - mean_absolute_error: 0.7997 - val_loss: 1.1339 - val_mean_absolute_error: 0.7887\n",
            "Epoch 16/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1696 - mean_absolute_error: 0.7998 - val_loss: 1.1312 - val_mean_absolute_error: 0.7865\n",
            "Epoch 17/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1710 - mean_absolute_error: 0.8004 - val_loss: 1.1325 - val_mean_absolute_error: 0.7849\n",
            "Epoch 18/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1692 - mean_absolute_error: 0.7995 - val_loss: 1.1339 - val_mean_absolute_error: 0.7868\n",
            "Epoch 19/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1669 - mean_absolute_error: 0.7992 - val_loss: 1.1256 - val_mean_absolute_error: 0.7863\n",
            "Epoch 20/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1671 - mean_absolute_error: 0.7995 - val_loss: 1.1285 - val_mean_absolute_error: 0.7846\n",
            "Epoch 21/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1662 - mean_absolute_error: 0.7992 - val_loss: 1.1372 - val_mean_absolute_error: 0.7892\n",
            "Epoch 22/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1659 - mean_absolute_error: 0.7992 - val_loss: 1.1265 - val_mean_absolute_error: 0.7861\n",
            "Epoch 23/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1665 - mean_absolute_error: 0.7998 - val_loss: 1.1295 - val_mean_absolute_error: 0.7872\n",
            "Epoch 24/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1652 - mean_absolute_error: 0.7990 - val_loss: 1.1282 - val_mean_absolute_error: 0.7885\n",
            "Epoch 25/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1668 - mean_absolute_error: 0.7995 - val_loss: 1.1268 - val_mean_absolute_error: 0.7873\n",
            "Epoch 26/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1652 - mean_absolute_error: 0.7990 - val_loss: 1.1294 - val_mean_absolute_error: 0.7847\n",
            "Epoch 27/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1642 - mean_absolute_error: 0.7986 - val_loss: 1.1322 - val_mean_absolute_error: 0.7853\n",
            "Epoch 28/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1631 - mean_absolute_error: 0.7986 - val_loss: 1.1313 - val_mean_absolute_error: 0.7901\n",
            "Epoch 29/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1639 - mean_absolute_error: 0.7989 - val_loss: 1.1283 - val_mean_absolute_error: 0.7857\n",
            "Epoch 30/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1646 - mean_absolute_error: 0.7993 - val_loss: 1.1294 - val_mean_absolute_error: 0.7856\n",
            "Epoch 31/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1641 - mean_absolute_error: 0.7994 - val_loss: 1.1463 - val_mean_absolute_error: 0.7957\n",
            "Epoch 32/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1637 - mean_absolute_error: 0.7989 - val_loss: 1.1353 - val_mean_absolute_error: 0.7886\n",
            "Epoch 33/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1635 - mean_absolute_error: 0.7990 - val_loss: 1.1322 - val_mean_absolute_error: 0.7865\n",
            "Epoch 34/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1620 - mean_absolute_error: 0.7988 - val_loss: 1.1379 - val_mean_absolute_error: 0.7883\n",
            "Epoch 35/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1623 - mean_absolute_error: 0.7988 - val_loss: 1.1260 - val_mean_absolute_error: 0.7865\n",
            "Epoch 36/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1623 - mean_absolute_error: 0.7989 - val_loss: 1.1272 - val_mean_absolute_error: 0.7861\n",
            "Epoch 37/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1613 - mean_absolute_error: 0.7981 - val_loss: 1.1423 - val_mean_absolute_error: 0.7902\n",
            "Epoch 38/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1628 - mean_absolute_error: 0.7990 - val_loss: 1.1653 - val_mean_absolute_error: 0.8029\n",
            "Epoch 39/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1620 - mean_absolute_error: 0.7987 - val_loss: 1.1312 - val_mean_absolute_error: 0.7882\n",
            "Epoch 40/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1611 - mean_absolute_error: 0.7988 - val_loss: 1.1284 - val_mean_absolute_error: 0.7871\n",
            "Epoch 41/50\n",
            "254361/254361 [==============================] - 4s 16us/step - loss: 1.1622 - mean_absolute_error: 0.7989 - val_loss: 1.1257 - val_mean_absolute_error: 0.7859\n",
            "Epoch 42/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1602 - mean_absolute_error: 0.7986 - val_loss: 1.1263 - val_mean_absolute_error: 0.7849\n",
            "Epoch 43/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1619 - mean_absolute_error: 0.7987 - val_loss: 1.1288 - val_mean_absolute_error: 0.7859\n",
            "Epoch 44/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1615 - mean_absolute_error: 0.7988 - val_loss: 1.1315 - val_mean_absolute_error: 0.7890\n",
            "Epoch 45/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1599 - mean_absolute_error: 0.7983 - val_loss: 1.1251 - val_mean_absolute_error: 0.7888\n",
            "Epoch 46/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1601 - mean_absolute_error: 0.7983 - val_loss: 1.1296 - val_mean_absolute_error: 0.7870\n",
            "Epoch 47/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1591 - mean_absolute_error: 0.7982 - val_loss: 1.1422 - val_mean_absolute_error: 0.7897\n",
            "Epoch 48/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1600 - mean_absolute_error: 0.7985 - val_loss: 1.1231 - val_mean_absolute_error: 0.7883\n",
            "Epoch 49/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1588 - mean_absolute_error: 0.7982 - val_loss: 1.1262 - val_mean_absolute_error: 0.7858\n",
            "Epoch 50/50\n",
            "254361/254361 [==============================] - 4s 15us/step - loss: 1.1598 - mean_absolute_error: 0.7986 - val_loss: 1.1300 - val_mean_absolute_error: 0.7852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC8W0_ZEXK1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "7ae2242a-cacf-4ac9-890e-61d14c23fc85"
      },
      "source": [
        "# plot the average train/val loss\n",
        "avg_train = [np.mean(i) for i in zip(*train_errs)]\n",
        "avg_val = [np.mean(i) for i in zip(*val_errs)]\n",
        "plot_train_val_error(avg_train, avg_val)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nECIiUpYCQkREylJAiIhIWf8/B3tB8LadbwIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI9CgTDQXRLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ea309e3f-11c2-4010-fe6a-8f752c6a7e01"
      },
      "source": [
        "# plot the train and val loss (smoother curve)\n",
        "smooth_avg_val = smooth_curve(avg_val, factor=0.8)\n",
        "plot_train_val_error(avg_train, smooth_avg_val)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}