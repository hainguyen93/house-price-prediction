{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HousePricePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwGJ-vhbujb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "71778055-5e92-474a-85db-2cb78f6a5afe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.models import Model, Sequential\n",
        "from keras.metrics import mean_absolute_error, mae, mse\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.regularizers import l2, l1_l2\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1v4zYdqej0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdf1066a-a204-44c4-bfa1-7d39a3620366"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erff7ShwIGKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(url, columns=[1, 2, 4, 6, 11]):\n",
        "  df = pd.read_csv(url, header=None, usecols=columns)\n",
        "  print('Data shape: ', np.shape(df))\n",
        "\n",
        "  # re-name all columns\n",
        "  column_names = ['Price', 'PurchaseDate', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  df.columns = column_names\n",
        "  \n",
        "  # resplace column values\n",
        "  df['PropertyType'] = df['PropertyType'].replace({'F':0, 'D':1, 'S':2, 'T':3, 'O':4})\n",
        "  df['LeaseDuration'] = df['LeaseDuration'].replace({'L':0, 'F':1, 'U':2})\n",
        "  df.loc[df['City']=='LONDON', 'City'] = 0\n",
        "  df.loc[df['City'] != 0, 'City'] = 1\n",
        "\n",
        "  # convert column values to appropriate dtype to save memory\n",
        "  df['PurchaseDate'] = pd.to_datetime(df['PurchaseDate'])\n",
        "  df['Price'] = pd.to_numeric(df[\"Price\"], downcast=\"integer\")\n",
        "  df['PropertyType'] = pd.to_numeric(df['PropertyType'], downcast='integer')\n",
        "  df['LeaseDuration'] = pd.to_numeric(df[\"LeaseDuration\"], downcast=\"integer\")\n",
        "  df['City'] = pd.to_numeric(df[\"City\"], downcast=\"integer\")\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByXQfJ7yd3P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(df):\n",
        "  cutoff = datetime(2016, 1, 1)\n",
        "  column_sels = ['Price', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  train_df = df.loc[df['PurchaseDate'] <= cutoff][column_sels]\n",
        "  test_df = df.loc[df['PurchaseDate'] > cutoff][column_sels] \n",
        "  \n",
        "  # remove duplicates\n",
        "  train_df.drop_duplicates(keep='first', inplace=True)\n",
        "  test_df.drop_duplicates(keep='first', inplace=True)\n",
        "\n",
        "  return train_df, test_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kI469W9I02n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_val(train_df):\n",
        "  # split to train and val (~20%)\n",
        "  train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=2019)\n",
        "  print(\"Train shape : \", train_df.shape)\n",
        "  print(\"Test shape : \", test_df.shape)\n",
        "  \n",
        "  return train_df, val_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "807FFQwiJW8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(train_df, val_df, test_df):\n",
        "  # training data\n",
        "  train_X = train_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  train_y = train_df['Price']\n",
        "\n",
        "  val_X = val_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  val_y = val_df['Price']\n",
        "\n",
        "  # testing data\n",
        "  test_X = test_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  test_y = test_df['Price']\n",
        "\n",
        "  # one-hot encoding the inputs\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(train_X)\n",
        "  train_X = ohc.transform(train_X)\n",
        "  val_X = ohc.transform(val_X)\n",
        "  test_X = ohc.transform(test_X)\n",
        "\n",
        "  # convert the targets to smaller range\n",
        "  train_y = np.log1p(train_y * 1e-3)\n",
        "  val_y = np.log1p(val_y * 1e-3)\n",
        "  test_y = np.log1p(test_y * 1e-3)\n",
        "\n",
        "  return (train_X, train_y), (val_X, val_y), (test_X, test_y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur0vmdiu1IDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_size):\n",
        "  \"\"\" Build the model with many fully connected layers\n",
        "  Arguments:\n",
        "    input_size : the size of the input layer\n",
        "  Return: a compiled model\n",
        "  \"\"\"\n",
        "  inp = Input(shape=(input_size,))\n",
        "  fc1 = Dense(100, activation='relu', kernel_regularizer=l2(0.001))(inp)\n",
        "  do1 = Dropout(0.5)(fc1)\n",
        "  fc2 = Dense(200, activation='relu', kernel_regularizer=l2(0.001))(do1)\n",
        "  do2 = Dropout(0.5)(fc2)\n",
        "  fc3 = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(do2)\n",
        "  do3 = Dropout(0.5)(fc3)\n",
        "  fc4 = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(do3)\n",
        "  out = Dense(1)(fc4)\n",
        "\n",
        "  model = Model(inputs=inp, outputs=out)\n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(optimizer=RMSprop(lr=1e-3), loss=mse, metrics=[mae])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwzVEw54HPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, epochs=10, b_size=10000):\n",
        "  \"\"\" Perform model traning\n",
        "  Arguments:\n",
        "    model : a compiled model\n",
        "    epochs : number of epochs (i.e., iterations over dataset)\n",
        "  Return:\n",
        "    the distionary containing training error/loss and val error/loss\n",
        "  \"\"\"\n",
        "  history = model.fit(train_X, train_y, batch_size=b_size, verbose=1,\n",
        "                      epochs=epochs, validation_data=(val_X, val_y))\n",
        "  \n",
        "  train_mae = history.history['mean_absolute_error']\n",
        "  val_mae = history.history['val_mean_absolute_error']\n",
        "  \n",
        "  return train_mae, val_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKWXA96351JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_val_error_plotter(train_mae, val_mae):\n",
        "  \"\"\" Plot the train/val loss over epochs\n",
        "  \"\"\"\n",
        "  epochs = len(train_mae)\n",
        "  plt.plot(range(1, epochs+1), train_mae, label='Training Loss')\n",
        "  plt.plot(range(1, epochs+1), val_mae, label='Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw_71SZg63Ei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42ce83a8-b6f5-4fd4-8ac8-73ce9f3e3668"
      },
      "source": [
        "url = '/content/drive/My Drive/pp-complete.csv'\n",
        "\n",
        "df = load_data(url)\n",
        "train_df, test_df = split_train_test(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape:  (24852949, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E342Lv5ApgmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "22d77a57-f17b-4417-d02d-c13ad17f459d"
      },
      "source": [
        "train_df, val_df = split_train_val(train_df)\n",
        "(train_X, train_y), (val_X, val_y), (test_X, test_y)  = prep(train_df, val_df, test_df)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (254361, 4)\n",
            "Test shape :  (144006, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0r8CEzb9UGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92f949f2-31d6-4745-a240-91de2015da56"
      },
      "source": [
        "# fitting the model using cross validation with k folds\n",
        "k = 10\n",
        "num_samples = np.shape(train_df)[0] // k\n",
        "num_epochs = 500\n",
        "b_size = 512\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('processing fold {0}'.format(i))\n",
        "  val_data = train_df.iloc[i*num_samples:(i+1)*num_samples]\n",
        "  train_data = pd.concat([train_df.iloc[:i*num_samples], train_df.iloc[(i+1)*num_samples:]])\n",
        "\n",
        "  (train_X, train_y), (val_X, val_y), (test_X, test_y) = prep(train_data, val_data, test_df)\n",
        "  model = build_model(np.shape(train_X)[1])\n",
        "  train_mae, val_mae = train_model(model, num_epochs, b_size)\n",
        "  train_errors.append(train_mae)\n",
        "  val_errors.append(val_mae)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "254362/254362 [==============================] - 8s 30us/step - loss: 3.4262 - mean_absolute_error: 1.0560 - val_loss: 1.6949 - val_mean_absolute_error: 0.6675\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.6367 - mean_absolute_error: 0.7986 - val_loss: 1.8474 - val_mean_absolute_error: 1.0238\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1810 - mean_absolute_error: 0.7478 - val_loss: 2.0081 - val_mean_absolute_error: 1.1362\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0798 - mean_absolute_error: 0.7347 - val_loss: 1.7442 - val_mean_absolute_error: 1.0523\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0378 - mean_absolute_error: 0.7273 - val_loss: 1.9720 - val_mean_absolute_error: 1.1405\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0270 - mean_absolute_error: 0.7253 - val_loss: 1.8486 - val_mean_absolute_error: 1.0955\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0255 - mean_absolute_error: 0.7258 - val_loss: 1.9793 - val_mean_absolute_error: 1.1434\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0233 - mean_absolute_error: 0.7256 - val_loss: 1.8183 - val_mean_absolute_error: 1.0859\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0216 - mean_absolute_error: 0.7250 - val_loss: 1.8396 - val_mean_absolute_error: 1.0963\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0196 - mean_absolute_error: 0.7252 - val_loss: 2.0484 - val_mean_absolute_error: 1.1730\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0183 - mean_absolute_error: 0.7252 - val_loss: 1.8847 - val_mean_absolute_error: 1.1092\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0173 - mean_absolute_error: 0.7248 - val_loss: 2.1793 - val_mean_absolute_error: 1.2152\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0168 - mean_absolute_error: 0.7246 - val_loss: 2.1589 - val_mean_absolute_error: 1.2099\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0155 - mean_absolute_error: 0.7247 - val_loss: 2.0010 - val_mean_absolute_error: 1.1456\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0148 - mean_absolute_error: 0.7244 - val_loss: 1.6905 - val_mean_absolute_error: 1.0391\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0149 - mean_absolute_error: 0.7246 - val_loss: 1.9096 - val_mean_absolute_error: 1.1239\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0144 - mean_absolute_error: 0.7243 - val_loss: 1.9060 - val_mean_absolute_error: 1.1225\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0144 - mean_absolute_error: 0.7245 - val_loss: 1.8247 - val_mean_absolute_error: 1.0874\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0121 - mean_absolute_error: 0.7239 - val_loss: 2.0938 - val_mean_absolute_error: 1.1888\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0118 - mean_absolute_error: 0.7240 - val_loss: 2.0655 - val_mean_absolute_error: 1.1777\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0124 - mean_absolute_error: 0.7244 - val_loss: 2.1211 - val_mean_absolute_error: 1.1969\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0102 - mean_absolute_error: 0.7238 - val_loss: 2.0565 - val_mean_absolute_error: 1.1748\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0106 - mean_absolute_error: 0.7238 - val_loss: 2.0072 - val_mean_absolute_error: 1.1581\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0102 - mean_absolute_error: 0.7240 - val_loss: 1.9393 - val_mean_absolute_error: 1.1373\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0115 - mean_absolute_error: 0.7242 - val_loss: 2.0200 - val_mean_absolute_error: 1.1623\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0099 - mean_absolute_error: 0.7241 - val_loss: 1.9726 - val_mean_absolute_error: 1.1490\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0097 - mean_absolute_error: 0.7238 - val_loss: 2.1071 - val_mean_absolute_error: 1.1912\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0100 - mean_absolute_error: 0.7243 - val_loss: 1.8879 - val_mean_absolute_error: 1.1158\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0097 - mean_absolute_error: 0.7244 - val_loss: 2.0177 - val_mean_absolute_error: 1.1621\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0094 - mean_absolute_error: 0.7242 - val_loss: 2.1170 - val_mean_absolute_error: 1.1961\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0076 - mean_absolute_error: 0.7236 - val_loss: 2.0849 - val_mean_absolute_error: 1.1869\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0092 - mean_absolute_error: 0.7244 - val_loss: 1.9261 - val_mean_absolute_error: 1.1307\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0096 - mean_absolute_error: 0.7244 - val_loss: 2.2922 - val_mean_absolute_error: 1.2573\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0081 - mean_absolute_error: 0.7240 - val_loss: 2.2265 - val_mean_absolute_error: 1.2293\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0085 - mean_absolute_error: 0.7237 - val_loss: 2.0048 - val_mean_absolute_error: 1.1581\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0075 - mean_absolute_error: 0.7239 - val_loss: 1.8208 - val_mean_absolute_error: 1.0856\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0087 - mean_absolute_error: 0.7244 - val_loss: 1.9469 - val_mean_absolute_error: 1.1381\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0087 - mean_absolute_error: 0.7239 - val_loss: 1.9928 - val_mean_absolute_error: 1.1523\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0074 - mean_absolute_error: 0.7237 - val_loss: 1.8428 - val_mean_absolute_error: 1.0983\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0074 - mean_absolute_error: 0.7237 - val_loss: 2.1025 - val_mean_absolute_error: 1.1915\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0067 - mean_absolute_error: 0.7239 - val_loss: 2.0007 - val_mean_absolute_error: 1.1579\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0072 - mean_absolute_error: 0.7238 - val_loss: 2.1488 - val_mean_absolute_error: 1.2086\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0072 - mean_absolute_error: 0.7241 - val_loss: 2.0007 - val_mean_absolute_error: 1.1558\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0069 - mean_absolute_error: 0.7237 - val_loss: 2.2496 - val_mean_absolute_error: 1.2435\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0073 - mean_absolute_error: 0.7240 - val_loss: 2.1084 - val_mean_absolute_error: 1.1970\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0066 - mean_absolute_error: 0.7237 - val_loss: 1.8739 - val_mean_absolute_error: 1.1087\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0065 - mean_absolute_error: 0.7237 - val_loss: 2.1138 - val_mean_absolute_error: 1.1998\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0061 - mean_absolute_error: 0.7237 - val_loss: 2.0187 - val_mean_absolute_error: 1.1622\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0055 - mean_absolute_error: 0.7235 - val_loss: 2.1657 - val_mean_absolute_error: 1.2128\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.0055 - mean_absolute_error: 0.7237 - val_loss: 1.8700 - val_mean_absolute_error: 1.1090\n",
            "processing fold 1\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 17us/step - loss: 3.4909 - mean_absolute_error: 1.0598 - val_loss: 1.8487 - val_mean_absolute_error: 0.7468\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.7752 - mean_absolute_error: 0.8431 - val_loss: 1.3946 - val_mean_absolute_error: 0.8506\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.3549 - mean_absolute_error: 0.8111 - val_loss: 1.1924 - val_mean_absolute_error: 0.8238\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2407 - mean_absolute_error: 0.7995 - val_loss: 1.1920 - val_mean_absolute_error: 0.8467\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2026 - mean_absolute_error: 0.7945 - val_loss: 1.1442 - val_mean_absolute_error: 0.8296\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1950 - mean_absolute_error: 0.7932 - val_loss: 1.0486 - val_mean_absolute_error: 0.7844\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1945 - mean_absolute_error: 0.7935 - val_loss: 1.1368 - val_mean_absolute_error: 0.8278\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1939 - mean_absolute_error: 0.7934 - val_loss: 1.0883 - val_mean_absolute_error: 0.8049\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1913 - mean_absolute_error: 0.7928 - val_loss: 1.1938 - val_mean_absolute_error: 0.8542\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1913 - mean_absolute_error: 0.7928 - val_loss: 1.0641 - val_mean_absolute_error: 0.7944\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1882 - mean_absolute_error: 0.7926 - val_loss: 1.0767 - val_mean_absolute_error: 0.7996\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1890 - mean_absolute_error: 0.7927 - val_loss: 1.1058 - val_mean_absolute_error: 0.8143\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1871 - mean_absolute_error: 0.7924 - val_loss: 1.1253 - val_mean_absolute_error: 0.8236\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1871 - mean_absolute_error: 0.7931 - val_loss: 1.0842 - val_mean_absolute_error: 0.8050\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1871 - mean_absolute_error: 0.7929 - val_loss: 1.0531 - val_mean_absolute_error: 0.7896\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1864 - mean_absolute_error: 0.7930 - val_loss: 1.2862 - val_mean_absolute_error: 0.8964\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1861 - mean_absolute_error: 0.7926 - val_loss: 1.1294 - val_mean_absolute_error: 0.8274\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1851 - mean_absolute_error: 0.7921 - val_loss: 1.0938 - val_mean_absolute_error: 0.8096\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1846 - mean_absolute_error: 0.7924 - val_loss: 1.1778 - val_mean_absolute_error: 0.8497\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1842 - mean_absolute_error: 0.7924 - val_loss: 1.2059 - val_mean_absolute_error: 0.8625\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1828 - mean_absolute_error: 0.7922 - val_loss: 1.1298 - val_mean_absolute_error: 0.8277\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.1833 - mean_absolute_error: 0.7921 - val_loss: 1.1940 - val_mean_absolute_error: 0.8575\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.1831 - mean_absolute_error: 0.7923 - val_loss: 1.2123 - val_mean_absolute_error: 0.8654\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.1830 - mean_absolute_error: 0.7922 - val_loss: 1.2454 - val_mean_absolute_error: 0.8800\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1829 - mean_absolute_error: 0.7921 - val_loss: 1.1179 - val_mean_absolute_error: 0.8223\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1825 - mean_absolute_error: 0.7925 - val_loss: 1.0798 - val_mean_absolute_error: 0.8049\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1834 - mean_absolute_error: 0.7928 - val_loss: 1.0883 - val_mean_absolute_error: 0.8072\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1828 - mean_absolute_error: 0.7921 - val_loss: 1.1599 - val_mean_absolute_error: 0.8421\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.1820 - mean_absolute_error: 0.7921 - val_loss: 1.0841 - val_mean_absolute_error: 0.8067\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.1797 - mean_absolute_error: 0.7918 - val_loss: 1.0406 - val_mean_absolute_error: 0.7839\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1796 - mean_absolute_error: 0.7922 - val_loss: 1.1192 - val_mean_absolute_error: 0.8234\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1792 - mean_absolute_error: 0.7914 - val_loss: 1.0637 - val_mean_absolute_error: 0.7968\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1804 - mean_absolute_error: 0.7921 - val_loss: 1.1344 - val_mean_absolute_error: 0.8305\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1811 - mean_absolute_error: 0.7923 - val_loss: 1.1335 - val_mean_absolute_error: 0.8308\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1808 - mean_absolute_error: 0.7919 - val_loss: 1.1187 - val_mean_absolute_error: 0.8239\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1800 - mean_absolute_error: 0.7921 - val_loss: 1.0974 - val_mean_absolute_error: 0.8143\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1793 - mean_absolute_error: 0.7916 - val_loss: 1.0941 - val_mean_absolute_error: 0.8118\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1787 - mean_absolute_error: 0.7920 - val_loss: 1.0501 - val_mean_absolute_error: 0.7906\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1799 - mean_absolute_error: 0.7917 - val_loss: 1.0677 - val_mean_absolute_error: 0.7996\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1779 - mean_absolute_error: 0.7917 - val_loss: 1.0981 - val_mean_absolute_error: 0.8137\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1791 - mean_absolute_error: 0.7918 - val_loss: 1.2091 - val_mean_absolute_error: 0.8646\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1797 - mean_absolute_error: 0.7921 - val_loss: 1.1548 - val_mean_absolute_error: 0.8410\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1769 - mean_absolute_error: 0.7913 - val_loss: 1.1339 - val_mean_absolute_error: 0.8312\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1787 - mean_absolute_error: 0.7917 - val_loss: 1.1365 - val_mean_absolute_error: 0.8331\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1761 - mean_absolute_error: 0.7910 - val_loss: 1.0812 - val_mean_absolute_error: 0.8065\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1769 - mean_absolute_error: 0.7916 - val_loss: 1.0491 - val_mean_absolute_error: 0.7915\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1771 - mean_absolute_error: 0.7916 - val_loss: 1.1756 - val_mean_absolute_error: 0.8511\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1786 - mean_absolute_error: 0.7921 - val_loss: 1.1246 - val_mean_absolute_error: 0.8271\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1764 - mean_absolute_error: 0.7915 - val_loss: 1.1491 - val_mean_absolute_error: 0.8381\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1761 - mean_absolute_error: 0.7912 - val_loss: 1.0738 - val_mean_absolute_error: 0.8033\n",
            "processing fold 2\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 3.4120 - mean_absolute_error: 1.0692 - val_loss: 1.8140 - val_mean_absolute_error: 0.8235\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.7650 - mean_absolute_error: 0.8807 - val_loss: 0.9561 - val_mean_absolute_error: 0.6489\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.4131 - mean_absolute_error: 0.8542 - val_loss: 0.9002 - val_mean_absolute_error: 0.6905\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.3146 - mean_absolute_error: 0.8446 - val_loss: 0.8131 - val_mean_absolute_error: 0.6586\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2836 - mean_absolute_error: 0.8400 - val_loss: 0.7699 - val_mean_absolute_error: 0.6397\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2792 - mean_absolute_error: 0.8402 - val_loss: 0.8225 - val_mean_absolute_error: 0.6733\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2759 - mean_absolute_error: 0.8394 - val_loss: 0.7759 - val_mean_absolute_error: 0.6445\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2726 - mean_absolute_error: 0.8395 - val_loss: 0.7898 - val_mean_absolute_error: 0.6553\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2737 - mean_absolute_error: 0.8398 - val_loss: 0.7599 - val_mean_absolute_error: 0.6392\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2693 - mean_absolute_error: 0.8389 - val_loss: 0.7622 - val_mean_absolute_error: 0.6420\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2687 - mean_absolute_error: 0.8389 - val_loss: 0.7597 - val_mean_absolute_error: 0.6397\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2670 - mean_absolute_error: 0.8385 - val_loss: 0.8120 - val_mean_absolute_error: 0.6707\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2665 - mean_absolute_error: 0.8383 - val_loss: 0.7397 - val_mean_absolute_error: 0.6250\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2670 - mean_absolute_error: 0.8386 - val_loss: 0.7586 - val_mean_absolute_error: 0.6363\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2663 - mean_absolute_error: 0.8387 - val_loss: 0.7551 - val_mean_absolute_error: 0.6374\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2656 - mean_absolute_error: 0.8386 - val_loss: 0.7614 - val_mean_absolute_error: 0.6418\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2658 - mean_absolute_error: 0.8384 - val_loss: 0.7791 - val_mean_absolute_error: 0.6534\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2649 - mean_absolute_error: 0.8382 - val_loss: 0.7610 - val_mean_absolute_error: 0.6404\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2648 - mean_absolute_error: 0.8386 - val_loss: 0.7320 - val_mean_absolute_error: 0.6218\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2638 - mean_absolute_error: 0.8383 - val_loss: 0.7496 - val_mean_absolute_error: 0.6325\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2641 - mean_absolute_error: 0.8385 - val_loss: 0.7539 - val_mean_absolute_error: 0.6354\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2625 - mean_absolute_error: 0.8376 - val_loss: 0.7830 - val_mean_absolute_error: 0.6535\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2620 - mean_absolute_error: 0.8382 - val_loss: 0.7676 - val_mean_absolute_error: 0.6465\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2623 - mean_absolute_error: 0.8380 - val_loss: 0.7662 - val_mean_absolute_error: 0.6452\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2627 - mean_absolute_error: 0.8382 - val_loss: 0.7937 - val_mean_absolute_error: 0.6657\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2616 - mean_absolute_error: 0.8379 - val_loss: 0.7397 - val_mean_absolute_error: 0.6276\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2605 - mean_absolute_error: 0.8377 - val_loss: 0.7669 - val_mean_absolute_error: 0.6474\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2611 - mean_absolute_error: 0.8380 - val_loss: 0.7775 - val_mean_absolute_error: 0.6533\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2622 - mean_absolute_error: 0.8383 - val_loss: 0.7589 - val_mean_absolute_error: 0.6395\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2606 - mean_absolute_error: 0.8380 - val_loss: 0.7510 - val_mean_absolute_error: 0.6374\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2609 - mean_absolute_error: 0.8377 - val_loss: 0.7589 - val_mean_absolute_error: 0.6429\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2603 - mean_absolute_error: 0.8384 - val_loss: 0.7442 - val_mean_absolute_error: 0.6310\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2600 - mean_absolute_error: 0.8377 - val_loss: 0.7474 - val_mean_absolute_error: 0.6344\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2607 - mean_absolute_error: 0.8377 - val_loss: 0.7460 - val_mean_absolute_error: 0.6326\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2614 - mean_absolute_error: 0.8381 - val_loss: 0.7507 - val_mean_absolute_error: 0.6368\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2586 - mean_absolute_error: 0.8369 - val_loss: 0.7320 - val_mean_absolute_error: 0.6240\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2574 - mean_absolute_error: 0.8374 - val_loss: 0.7462 - val_mean_absolute_error: 0.6350\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2596 - mean_absolute_error: 0.8380 - val_loss: 0.7937 - val_mean_absolute_error: 0.6605\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2587 - mean_absolute_error: 0.8377 - val_loss: 0.7427 - val_mean_absolute_error: 0.6317\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2586 - mean_absolute_error: 0.8377 - val_loss: 0.7620 - val_mean_absolute_error: 0.6426\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2582 - mean_absolute_error: 0.8376 - val_loss: 0.7504 - val_mean_absolute_error: 0.6367\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2571 - mean_absolute_error: 0.8371 - val_loss: 0.7339 - val_mean_absolute_error: 0.6271\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2576 - mean_absolute_error: 0.8377 - val_loss: 0.7488 - val_mean_absolute_error: 0.6344\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2571 - mean_absolute_error: 0.8376 - val_loss: 0.7557 - val_mean_absolute_error: 0.6408\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2568 - mean_absolute_error: 0.8377 - val_loss: 0.7839 - val_mean_absolute_error: 0.6560\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2585 - mean_absolute_error: 0.8377 - val_loss: 0.7726 - val_mean_absolute_error: 0.6505\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2582 - mean_absolute_error: 0.8378 - val_loss: 0.7448 - val_mean_absolute_error: 0.6353\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2574 - mean_absolute_error: 0.8377 - val_loss: 0.7387 - val_mean_absolute_error: 0.6324\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2570 - mean_absolute_error: 0.8373 - val_loss: 0.7566 - val_mean_absolute_error: 0.6425\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2559 - mean_absolute_error: 0.8373 - val_loss: 0.7591 - val_mean_absolute_error: 0.6450\n",
            "processing fold 3\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 17us/step - loss: 3.3477 - mean_absolute_error: 1.0737 - val_loss: 2.5170 - val_mean_absolute_error: 1.1416\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.8003 - mean_absolute_error: 0.8865 - val_loss: 1.3049 - val_mean_absolute_error: 0.8089\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.4359 - mean_absolute_error: 0.8577 - val_loss: 0.8915 - val_mean_absolute_error: 0.6563\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.3218 - mean_absolute_error: 0.8474 - val_loss: 0.7757 - val_mean_absolute_error: 0.6222\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2779 - mean_absolute_error: 0.8417 - val_loss: 0.8050 - val_mean_absolute_error: 0.6488\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2701 - mean_absolute_error: 0.8415 - val_loss: 0.8248 - val_mean_absolute_error: 0.6612\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2695 - mean_absolute_error: 0.8419 - val_loss: 0.8180 - val_mean_absolute_error: 0.6531\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2655 - mean_absolute_error: 0.8409 - val_loss: 0.8714 - val_mean_absolute_error: 0.6860\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2664 - mean_absolute_error: 0.8412 - val_loss: 0.7214 - val_mean_absolute_error: 0.6021\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2648 - mean_absolute_error: 0.8410 - val_loss: 0.7985 - val_mean_absolute_error: 0.6502\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2637 - mean_absolute_error: 0.8412 - val_loss: 0.8598 - val_mean_absolute_error: 0.6811\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2614 - mean_absolute_error: 0.8409 - val_loss: 0.8127 - val_mean_absolute_error: 0.6547\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2618 - mean_absolute_error: 0.8407 - val_loss: 0.8345 - val_mean_absolute_error: 0.6694\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2613 - mean_absolute_error: 0.8410 - val_loss: 0.8180 - val_mean_absolute_error: 0.6596\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2611 - mean_absolute_error: 0.8406 - val_loss: 0.8294 - val_mean_absolute_error: 0.6661\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2613 - mean_absolute_error: 0.8408 - val_loss: 0.7832 - val_mean_absolute_error: 0.6412\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2586 - mean_absolute_error: 0.8398 - val_loss: 0.7902 - val_mean_absolute_error: 0.6457\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2587 - mean_absolute_error: 0.8399 - val_loss: 0.8444 - val_mean_absolute_error: 0.6753\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2586 - mean_absolute_error: 0.8402 - val_loss: 0.8927 - val_mean_absolute_error: 0.7013\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2572 - mean_absolute_error: 0.8400 - val_loss: 0.8204 - val_mean_absolute_error: 0.6625\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2579 - mean_absolute_error: 0.8400 - val_loss: 0.7974 - val_mean_absolute_error: 0.6505\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2570 - mean_absolute_error: 0.8402 - val_loss: 0.8206 - val_mean_absolute_error: 0.6627\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2571 - mean_absolute_error: 0.8401 - val_loss: 0.8078 - val_mean_absolute_error: 0.6544\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2559 - mean_absolute_error: 0.8399 - val_loss: 0.8082 - val_mean_absolute_error: 0.6549\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2551 - mean_absolute_error: 0.8398 - val_loss: 0.8603 - val_mean_absolute_error: 0.6852\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2551 - mean_absolute_error: 0.8399 - val_loss: 0.8080 - val_mean_absolute_error: 0.6568\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2542 - mean_absolute_error: 0.8394 - val_loss: 0.8972 - val_mean_absolute_error: 0.7045\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2541 - mean_absolute_error: 0.8396 - val_loss: 0.8889 - val_mean_absolute_error: 0.7003\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2557 - mean_absolute_error: 0.8401 - val_loss: 0.8702 - val_mean_absolute_error: 0.6880\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2539 - mean_absolute_error: 0.8399 - val_loss: 0.8484 - val_mean_absolute_error: 0.6798\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2538 - mean_absolute_error: 0.8398 - val_loss: 0.8403 - val_mean_absolute_error: 0.6768\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2521 - mean_absolute_error: 0.8397 - val_loss: 0.8403 - val_mean_absolute_error: 0.6760\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2529 - mean_absolute_error: 0.8393 - val_loss: 0.9152 - val_mean_absolute_error: 0.7127\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2513 - mean_absolute_error: 0.8391 - val_loss: 0.8122 - val_mean_absolute_error: 0.6589\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2536 - mean_absolute_error: 0.8401 - val_loss: 0.8714 - val_mean_absolute_error: 0.6893\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2526 - mean_absolute_error: 0.8395 - val_loss: 0.8165 - val_mean_absolute_error: 0.6597\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2539 - mean_absolute_error: 0.8397 - val_loss: 0.7866 - val_mean_absolute_error: 0.6446\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2529 - mean_absolute_error: 0.8398 - val_loss: 0.7400 - val_mean_absolute_error: 0.6161\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2513 - mean_absolute_error: 0.8393 - val_loss: 0.8705 - val_mean_absolute_error: 0.6905\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2523 - mean_absolute_error: 0.8395 - val_loss: 0.9151 - val_mean_absolute_error: 0.7181\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2517 - mean_absolute_error: 0.8393 - val_loss: 0.9178 - val_mean_absolute_error: 0.7153\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2506 - mean_absolute_error: 0.8391 - val_loss: 0.8305 - val_mean_absolute_error: 0.6686\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2519 - mean_absolute_error: 0.8395 - val_loss: 0.7780 - val_mean_absolute_error: 0.6389\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2496 - mean_absolute_error: 0.8395 - val_loss: 0.7483 - val_mean_absolute_error: 0.6214\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2509 - mean_absolute_error: 0.8393 - val_loss: 0.7981 - val_mean_absolute_error: 0.6500\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2494 - mean_absolute_error: 0.8393 - val_loss: 0.8180 - val_mean_absolute_error: 0.6628\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2492 - mean_absolute_error: 0.8389 - val_loss: 0.8111 - val_mean_absolute_error: 0.6568\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2493 - mean_absolute_error: 0.8387 - val_loss: 0.7800 - val_mean_absolute_error: 0.6421\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2490 - mean_absolute_error: 0.8390 - val_loss: 0.7520 - val_mean_absolute_error: 0.6268\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.2499 - mean_absolute_error: 0.8392 - val_loss: 0.8913 - val_mean_absolute_error: 0.7020\n",
            "processing fold 4\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 5s 18us/step - loss: 3.1181 - mean_absolute_error: 0.9972 - val_loss: 3.9344 - val_mean_absolute_error: 1.4202\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 1.5317 - mean_absolute_error: 0.7999 - val_loss: 2.3981 - val_mean_absolute_error: 1.0867\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1345 - mean_absolute_error: 0.7682 - val_loss: 2.2888 - val_mean_absolute_error: 1.0911\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0324 - mean_absolute_error: 0.7578 - val_loss: 2.3355 - val_mean_absolute_error: 1.1218\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0057 - mean_absolute_error: 0.7534 - val_loss: 2.5186 - val_mean_absolute_error: 1.1903\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0024 - mean_absolute_error: 0.7534 - val_loss: 2.3193 - val_mean_absolute_error: 1.1151\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0012 - mean_absolute_error: 0.7534 - val_loss: 2.6215 - val_mean_absolute_error: 1.2263\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0007 - mean_absolute_error: 0.7533 - val_loss: 2.4142 - val_mean_absolute_error: 1.1528\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9987 - mean_absolute_error: 0.7524 - val_loss: 2.1661 - val_mean_absolute_error: 1.0611\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9989 - mean_absolute_error: 0.7530 - val_loss: 2.2148 - val_mean_absolute_error: 1.0750\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9985 - mean_absolute_error: 0.7530 - val_loss: 2.4328 - val_mean_absolute_error: 1.1582\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9978 - mean_absolute_error: 0.7532 - val_loss: 2.6995 - val_mean_absolute_error: 1.2505\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9975 - mean_absolute_error: 0.7529 - val_loss: 2.4478 - val_mean_absolute_error: 1.1612\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9971 - mean_absolute_error: 0.7531 - val_loss: 2.2066 - val_mean_absolute_error: 1.0732\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9966 - mean_absolute_error: 0.7530 - val_loss: 2.2191 - val_mean_absolute_error: 1.0792\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9962 - mean_absolute_error: 0.7529 - val_loss: 2.4214 - val_mean_absolute_error: 1.1540\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9961 - mean_absolute_error: 0.7530 - val_loss: 2.2541 - val_mean_absolute_error: 1.0933\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9957 - mean_absolute_error: 0.7528 - val_loss: 2.4447 - val_mean_absolute_error: 1.1639\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9951 - mean_absolute_error: 0.7531 - val_loss: 2.1674 - val_mean_absolute_error: 1.0623\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9952 - mean_absolute_error: 0.7528 - val_loss: 2.7166 - val_mean_absolute_error: 1.2581\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9950 - mean_absolute_error: 0.7530 - val_loss: 2.5511 - val_mean_absolute_error: 1.2002\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9952 - mean_absolute_error: 0.7532 - val_loss: 2.3856 - val_mean_absolute_error: 1.1439\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9955 - mean_absolute_error: 0.7530 - val_loss: 2.3799 - val_mean_absolute_error: 1.1400\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9931 - mean_absolute_error: 0.7525 - val_loss: 2.3667 - val_mean_absolute_error: 1.1362\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9938 - mean_absolute_error: 0.7532 - val_loss: 2.4692 - val_mean_absolute_error: 1.1720\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9929 - mean_absolute_error: 0.7525 - val_loss: 2.5492 - val_mean_absolute_error: 1.1996\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9930 - mean_absolute_error: 0.7524 - val_loss: 2.1912 - val_mean_absolute_error: 1.0715\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9941 - mean_absolute_error: 0.7531 - val_loss: 2.4957 - val_mean_absolute_error: 1.1810\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9932 - mean_absolute_error: 0.7526 - val_loss: 2.3714 - val_mean_absolute_error: 1.1355\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9933 - mean_absolute_error: 0.7528 - val_loss: 2.4712 - val_mean_absolute_error: 1.1714\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9918 - mean_absolute_error: 0.7522 - val_loss: 2.3003 - val_mean_absolute_error: 1.1106\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9922 - mean_absolute_error: 0.7526 - val_loss: 2.3292 - val_mean_absolute_error: 1.1218\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 17us/step - loss: 0.9926 - mean_absolute_error: 0.7526 - val_loss: 2.2233 - val_mean_absolute_error: 1.0841\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9926 - mean_absolute_error: 0.7527 - val_loss: 2.6244 - val_mean_absolute_error: 1.2259\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9919 - mean_absolute_error: 0.7526 - val_loss: 2.3574 - val_mean_absolute_error: 1.1321\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9917 - mean_absolute_error: 0.7525 - val_loss: 2.7152 - val_mean_absolute_error: 1.2568\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9916 - mean_absolute_error: 0.7527 - val_loss: 2.4958 - val_mean_absolute_error: 1.1823\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9918 - mean_absolute_error: 0.7527 - val_loss: 2.4093 - val_mean_absolute_error: 1.1516\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9910 - mean_absolute_error: 0.7526 - val_loss: 2.4587 - val_mean_absolute_error: 1.1690\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9919 - mean_absolute_error: 0.7527 - val_loss: 2.5626 - val_mean_absolute_error: 1.2046\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9908 - mean_absolute_error: 0.7523 - val_loss: 2.2914 - val_mean_absolute_error: 1.1060\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9902 - mean_absolute_error: 0.7523 - val_loss: 2.4321 - val_mean_absolute_error: 1.1577\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9918 - mean_absolute_error: 0.7528 - val_loss: 2.6016 - val_mean_absolute_error: 1.2177\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9918 - mean_absolute_error: 0.7528 - val_loss: 2.4066 - val_mean_absolute_error: 1.1495\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9906 - mean_absolute_error: 0.7525 - val_loss: 2.3397 - val_mean_absolute_error: 1.1280\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9903 - mean_absolute_error: 0.7523 - val_loss: 2.5958 - val_mean_absolute_error: 1.2171\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9894 - mean_absolute_error: 0.7522 - val_loss: 2.5537 - val_mean_absolute_error: 1.2038\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9909 - mean_absolute_error: 0.7526 - val_loss: 2.3832 - val_mean_absolute_error: 1.1414\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 0.9903 - mean_absolute_error: 0.7525 - val_loss: 2.5791 - val_mean_absolute_error: 1.2115\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9899 - mean_absolute_error: 0.7524 - val_loss: 2.4013 - val_mean_absolute_error: 1.1493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0oNXD0wZjUJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c13a4edd-dfd6-43cb-8c0a-8edd8994b3d3"
      },
      "source": [
        "len(train_errors[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi0BN6UdZv0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f6bbda75-cd2b-4ef2-efbe-02e55d900f18"
      },
      "source": [
        "avg_train = [np.mean(i) for i in zip(*train_errors)]\n",
        "avg_val = [np.mean(i) for i in zip(*val_errors)]\n",
        "training_val_error_plotter(avg_train, avg_val)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ1L1C19a7Pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous*factor + point*(1-factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDkFfd1ja8ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "6bf02aa4-30d7-4b48-efa0-eb9617eb08e5"
      },
      "source": [
        "smooth_avg_val = smooth_curve(avg_val, factor=0.8)\n",
        "training_val_error_plotter(avg_train, smooth_avg_val)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpKWEoSIiKT1/wF2P+h+mX91RwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jccWyDYQ9YnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_val_error_plotter(train_mae, val_mae)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-XtNzXSDDzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_mse, test_mae = model.evaluate(test_X, test_y)\n",
        "test_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaGf8LsiD7DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(test_X)\n",
        "preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nj5IaELEEAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGUTmzAETmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voH70odeb5y_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c24f0e1c-d803-4e97-ca92-a8f73a7b0b0a"
      },
      "source": [
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "\n",
        "x_scaled = train_X \n",
        "y = train_y\n",
        "\n",
        "l_regression = linear_model.LinearRegression()\n",
        "kf = KFold(n_splits=10)\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "#x_scaled = min_max_scaler.fit_transform(x)\n",
        "scores = cross_val_score(l_regression, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "print(\"MSE: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))\n",
        "\n",
        "scores_map = {}\n",
        "scores_map['LinearRegression'] = scores\n",
        "l_ridge = linear_model.Ridge()\n",
        "scores = cross_val_score(l_ridge, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['Ridge'] = scores\n",
        "print(\"MSE: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: -1.08883 (+/- 0.01375)\n",
            "MSE: -1.08883 (+/- 0.01375)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKSwsYrxyWrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "#for degree in range(2, 6):\n",
        "#    model = make_pipeline(PolynomialFeatures(degree=degree), linear_model.Ridge())\n",
        "#    scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "#    print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
        "model = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\n",
        "scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['PolyRidge'] = scores\n",
        "print(\"MSE: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0bB3H8hyW64",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "054ef65f-75fd-4d51-caa1-67cdea6b6099"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "desc_tr = DecisionTreeRegressor(max_depth=5)\n",
        "#grid_sv = GridSearchCV(desc_tr, cv=kf, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
        "#grid_sv.fit(x_scaled, y)\n",
        "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
        "scores = cross_val_score(desc_tr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['DecisionTreeRegressor'] = scores\n",
        "print(\"MSE: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: -1.07826 (+/- 0.01391)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13n6ABrKyXIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "knn = KNeighborsRegressor(n_neighbors=7)\n",
        "scores = cross_val_score(knn, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['KNeighborsRegressor'] = scores\n",
        "#grid_sv = GridSearchCV(knn, cv=kf, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
        "#grid_sv.fit(x_scaled, y)\n",
        "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
        "print(\"KNN Accuracy: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgl6PpVqyQzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbr = GradientBoostingRegressor(alpha=0.9, learning_rate=0.05, max_depth=2, \n",
        "                                min_samples_leaf=5, min_samples_split=2, \n",
        "                                n_estimators=100, random_state=30)\n",
        "param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n",
        "grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
        "grid_sv.fit(x_scaled, y)\n",
        "print(\"Best classifier :\", grid_sv.best_estimator_)\n",
        "#scores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "#scores_map['GradientBoostingRegressor'] = scores\n",
        "#print(\"MSE: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAU5_c-ZpCon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n",
        "#grid_sv.fit(x_scaled, y)\n",
        "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
        "scores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "scores_map['SVR'] = scores\n",
        "print(\"MSE: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6xKUnaVyTTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "scores_map = pd.DataFrame(scores_map)\n",
        "sns.boxplot(data=scores_map)\n",
        "print(list(scores_map.keys()))\n",
        "scores_map['DecisionTreeRegressor']"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}