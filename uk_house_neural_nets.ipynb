{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HousePricePrediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImwGJ-vhbujb",
        "colab_type": "code",
        "outputId": "2811ce52-8721-4af1-d44a-b45060e6c625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.models import Model, Sequential\n",
        "from keras.metrics import mean_absolute_error, mae, mse\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.regularizers import l2, l1_l2\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1v4zYdqej0k",
        "colab_type": "code",
        "outputId": "9ba522ef-8bfb-4572-e466-e7504e6e2a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erff7ShwIGKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(url, columns=[1, 2, 4, 6, 11]):\n",
        "  \"\"\" Load the dataset, change the column names, \n",
        "      and replace the categorical data by numeric values\n",
        "  \"\"\"\n",
        "  # load data\n",
        "  df = pd.read_csv(url, header=None, usecols=columns)\n",
        "  print('Data shape: ', np.shape(df))\n",
        "\n",
        "  # re-name all columns\n",
        "  column_names = ['Price', 'PurchaseDate', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  df.columns = column_names\n",
        "  \n",
        "  # resplace column values\n",
        "  df['PropertyType'] = df['PropertyType'].replace({'F':0, 'D':1, 'S':2, 'T':3, 'O':4})\n",
        "  df['LeaseDuration'] = df['LeaseDuration'].replace({'L':0, 'F':1, 'U':2})\n",
        "  df.loc[df['City']=='LONDON', 'City'] = 0\n",
        "  df.loc[df['City'] != 0, 'City'] = 1\n",
        "\n",
        "  # convert column values to appropriate dtype (to save memory)\n",
        "  df['PurchaseDate'] = pd.to_datetime(df['PurchaseDate'])\n",
        "  df['Price'] = pd.to_numeric(df[\"Price\"], downcast=\"integer\")\n",
        "  df['PropertyType'] = pd.to_numeric(df['PropertyType'], downcast='integer')\n",
        "  df['LeaseDuration'] = pd.to_numeric(df[\"LeaseDuration\"], downcast=\"integer\")\n",
        "  df['City'] = pd.to_numeric(df[\"City\"], downcast=\"integer\")\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByXQfJ7yd3P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(df):\n",
        "  \"\"\" Split the data into training and test dataset \n",
        "  \"\"\"\n",
        "  # purchases prior to 1/1/2016 as training\n",
        "  cutoff = datetime(2016, 1, 1)\n",
        "  column_sels = ['Price', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  train_df = df.loc[df['PurchaseDate'] <= cutoff][column_sels]\n",
        "  test_df = df.loc[df['PurchaseDate'] > cutoff][column_sels] \n",
        "  print(\"Train shape: \", train_df.shape)\n",
        "  print(\"Test shape: \", test_df.shape)\n",
        "  \n",
        "  # remove duplicates\n",
        "  train_df.drop_duplicates(keep='first', inplace=True)\n",
        "  test_df.drop_duplicates(keep='first', inplace=True)\n",
        "  print(\"After duplicates removed: \\n\\tTrain shape: \", train_df.shape)\n",
        "  print(\"\\tTest shape: \", test_df.shape)\n",
        "\n",
        "  return train_df, test_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kI469W9I02n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_val(train_df, perc=0.2):\n",
        "  \"\"\" Split the training into (partial) train and val data (~20% default)\n",
        "  \"\"\"\n",
        "  # split to train and val\n",
        "  train_df, val_df = train_test_split(train_df, test_size=perc, random_state=2019)\n",
        "  print(\"Train shape : \", train_df.shape)\n",
        "  print(\"Test shape : \", test_df.shape)\n",
        "  \n",
        "  return train_df, val_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "807FFQwiJW8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(train_df, val_df, test_df):\n",
        "  \"\"\" Prepare inputs/targets pair for training, val, and testing\n",
        "      using one-hot encoding (for categorical data), and \n",
        "      down-scale the target values (prices) \n",
        "  \"\"\"\n",
        "  # training data\n",
        "  train_X = train_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  train_y = train_df['Price']\n",
        "\n",
        "  val_X = val_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  val_y = val_df['Price']\n",
        "\n",
        "  # testing data\n",
        "  test_X = test_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  test_y = test_df['Price']\n",
        "\n",
        "  # one-hot encoding the inputs\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(train_X)\n",
        "  train_X = ohc.transform(train_X)\n",
        "  val_X = ohc.transform(val_X)\n",
        "  test_X = ohc.transform(test_X)\n",
        "\n",
        "  # convert the targets to smaller range\n",
        "  train_y = np.log1p(train_y * 1e-3)\n",
        "  val_y = np.log1p(val_y * 1e-3)\n",
        "  test_y = np.log1p(test_y * 1e-3)\n",
        "\n",
        "  return (train_X, train_y), (val_X, val_y), (test_X, test_y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur0vmdiu1IDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_size, l_rate=1e-3):\n",
        "  \"\"\" Build and compile a fully-connected neural network \n",
        "  \"\"\"\n",
        "  inp = Input(shape=(input_size,))\n",
        "  fc1 = Dense(100, activation='relu', kernel_regularizer=l2(0.001))(inp)\n",
        "  do1 = Dropout(0.5)(fc1)\n",
        "  fc2 = Dense(200, activation='relu', kernel_regularizer=l2(0.001))(do1)\n",
        "  do2 = Dropout(0.5)(fc2)\n",
        "  fc3 = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(do2)\n",
        "  do3 = Dropout(0.5)(fc3)\n",
        "  fc4 = Dense(100, activation='relu', kernel_regularizer=l1_l2(l1=0.001, l2=0.001))(do3)\n",
        "  out = Dense(1)(fc4)\n",
        "\n",
        "  model = Model(inputs=inp, outputs=out)\n",
        "  print(model.summary())\n",
        "  model.compile(optimizer=RMSprop(lr=l_rate), loss=mse, metrics=[mae])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygwzVEw54HPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, epchs=50, b_size=512, v_bose=1):\n",
        "  \"\"\" Train a compiled model\n",
        "  \"\"\"\n",
        "  history = model.fit(train_X, train_y, batch_size=b_size, verbose=v_bose,\n",
        "                      epochs=epchs, validation_data=(val_X, val_y))\n",
        "  \n",
        "  train_mae = history.history['mean_absolute_error']\n",
        "  val_mae = history.history['val_mean_absolute_error']\n",
        "  \n",
        "  return train_mae, val_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKWXA96351JL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_train_val_error(train_errs, val_errs):\n",
        "  \"\"\" Plot the train/val loss over epochs\n",
        "  \"\"\"\n",
        "  epochs = len(train_errs)\n",
        "  plt.plot(range(1, epochs+1), train_errs, label='Training Loss')\n",
        "  plt.plot(range(1, epochs+1), val_errs, label='Validation Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JibXm2MV__dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "  \"\"\" Smooth with an exponential moving average\n",
        "  \"\"\"\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous*factor + point*(1-factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GMP6Xv-AVse",
        "colab_type": "text"
      },
      "source": [
        "# **Loading and Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw_71SZg63Ei",
        "colab_type": "code",
        "outputId": "867f7537-1d54-424e-b5e3-dd38b39fb0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# load and pre-processing\n",
        "url = '/content/drive/My Drive/pp-complete.csv'\n",
        "df = load_data(url)\n",
        "train_df, test_df = split_train_test(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape:  (24852949, 5)\n",
            "Train shape:  (21018083, 4)\n",
            "Test shape:  (3834866, 4)\n",
            "After duplicates removed. \n",
            "\tTrain shape:  (317952, 4)\n",
            "\tTest shape:  (144006, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8AZDcDkAbka",
        "colab_type": "text"
      },
      "source": [
        "# **Training the model with k-fold cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0r8CEzb9UGt",
        "colab_type": "code",
        "outputId": "362808f4-392a-48b9-c2d3-548b804683df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# fitting the model using cross validation with k folds\n",
        "k = 5\n",
        "num_samples = train_df.shape[0] // k\n",
        "num_epochs = 50\n",
        "b_size = 512\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('Processing fold {0}'.format(i))\n",
        "\n",
        "  # prepare train and val data\n",
        "  val_data = train_df.iloc[i*num_samples: (i+1)*num_samples]\n",
        "  train_data = pd.concat([train_df.iloc[:i*num_samples], train_df.iloc[(i+1)*num_samples:]])\n",
        "  (train_X, train_y), (val_X, val_y), _ = prep(train_data, val_data, test_df)\n",
        "\n",
        "  # build a new model\n",
        "  model = build_model(train_X.shape[1])\n",
        "  train_mae, val_mae = train_model(model, num_epochs, b_size, v_bose=1)\n",
        "\n",
        "  # append train/val errors to error lists\n",
        "  train_errors.append(train_mae)\n",
        "  val_errors.append(val_mae)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing fold 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "254362/254362 [==============================] - 8s 32us/step - loss: 3.4329 - mean_absolute_error: 1.0395 - val_loss: 2.2356 - val_mean_absolute_error: 0.8557\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.7552 - mean_absolute_error: 0.8010 - val_loss: 2.0755 - val_mean_absolute_error: 1.0852\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2206 - mean_absolute_error: 0.7488 - val_loss: 2.1430 - val_mean_absolute_error: 1.1769\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0877 - mean_absolute_error: 0.7347 - val_loss: 2.0375 - val_mean_absolute_error: 1.1642\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0363 - mean_absolute_error: 0.7267 - val_loss: 1.8471 - val_mean_absolute_error: 1.0985\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0281 - mean_absolute_error: 0.7261 - val_loss: 2.1895 - val_mean_absolute_error: 1.2159\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0253 - mean_absolute_error: 0.7257 - val_loss: 2.0330 - val_mean_absolute_error: 1.1617\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0235 - mean_absolute_error: 0.7262 - val_loss: 1.9863 - val_mean_absolute_error: 1.1512\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0222 - mean_absolute_error: 0.7257 - val_loss: 1.9080 - val_mean_absolute_error: 1.1203\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0198 - mean_absolute_error: 0.7256 - val_loss: 1.9819 - val_mean_absolute_error: 1.1404\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0191 - mean_absolute_error: 0.7254 - val_loss: 2.1994 - val_mean_absolute_error: 1.2275\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0167 - mean_absolute_error: 0.7250 - val_loss: 2.0360 - val_mean_absolute_error: 1.1631\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0175 - mean_absolute_error: 0.7253 - val_loss: 1.8495 - val_mean_absolute_error: 1.0992\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0161 - mean_absolute_error: 0.7250 - val_loss: 2.0168 - val_mean_absolute_error: 1.1592\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0162 - mean_absolute_error: 0.7250 - val_loss: 2.0884 - val_mean_absolute_error: 1.1869\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0158 - mean_absolute_error: 0.7248 - val_loss: 2.0509 - val_mean_absolute_error: 1.1679\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0147 - mean_absolute_error: 0.7245 - val_loss: 1.9080 - val_mean_absolute_error: 1.1217\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0148 - mean_absolute_error: 0.7246 - val_loss: 2.1841 - val_mean_absolute_error: 1.2213\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0130 - mean_absolute_error: 0.7245 - val_loss: 1.7535 - val_mean_absolute_error: 1.0612\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0132 - mean_absolute_error: 0.7247 - val_loss: 1.8784 - val_mean_absolute_error: 1.1124\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0119 - mean_absolute_error: 0.7244 - val_loss: 2.0545 - val_mean_absolute_error: 1.1758\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0111 - mean_absolute_error: 0.7240 - val_loss: 1.9261 - val_mean_absolute_error: 1.1295\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0108 - mean_absolute_error: 0.7245 - val_loss: 2.1267 - val_mean_absolute_error: 1.1974\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0114 - mean_absolute_error: 0.7242 - val_loss: 2.0660 - val_mean_absolute_error: 1.1778\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0108 - mean_absolute_error: 0.7241 - val_loss: 1.9590 - val_mean_absolute_error: 1.1419\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0099 - mean_absolute_error: 0.7239 - val_loss: 2.1897 - val_mean_absolute_error: 1.2220\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0105 - mean_absolute_error: 0.7244 - val_loss: 1.9265 - val_mean_absolute_error: 1.1306\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0111 - mean_absolute_error: 0.7246 - val_loss: 2.0168 - val_mean_absolute_error: 1.1647\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0085 - mean_absolute_error: 0.7238 - val_loss: 1.8637 - val_mean_absolute_error: 1.1055\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0083 - mean_absolute_error: 0.7237 - val_loss: 1.8680 - val_mean_absolute_error: 1.1061\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0098 - mean_absolute_error: 0.7244 - val_loss: 1.9658 - val_mean_absolute_error: 1.1440\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0079 - mean_absolute_error: 0.7237 - val_loss: 2.2208 - val_mean_absolute_error: 1.2335\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0095 - mean_absolute_error: 0.7242 - val_loss: 1.9499 - val_mean_absolute_error: 1.1380\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0075 - mean_absolute_error: 0.7236 - val_loss: 2.3018 - val_mean_absolute_error: 1.2613\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0077 - mean_absolute_error: 0.7238 - val_loss: 1.8911 - val_mean_absolute_error: 1.1138\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0079 - mean_absolute_error: 0.7237 - val_loss: 1.9991 - val_mean_absolute_error: 1.1535\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0085 - mean_absolute_error: 0.7237 - val_loss: 2.0383 - val_mean_absolute_error: 1.1672\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0090 - mean_absolute_error: 0.7240 - val_loss: 2.0674 - val_mean_absolute_error: 1.1777\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0071 - mean_absolute_error: 0.7239 - val_loss: 2.2402 - val_mean_absolute_error: 1.2382\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0066 - mean_absolute_error: 0.7235 - val_loss: 2.1289 - val_mean_absolute_error: 1.2008\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0081 - mean_absolute_error: 0.7241 - val_loss: 1.8976 - val_mean_absolute_error: 1.1205\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0062 - mean_absolute_error: 0.7236 - val_loss: 2.1592 - val_mean_absolute_error: 1.2123\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0067 - mean_absolute_error: 0.7238 - val_loss: 1.9890 - val_mean_absolute_error: 1.1571\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0053 - mean_absolute_error: 0.7236 - val_loss: 2.1515 - val_mean_absolute_error: 1.2096\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0062 - mean_absolute_error: 0.7240 - val_loss: 1.8586 - val_mean_absolute_error: 1.1059\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0051 - mean_absolute_error: 0.7234 - val_loss: 2.0593 - val_mean_absolute_error: 1.1774\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0061 - mean_absolute_error: 0.7236 - val_loss: 1.8994 - val_mean_absolute_error: 1.1197\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0053 - mean_absolute_error: 0.7235 - val_loss: 1.8218 - val_mean_absolute_error: 1.0900\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0052 - mean_absolute_error: 0.7234 - val_loss: 2.3719 - val_mean_absolute_error: 1.2787\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.0062 - mean_absolute_error: 0.7238 - val_loss: 1.9761 - val_mean_absolute_error: 1.1499\n",
            "Processing fold 1\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 3.4937 - mean_absolute_error: 1.0595 - val_loss: 1.9943 - val_mean_absolute_error: 0.7963\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.8440 - mean_absolute_error: 0.8519 - val_loss: 1.5684 - val_mean_absolute_error: 0.9110\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.3732 - mean_absolute_error: 0.8116 - val_loss: 1.2312 - val_mean_absolute_error: 0.8379\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2474 - mean_absolute_error: 0.8005 - val_loss: 1.1360 - val_mean_absolute_error: 0.8207\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2060 - mean_absolute_error: 0.7948 - val_loss: 1.1537 - val_mean_absolute_error: 0.8327\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1998 - mean_absolute_error: 0.7942 - val_loss: 1.0673 - val_mean_absolute_error: 0.7926\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1953 - mean_absolute_error: 0.7936 - val_loss: 1.1021 - val_mean_absolute_error: 0.8106\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.1932 - mean_absolute_error: 0.7932 - val_loss: 1.0613 - val_mean_absolute_error: 0.7913\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1929 - mean_absolute_error: 0.7933 - val_loss: 1.1613 - val_mean_absolute_error: 0.8393\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1908 - mean_absolute_error: 0.7930 - val_loss: 1.0943 - val_mean_absolute_error: 0.8080\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1901 - mean_absolute_error: 0.7928 - val_loss: 1.0806 - val_mean_absolute_error: 0.8021\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1908 - mean_absolute_error: 0.7932 - val_loss: 1.1631 - val_mean_absolute_error: 0.8413\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1894 - mean_absolute_error: 0.7931 - val_loss: 1.1963 - val_mean_absolute_error: 0.8563\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1883 - mean_absolute_error: 0.7923 - val_loss: 1.1036 - val_mean_absolute_error: 0.8136\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1886 - mean_absolute_error: 0.7922 - val_loss: 1.0798 - val_mean_absolute_error: 0.8032\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1872 - mean_absolute_error: 0.7927 - val_loss: 1.1420 - val_mean_absolute_error: 0.8325\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1865 - mean_absolute_error: 0.7923 - val_loss: 1.1610 - val_mean_absolute_error: 0.8411\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1850 - mean_absolute_error: 0.7923 - val_loss: 1.1365 - val_mean_absolute_error: 0.8297\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1847 - mean_absolute_error: 0.7921 - val_loss: 1.1233 - val_mean_absolute_error: 0.8243\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1853 - mean_absolute_error: 0.7925 - val_loss: 1.1072 - val_mean_absolute_error: 0.8165\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1862 - mean_absolute_error: 0.7929 - val_loss: 1.2116 - val_mean_absolute_error: 0.8643\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1859 - mean_absolute_error: 0.7927 - val_loss: 1.1711 - val_mean_absolute_error: 0.8460\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1828 - mean_absolute_error: 0.7921 - val_loss: 1.1876 - val_mean_absolute_error: 0.8532\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1837 - mean_absolute_error: 0.7926 - val_loss: 1.1349 - val_mean_absolute_error: 0.8299\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1835 - mean_absolute_error: 0.7923 - val_loss: 1.0877 - val_mean_absolute_error: 0.8076\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1822 - mean_absolute_error: 0.7919 - val_loss: 1.0995 - val_mean_absolute_error: 0.8138\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1824 - mean_absolute_error: 0.7920 - val_loss: 1.0526 - val_mean_absolute_error: 0.7925\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1800 - mean_absolute_error: 0.7921 - val_loss: 1.1196 - val_mean_absolute_error: 0.8229\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1822 - mean_absolute_error: 0.7924 - val_loss: 1.2881 - val_mean_absolute_error: 0.8988\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1805 - mean_absolute_error: 0.7918 - val_loss: 1.1181 - val_mean_absolute_error: 0.8232\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1814 - mean_absolute_error: 0.7922 - val_loss: 1.1092 - val_mean_absolute_error: 0.8192\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1806 - mean_absolute_error: 0.7922 - val_loss: 1.1030 - val_mean_absolute_error: 0.8156\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1796 - mean_absolute_error: 0.7916 - val_loss: 1.0328 - val_mean_absolute_error: 0.7822\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1796 - mean_absolute_error: 0.7918 - val_loss: 1.0675 - val_mean_absolute_error: 0.7991\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1792 - mean_absolute_error: 0.7917 - val_loss: 1.1543 - val_mean_absolute_error: 0.8402\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1798 - mean_absolute_error: 0.7915 - val_loss: 1.0541 - val_mean_absolute_error: 0.7927\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1792 - mean_absolute_error: 0.7922 - val_loss: 1.0904 - val_mean_absolute_error: 0.8112\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1776 - mean_absolute_error: 0.7915 - val_loss: 1.0556 - val_mean_absolute_error: 0.7947\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1791 - mean_absolute_error: 0.7918 - val_loss: 1.1037 - val_mean_absolute_error: 0.8173\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1791 - mean_absolute_error: 0.7921 - val_loss: 1.1589 - val_mean_absolute_error: 0.8424\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1791 - mean_absolute_error: 0.7920 - val_loss: 1.0863 - val_mean_absolute_error: 0.8079\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1787 - mean_absolute_error: 0.7918 - val_loss: 1.1276 - val_mean_absolute_error: 0.8285\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1774 - mean_absolute_error: 0.7915 - val_loss: 1.1258 - val_mean_absolute_error: 0.8276\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1776 - mean_absolute_error: 0.7918 - val_loss: 1.1498 - val_mean_absolute_error: 0.8382\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1756 - mean_absolute_error: 0.7911 - val_loss: 1.1355 - val_mean_absolute_error: 0.8324\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1783 - mean_absolute_error: 0.7918 - val_loss: 1.1428 - val_mean_absolute_error: 0.8354\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1777 - mean_absolute_error: 0.7919 - val_loss: 1.1630 - val_mean_absolute_error: 0.8447\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1786 - mean_absolute_error: 0.7924 - val_loss: 1.0612 - val_mean_absolute_error: 0.7966\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1773 - mean_absolute_error: 0.7917 - val_loss: 1.1329 - val_mean_absolute_error: 0.8315\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1770 - mean_absolute_error: 0.7918 - val_loss: 1.1280 - val_mean_absolute_error: 0.8289\n",
            "Processing fold 2\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 3.4875 - mean_absolute_error: 1.0860 - val_loss: 1.8748 - val_mean_absolute_error: 0.8106\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.8545 - mean_absolute_error: 0.8868 - val_loss: 0.9456 - val_mean_absolute_error: 0.6269\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.4309 - mean_absolute_error: 0.8551 - val_loss: 0.8967 - val_mean_absolute_error: 0.6844\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.3234 - mean_absolute_error: 0.8458 - val_loss: 0.7648 - val_mean_absolute_error: 0.6287\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2866 - mean_absolute_error: 0.8408 - val_loss: 0.7645 - val_mean_absolute_error: 0.6355\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2796 - mean_absolute_error: 0.8404 - val_loss: 0.7731 - val_mean_absolute_error: 0.6432\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2760 - mean_absolute_error: 0.8393 - val_loss: 0.7651 - val_mean_absolute_error: 0.6380\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2740 - mean_absolute_error: 0.8395 - val_loss: 0.7699 - val_mean_absolute_error: 0.6438\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2733 - mean_absolute_error: 0.8391 - val_loss: 0.7550 - val_mean_absolute_error: 0.6333\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2699 - mean_absolute_error: 0.8387 - val_loss: 0.7567 - val_mean_absolute_error: 0.6353\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2702 - mean_absolute_error: 0.8391 - val_loss: 0.7628 - val_mean_absolute_error: 0.6393\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2676 - mean_absolute_error: 0.8384 - val_loss: 0.7794 - val_mean_absolute_error: 0.6504\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2682 - mean_absolute_error: 0.8386 - val_loss: 0.7563 - val_mean_absolute_error: 0.6370\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2674 - mean_absolute_error: 0.8385 - val_loss: 0.7644 - val_mean_absolute_error: 0.6405\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2658 - mean_absolute_error: 0.8382 - val_loss: 0.7711 - val_mean_absolute_error: 0.6472\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2660 - mean_absolute_error: 0.8385 - val_loss: 0.7480 - val_mean_absolute_error: 0.6331\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2638 - mean_absolute_error: 0.8380 - val_loss: 0.7636 - val_mean_absolute_error: 0.6414\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2661 - mean_absolute_error: 0.8386 - val_loss: 0.7451 - val_mean_absolute_error: 0.6309\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2647 - mean_absolute_error: 0.8384 - val_loss: 0.7803 - val_mean_absolute_error: 0.6541\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2655 - mean_absolute_error: 0.8386 - val_loss: 0.7702 - val_mean_absolute_error: 0.6458\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2625 - mean_absolute_error: 0.8379 - val_loss: 0.7621 - val_mean_absolute_error: 0.6430\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2646 - mean_absolute_error: 0.8388 - val_loss: 0.7521 - val_mean_absolute_error: 0.6335\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2623 - mean_absolute_error: 0.8380 - val_loss: 0.7346 - val_mean_absolute_error: 0.6257\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2623 - mean_absolute_error: 0.8381 - val_loss: 0.7488 - val_mean_absolute_error: 0.6372\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2616 - mean_absolute_error: 0.8381 - val_loss: 0.7508 - val_mean_absolute_error: 0.6347\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2627 - mean_absolute_error: 0.8383 - val_loss: 0.7532 - val_mean_absolute_error: 0.6377\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2609 - mean_absolute_error: 0.8374 - val_loss: 0.7886 - val_mean_absolute_error: 0.6613\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2597 - mean_absolute_error: 0.8380 - val_loss: 0.7596 - val_mean_absolute_error: 0.6422\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2602 - mean_absolute_error: 0.8379 - val_loss: 0.7750 - val_mean_absolute_error: 0.6523\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2607 - mean_absolute_error: 0.8380 - val_loss: 0.7918 - val_mean_absolute_error: 0.6632\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2576 - mean_absolute_error: 0.8371 - val_loss: 0.7533 - val_mean_absolute_error: 0.6375\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2591 - mean_absolute_error: 0.8376 - val_loss: 0.7583 - val_mean_absolute_error: 0.6395\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2603 - mean_absolute_error: 0.8381 - val_loss: 0.7474 - val_mean_absolute_error: 0.6354\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2578 - mean_absolute_error: 0.8372 - val_loss: 0.7659 - val_mean_absolute_error: 0.6453\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2584 - mean_absolute_error: 0.8374 - val_loss: 0.7559 - val_mean_absolute_error: 0.6387\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2598 - mean_absolute_error: 0.8379 - val_loss: 0.7399 - val_mean_absolute_error: 0.6301\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2577 - mean_absolute_error: 0.8372 - val_loss: 0.7481 - val_mean_absolute_error: 0.6355\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2594 - mean_absolute_error: 0.8379 - val_loss: 0.7433 - val_mean_absolute_error: 0.6342\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2584 - mean_absolute_error: 0.8376 - val_loss: 0.7482 - val_mean_absolute_error: 0.6364\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2572 - mean_absolute_error: 0.8375 - val_loss: 0.7750 - val_mean_absolute_error: 0.6525\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2570 - mean_absolute_error: 0.8372 - val_loss: 0.7538 - val_mean_absolute_error: 0.6391\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2556 - mean_absolute_error: 0.8369 - val_loss: 0.7420 - val_mean_absolute_error: 0.6323\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2563 - mean_absolute_error: 0.8375 - val_loss: 0.7829 - val_mean_absolute_error: 0.6563\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2558 - mean_absolute_error: 0.8375 - val_loss: 0.7815 - val_mean_absolute_error: 0.6572\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2550 - mean_absolute_error: 0.8369 - val_loss: 0.7651 - val_mean_absolute_error: 0.6434\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2555 - mean_absolute_error: 0.8372 - val_loss: 0.7718 - val_mean_absolute_error: 0.6525\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2555 - mean_absolute_error: 0.8370 - val_loss: 0.7389 - val_mean_absolute_error: 0.6321\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2550 - mean_absolute_error: 0.8375 - val_loss: 0.7501 - val_mean_absolute_error: 0.6375\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2566 - mean_absolute_error: 0.8372 - val_loss: 0.7591 - val_mean_absolute_error: 0.6430\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2546 - mean_absolute_error: 0.8370 - val_loss: 0.7693 - val_mean_absolute_error: 0.6506\n",
            "Processing fold 3\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 16us/step - loss: 3.4781 - mean_absolute_error: 1.0865 - val_loss: 2.1717 - val_mean_absolute_error: 0.9452\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.8612 - mean_absolute_error: 0.8928 - val_loss: 1.3362 - val_mean_absolute_error: 0.8247\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.4335 - mean_absolute_error: 0.8585 - val_loss: 1.1860 - val_mean_absolute_error: 0.8170\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.3228 - mean_absolute_error: 0.8485 - val_loss: 0.8910 - val_mean_absolute_error: 0.6828\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2826 - mean_absolute_error: 0.8429 - val_loss: 0.8743 - val_mean_absolute_error: 0.6852\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2749 - mean_absolute_error: 0.8416 - val_loss: 0.9592 - val_mean_absolute_error: 0.7267\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2710 - mean_absolute_error: 0.8416 - val_loss: 0.8262 - val_mean_absolute_error: 0.6598\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2677 - mean_absolute_error: 0.8409 - val_loss: 0.7335 - val_mean_absolute_error: 0.6074\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2655 - mean_absolute_error: 0.8404 - val_loss: 0.7986 - val_mean_absolute_error: 0.6465\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2653 - mean_absolute_error: 0.8412 - val_loss: 0.8893 - val_mean_absolute_error: 0.6960\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2617 - mean_absolute_error: 0.8402 - val_loss: 0.7127 - val_mean_absolute_error: 0.5974\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2626 - mean_absolute_error: 0.8404 - val_loss: 0.9221 - val_mean_absolute_error: 0.7153\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2628 - mean_absolute_error: 0.8406 - val_loss: 0.8863 - val_mean_absolute_error: 0.6981\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2597 - mean_absolute_error: 0.8402 - val_loss: 0.7562 - val_mean_absolute_error: 0.6254\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2597 - mean_absolute_error: 0.8401 - val_loss: 0.8582 - val_mean_absolute_error: 0.6801\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2605 - mean_absolute_error: 0.8407 - val_loss: 0.8039 - val_mean_absolute_error: 0.6524\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2588 - mean_absolute_error: 0.8402 - val_loss: 0.8593 - val_mean_absolute_error: 0.6826\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2588 - mean_absolute_error: 0.8406 - val_loss: 0.9271 - val_mean_absolute_error: 0.7220\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2584 - mean_absolute_error: 0.8404 - val_loss: 0.7935 - val_mean_absolute_error: 0.6470\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2565 - mean_absolute_error: 0.8400 - val_loss: 0.8248 - val_mean_absolute_error: 0.6639\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2554 - mean_absolute_error: 0.8398 - val_loss: 0.8366 - val_mean_absolute_error: 0.6713\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2573 - mean_absolute_error: 0.8402 - val_loss: 0.8720 - val_mean_absolute_error: 0.6902\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2558 - mean_absolute_error: 0.8399 - val_loss: 0.8000 - val_mean_absolute_error: 0.6549\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2574 - mean_absolute_error: 0.8401 - val_loss: 0.8710 - val_mean_absolute_error: 0.6907\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2543 - mean_absolute_error: 0.8397 - val_loss: 0.8089 - val_mean_absolute_error: 0.6574\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2538 - mean_absolute_error: 0.8395 - val_loss: 0.8490 - val_mean_absolute_error: 0.6787\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2542 - mean_absolute_error: 0.8399 - val_loss: 0.8055 - val_mean_absolute_error: 0.6541\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2556 - mean_absolute_error: 0.8400 - val_loss: 0.7855 - val_mean_absolute_error: 0.6431\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2528 - mean_absolute_error: 0.8391 - val_loss: 0.7730 - val_mean_absolute_error: 0.6344\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2541 - mean_absolute_error: 0.8395 - val_loss: 0.7561 - val_mean_absolute_error: 0.6276\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2559 - mean_absolute_error: 0.8403 - val_loss: 0.7732 - val_mean_absolute_error: 0.6371\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2533 - mean_absolute_error: 0.8396 - val_loss: 0.8478 - val_mean_absolute_error: 0.6777\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2530 - mean_absolute_error: 0.8391 - val_loss: 0.7510 - val_mean_absolute_error: 0.6239\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2524 - mean_absolute_error: 0.8395 - val_loss: 0.7566 - val_mean_absolute_error: 0.6271\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2529 - mean_absolute_error: 0.8396 - val_loss: 0.8280 - val_mean_absolute_error: 0.6702\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2505 - mean_absolute_error: 0.8392 - val_loss: 0.8638 - val_mean_absolute_error: 0.6880\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2515 - mean_absolute_error: 0.8395 - val_loss: 0.8341 - val_mean_absolute_error: 0.6718\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2521 - mean_absolute_error: 0.8395 - val_loss: 0.9507 - val_mean_absolute_error: 0.7339\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2507 - mean_absolute_error: 0.8387 - val_loss: 0.7770 - val_mean_absolute_error: 0.6396\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2518 - mean_absolute_error: 0.8395 - val_loss: 0.7973 - val_mean_absolute_error: 0.6508\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2515 - mean_absolute_error: 0.8394 - val_loss: 0.8357 - val_mean_absolute_error: 0.6738\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2501 - mean_absolute_error: 0.8391 - val_loss: 0.7499 - val_mean_absolute_error: 0.6236\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2501 - mean_absolute_error: 0.8395 - val_loss: 0.8898 - val_mean_absolute_error: 0.6986\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2492 - mean_absolute_error: 0.8390 - val_loss: 0.9204 - val_mean_absolute_error: 0.7164\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2498 - mean_absolute_error: 0.8396 - val_loss: 0.8462 - val_mean_absolute_error: 0.6817\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.2495 - mean_absolute_error: 0.8394 - val_loss: 0.7702 - val_mean_absolute_error: 0.6334\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2498 - mean_absolute_error: 0.8392 - val_loss: 0.7867 - val_mean_absolute_error: 0.6452\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2496 - mean_absolute_error: 0.8394 - val_loss: 0.8782 - val_mean_absolute_error: 0.6968\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2488 - mean_absolute_error: 0.8390 - val_loss: 0.7738 - val_mean_absolute_error: 0.6390\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.2487 - mean_absolute_error: 0.8390 - val_loss: 0.7559 - val_mean_absolute_error: 0.6293\n",
            "Processing fold 4\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 254362 samples, validate on 63590 samples\n",
            "Epoch 1/50\n",
            "254362/254362 [==============================] - 4s 17us/step - loss: 3.0564 - mean_absolute_error: 0.9944 - val_loss: 5.0419 - val_mean_absolute_error: 1.7734\n",
            "Epoch 2/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.4626 - mean_absolute_error: 0.7962 - val_loss: 2.6614 - val_mean_absolute_error: 1.1978\n",
            "Epoch 3/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 1.1069 - mean_absolute_error: 0.7677 - val_loss: 2.8500 - val_mean_absolute_error: 1.2987\n",
            "Epoch 4/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0251 - mean_absolute_error: 0.7571 - val_loss: 2.4046 - val_mean_absolute_error: 1.1496\n",
            "Epoch 5/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0067 - mean_absolute_error: 0.7538 - val_loss: 2.5786 - val_mean_absolute_error: 1.2112\n",
            "Epoch 6/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0033 - mean_absolute_error: 0.7538 - val_loss: 2.4003 - val_mean_absolute_error: 1.1445\n",
            "Epoch 7/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0011 - mean_absolute_error: 0.7528 - val_loss: 2.5433 - val_mean_absolute_error: 1.1974\n",
            "Epoch 8/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 1.0001 - mean_absolute_error: 0.7527 - val_loss: 2.2291 - val_mean_absolute_error: 1.0828\n",
            "Epoch 9/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9992 - mean_absolute_error: 0.7529 - val_loss: 2.2639 - val_mean_absolute_error: 1.0977\n",
            "Epoch 10/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9978 - mean_absolute_error: 0.7526 - val_loss: 2.4905 - val_mean_absolute_error: 1.1799\n",
            "Epoch 11/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9987 - mean_absolute_error: 0.7530 - val_loss: 2.3616 - val_mean_absolute_error: 1.1325\n",
            "Epoch 12/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9970 - mean_absolute_error: 0.7524 - val_loss: 2.4013 - val_mean_absolute_error: 1.1484\n",
            "Epoch 13/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9977 - mean_absolute_error: 0.7533 - val_loss: 2.3077 - val_mean_absolute_error: 1.1129\n",
            "Epoch 14/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9962 - mean_absolute_error: 0.7524 - val_loss: 2.4259 - val_mean_absolute_error: 1.1584\n",
            "Epoch 15/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9960 - mean_absolute_error: 0.7528 - val_loss: 2.2300 - val_mean_absolute_error: 1.0840\n",
            "Epoch 16/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9954 - mean_absolute_error: 0.7527 - val_loss: 2.3713 - val_mean_absolute_error: 1.1362\n",
            "Epoch 17/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9961 - mean_absolute_error: 0.7530 - val_loss: 2.5743 - val_mean_absolute_error: 1.2102\n",
            "Epoch 18/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9944 - mean_absolute_error: 0.7522 - val_loss: 2.2251 - val_mean_absolute_error: 1.0818\n",
            "Epoch 19/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9942 - mean_absolute_error: 0.7523 - val_loss: 2.2994 - val_mean_absolute_error: 1.1086\n",
            "Epoch 20/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9947 - mean_absolute_error: 0.7528 - val_loss: 2.4707 - val_mean_absolute_error: 1.1713\n",
            "Epoch 21/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9932 - mean_absolute_error: 0.7524 - val_loss: 2.4700 - val_mean_absolute_error: 1.1735\n",
            "Epoch 22/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9933 - mean_absolute_error: 0.7523 - val_loss: 2.2421 - val_mean_absolute_error: 1.0902\n",
            "Epoch 23/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9931 - mean_absolute_error: 0.7523 - val_loss: 2.2587 - val_mean_absolute_error: 1.0955\n",
            "Epoch 24/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9942 - mean_absolute_error: 0.7529 - val_loss: 2.4365 - val_mean_absolute_error: 1.1604\n",
            "Epoch 25/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9933 - mean_absolute_error: 0.7524 - val_loss: 2.5711 - val_mean_absolute_error: 1.2079\n",
            "Epoch 26/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9925 - mean_absolute_error: 0.7523 - val_loss: 2.4085 - val_mean_absolute_error: 1.1524\n",
            "Epoch 27/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9931 - mean_absolute_error: 0.7525 - val_loss: 2.2664 - val_mean_absolute_error: 1.1001\n",
            "Epoch 28/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9934 - mean_absolute_error: 0.7526 - val_loss: 2.4444 - val_mean_absolute_error: 1.1627\n",
            "Epoch 29/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9921 - mean_absolute_error: 0.7525 - val_loss: 2.4637 - val_mean_absolute_error: 1.1688\n",
            "Epoch 30/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9923 - mean_absolute_error: 0.7522 - val_loss: 2.5244 - val_mean_absolute_error: 1.1931\n",
            "Epoch 31/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9926 - mean_absolute_error: 0.7528 - val_loss: 2.5149 - val_mean_absolute_error: 1.1890\n",
            "Epoch 32/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9911 - mean_absolute_error: 0.7522 - val_loss: 2.3076 - val_mean_absolute_error: 1.1122\n",
            "Epoch 33/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9909 - mean_absolute_error: 0.7520 - val_loss: 2.3441 - val_mean_absolute_error: 1.1264\n",
            "Epoch 34/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9918 - mean_absolute_error: 0.7525 - val_loss: 2.4592 - val_mean_absolute_error: 1.1690\n",
            "Epoch 35/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9912 - mean_absolute_error: 0.7525 - val_loss: 2.5826 - val_mean_absolute_error: 1.2131\n",
            "Epoch 36/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9898 - mean_absolute_error: 0.7520 - val_loss: 2.5440 - val_mean_absolute_error: 1.1999\n",
            "Epoch 37/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9909 - mean_absolute_error: 0.7524 - val_loss: 2.6394 - val_mean_absolute_error: 1.2322\n",
            "Epoch 38/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9904 - mean_absolute_error: 0.7520 - val_loss: 2.3405 - val_mean_absolute_error: 1.1263\n",
            "Epoch 39/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9907 - mean_absolute_error: 0.7524 - val_loss: 2.4424 - val_mean_absolute_error: 1.1624\n",
            "Epoch 40/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9901 - mean_absolute_error: 0.7520 - val_loss: 2.3588 - val_mean_absolute_error: 1.1324\n",
            "Epoch 41/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9896 - mean_absolute_error: 0.7518 - val_loss: 2.2713 - val_mean_absolute_error: 1.1011\n",
            "Epoch 42/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9892 - mean_absolute_error: 0.7518 - val_loss: 2.2481 - val_mean_absolute_error: 1.0907\n",
            "Epoch 43/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9906 - mean_absolute_error: 0.7523 - val_loss: 2.5832 - val_mean_absolute_error: 1.2125\n",
            "Epoch 44/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9897 - mean_absolute_error: 0.7522 - val_loss: 2.4204 - val_mean_absolute_error: 1.1549\n",
            "Epoch 45/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9898 - mean_absolute_error: 0.7522 - val_loss: 2.4120 - val_mean_absolute_error: 1.1515\n",
            "Epoch 46/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9892 - mean_absolute_error: 0.7522 - val_loss: 2.4880 - val_mean_absolute_error: 1.1785\n",
            "Epoch 47/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9899 - mean_absolute_error: 0.7523 - val_loss: 2.1859 - val_mean_absolute_error: 1.0666\n",
            "Epoch 48/50\n",
            "254362/254362 [==============================] - 4s 15us/step - loss: 0.9880 - mean_absolute_error: 0.7518 - val_loss: 2.2594 - val_mean_absolute_error: 1.0948\n",
            "Epoch 49/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9891 - mean_absolute_error: 0.7519 - val_loss: 2.3354 - val_mean_absolute_error: 1.1242\n",
            "Epoch 50/50\n",
            "254362/254362 [==============================] - 4s 14us/step - loss: 0.9883 - mean_absolute_error: 0.7516 - val_loss: 2.5632 - val_mean_absolute_error: 1.2063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjxKZMUEAvHK",
        "colab_type": "text"
      },
      "source": [
        "# **Examining the train/val errors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi0BN6UdZv0F",
        "colab_type": "code",
        "outputId": "6588c819-1f56-4f3a-c0a1-5dadd242caf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "avg_train = [np.mean(i) for i in zip(*train_errors)]\n",
        "avg_val = [np.mean(i) for i in zip(*val_errors)]\n",
        "plot_train_val_error(avg_train, avg_val)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1fn48c8zk5ksEBI2RQiQqMi+\nxYAim7gCWnGhKhW3WmlttbYuLbV+q7X1W2v9WatVW/0Wra2CiKK0omiVqriyKCAggiySgBD2JevM\nnN8f504yCTPJTJKZyfK8X6+8ZubOnTtnksl97jnPWcQYg1JKKVWbK9kFUEop1TxpgFBKKRWWBgil\nlFJhaYBQSikVlgYIpZRSYaUkuwBNpUuXLiY3NzfZxVBKqRZl+fLlu40xXcM912oCRG5uLsuWLUt2\nMZRSqkURka2RntMmJqWUUmFpgFBKKRWWBgillFJhtZochFIqMSorKyksLKSsrCzZRVExSEtLIycn\nB4/HE/VrNEAopWJSWFhIZmYmubm5iEiyi6OiYIxhz549FBYWkpeXF/XrtIlJKRWTsrIyOnfurMGh\nBREROnfuHHOtTwOEUipmGhxanob8zeIWIERklojsEpHPIzwvIvKwiGwUkVUikh/ynF9EPnN+FsSr\njACHy308+OaXfLZtfzzfRimlWpx41iCeBibW8fwkoI/zMwN4POS5UmPMMOfngvgVESp9AR5+awOf\nfb0vnm+jlGoie/bsYdiwYQwbNoxu3brRo0ePqscVFRVRHePaa69l/fr1de7z6KOP8uyzzzZFkRkz\nZgyfffZZkxwrkeKWpDbGvCsiuXXsMgV4xtgViz4SkWwROc4YsyNeZQon3esGoKTSn8i3VUo1UOfO\nnatOtnfffTft27fntttuq7GPMQZjDC5X+Gvgp556qt73+dGPftT4wrZwycxB9AC2hTwudLYBpInI\nMhH5SEQujGchUlNciEBZhQYIpVqyjRs3MmDAAK644goGDhzIjh07mDFjBgUFBQwcOJB77rmnat/g\nFb3P5yM7O5uZM2cydOhQRo0axa5duwC48847eeihh6r2nzlzJiNHjqRv37588MEHABw5coRLLrmE\nAQMGMHXqVAoKCqKuKZSWlnL11VczePBg8vPzeffddwFYvXo1I0aMYNiwYQwZMoRNmzZx6NAhJk2a\nxNChQxk0aBDz5s1ryl9dRM21m2tvY0yRiBwPvC0iq40xX9XeSURmYJun6NWrV4PeSERI97gp1RqE\nUjH79b/WsHb7wSY95oDuHbjrWwMb9NovvviCZ555hoKCAgDuu+8+OnXqhM/nY8KECUydOpUBAwbU\neM2BAwcYP3489913H7fccguzZs1i5syZRx3bGMMnn3zCggULuOeee3j99dd55JFH6NatGy+++CIr\nV64kPz//qNdF8vDDD5Oamsrq1atZs2YNkydPZsOGDTz22GPcdtttXHbZZZSXl2OM4ZVXXiE3N5fX\nXnutqsyJkMwaRBHQM+RxjrMNY0zwdhPwX2B4uAMYY54wxhQYYwq6dg07GWFUNEAo1TqccMIJVcEB\nYPbs2eTn55Ofn8+6detYu3btUa9JT09n0qRJAJx88sls2bIl7LEvvvjio/ZZsmQJl19+OQBDhw5l\n4MDoA9uSJUuYPn06AAMHDqR79+5s3LiR0047jd/+9rfcf//9bNu2jbS0NIYMGcLrr7/OzJkzef/9\n98nKyor6fRojmTWIBcCNIjIHOAU4YIzZISIdgRJjTLmIdAFGA/fHsyDpXjcl2sSkVMwaeqUfL+3a\ntau6v2HDBv70pz/xySefkJ2dzfTp08OOA/B6vVX33W43Pp8v7LFTU1Pr3acpXHnllYwaNYpXX32V\niRMnMmvWLMaNG8eyZctYuHAhM2fOZNKkSdxxxx1xK0NQPLu5zgY+BPqKSKGIXCciPxCRHzi7LAQ2\nARuBJ4EfOtv7A8tEZCWwGLjPGHN02G9C6R43ZVqDUKpVOXjwIJmZmXTo0IEdO3awaNGiJn+P0aNH\nM3fuXMDmDsLVUCIZO3ZsVS+pdevWsWPHDk488UQ2bdrEiSeeyM0338z555/PqlWrKCoqon379lx5\n5ZXceuutrFixosk/Szjx7MU0rZ7nDXBUNwFjzAfA4HiVK5x0r5tSrUEo1ark5+czYMAA+vXrR+/e\nvRk9enSTv8dNN93EVVddxYABA6p+IjX/nHvuuVXzII0dO5ZZs2bx/e9/n8GDB+PxeHjmmWfwer08\n99xzzJ49G4/HQ/fu3bn77rv54IMPmDlzJi6XC6/Xy1/+8pcm/yzhiD1Pt3wFBQWmoQsGXfrXD3EJ\nzJkxqolLpVTrs27dOvr375/sYjQLPp8Pn89HWloaGzZs4JxzzmHDhg2kpDTP/j/h/nYistwYUxBu\n/+b5KRIsw+tm35HoBtgopVTQ4cOHOfPMM/H5fBhj+Otf/9psg0NDtJ5P0gjpHjfbNQehlIpRdnY2\ny5cvT3Yx4kYn60O7uSqlVDgaIA4X87PN1zKu7J1kl0QppZoVbWLytqNb2SY6B3YluyRKKdWsaA3C\nm0GlK5X2gQO0lh5dSinVFDRAAGWebDpyiAp/INlFUUrVY8KECUcNenvooYe44YYb6nxd+/btAdi+\nfTtTp04Nu8/pp59Ofd3lH3roIUpKSqoeT548mf37G7+ezN13380DDzzQ6OM0JQ0QQIUnm45yiLIK\nDRBKNXfTpk1jzpw5NbbNmTOHadPqHJtbpXv37o2aDbV2gFi4cCHZ2dkNPl5zpgECqEztSCc5REll\n/OZXUUo1jalTp/Lqq69WLQ60ZcsWtm/fztixY6vGJeTn5zN48GBeeeWVo16/ZcsWBg0aBNgpty+/\n/HL69+/PRRddRGlpadV+N9xwQ9VU4XfddRdgZ2Ddvn07EyZMYMKECQDk5uaye/duAB588EEGDRrE\noEGDqqYK37JlC/379+f6669n4MCBnHPOOTXepz7hjnnkyBHOO++8qum/n3/+eQBmzpzJgAEDGDJk\nyFFrZDSEJqkBX1onOrJZp9tQKlavzYRvVjftMbsNhkn3RXy6U6dOjBw5ktdee40pU6YwZ84cLr30\nUkSEtLQ05s+fT4cOHdi9ezennnoqF1xwQcT1mB9//HEyMjJYt24dq1atqjFd97333kunTp3w+/2c\neeaZrFq1ih//+Mc8+OCDLF68mC5dutQ41vLly3nqqaf4+OOPMcZwyimnMH78eDp27MiGDRuYPXs2\nTz75JJdeeikvvvhi1UyudYl0zE2bNtG9e3deffVVwE7/vWfPHubPn88XX3yBiDRJs5fWIAB/eic6\nymEdC6FUCxHazBTavGSM4Y477mDIkCGcddZZFBUVsXPnzojHeffdd6tO1EOGDGHIkCFVz82dO5f8\n/HyGDx/OmjVr6p2Ib8mSJVx00UW0a9eO9u3bc/HFF/Pee+8BkJeXx7Bhw4C6pxSP9piDBw/mzTff\n5Oc//znvvfceWVlZZGVlkZaWxnXXXcdLL71ERkZGVO9RF61BACa9E9lyhK/Ky5NdFKValjqu9ONp\nypQp/PSnP2XFihWUlJRw8sknA/Dss89SXFzM8uXL8Xg85Obmhp3iuz6bN2/mgQceYOnSpXTs2JFr\nrrmmQccJCk4VDna68FiamMI56aSTWLFiBQsXLuTOO+/kzDPP5Fe/+hWffPIJb731FvPmzePPf/4z\nb7/9dqPeR2sQgGR0BqDy8N4kl0QpFY327dszYcIEvvvd79ZITh84cIBjjjkGj8fD4sWL2bp1a53H\nGTduHM899xwAn3/+OatWrQLsVOHt2rUjKyuLnTt3Vq3kBpCZmcmhQ4eOOtbYsWN5+eWXKSkp4ciR\nI8yfP5+xY8c26nNGOub27dvJyMhg+vTp3H777axYsYLDhw9z4MABJk+ezB//+EdWrlzZqPcGrUEA\nIO1sgPAf3p3kkiilojVt2jQuuuiiGj2arrjiCr71rW8xePBgCgoK6NevX53HuOGGG7j22mvp378/\n/fv3r6qJDB06lOHDh9OvXz969uxZY6rwGTNmMHHiRLp3787ixYurtufn53PNNdcwcuRIAL73ve8x\nfPjwqJuTAH77299WJaIBCgsLwx5z0aJF3H777bhcLjweD48//jiHDh1iypQplJWVYYzhwQcfjPp9\nI9HpvoHtKxbSfcE0lox5hjFnTWnikinVuuh03y1XrNN9axMTkNLeWc+6ZE9yC6KUUs2IBgggNcsG\nCCnVAKGUUkEaIIDUTNuf2V22L8klUaplaC1N021JQ/5mGiCA1PR2HDGppGiAUKpeaWlp7NmzR4NE\nC2KMYc+ePaSlpcX0Ou3FBHbUIR3wVmiAUKo+OTk5FBYWUlxcnOyiqBikpaWRk5MT02s0QDgOSCap\nFY0fmq5Ua+fxeMjLy0t2MVQCaBOT45Ari7RKDRBKKRWkAcJx2NWBDL8GCKWUCtIA4ShJyaKd70Cy\ni6GUUs2GBghHSUoWGaYEfBXJLopSSjULGiAcZd6O9k6p9mRSSinQAFGlwussGajTbSilFKABokpl\nqlOD0AChlFJAHAOEiMwSkV0i8nmE50VEHhaRjSKySkTyQ567WkQ2OD9Xx6uMoXypnewdDRBKKQXE\ntwbxNDCxjucnAX2cnxnA4wAi0gm4CzgFGAncJSId41hOAAJpGiCUUipU3AKEMeZdoK4l2qYAzxjr\nIyBbRI4DzgXeNMbsNcbsA96k7kDTNOXNsAHCaIBQSikguTmIHsC2kMeFzrZI248iIjNEZJmILGvs\nvDCpaWkcMum6qpxSSjladJLaGPOEMabAGFPQtWvXRh0r3eNmn2lP4IiuS62UUpDcAFEE9Ax5nONs\ni7Q9rtI9bvaSSaBEaxBKKQXJDRALgKuc3kynAgeMMTuARcA5ItLRSU6f42yLq3Svm30mE0q0BqGU\nUhDH6b5FZDZwOtBFRAqxPZM8AMaYvwALgcnARqAEuNZ5bq+I/AZY6hzqHmNM3M/awRqEq2RrvN9K\nKaVahLgFCGPMtHqeN8CPIjw3C5gVj3JFku5187XJxF2mNQillIIWnqRuSukeN3tNJm7fEagsS3Zx\nlFIq6TRAONI8bvaTaR+Uai1CKaU0QDgyvLYGAehoaqWUQgNElapeTKA9mZRSCg0QVYK9mACtQSil\nFBogqqR5QmsQGiCUUkoDhCM1xcVBaWcfaBOTUkppgAgSETyeVErdmVqDUEopNEDUkO51c8SdpQFC\nKaXQAFFDmsfNEXcHDRBKKYUGiBoyvG4OSgcdKKeUUmiAqCHd4+aA6IyuSikFGiBqSPO42Yc2MSml\nFGiAqCHd62avaQ+VJVBRkuziKKVUUmmACJHhdbMnoBP2KaUUaICoIc3jpjjQ3j7QZialVBunASJE\nusdNsT/DPtAAoZRq4zRAhEj3uPmmUqfbUEop0ABRQ4bXzQ4NEEopBWiAqCHN62a/aYdBtIlJKdXm\naYAIke5x48eNScuuO0D4feCvTFzBlFIqCTRAhEj3uAEIpHeqO0C8djs8MyVBpVJKqeRISXYBmpN0\nrw0QlakdSYkUIIyBLxZCxZEElkwppRJPaxAhgjWIytSOkZPUezfB4W+g4hCUHUxg6ZRSKrE0QIQI\n1iAqPHXkILYsqb5/sCgBpVJKqeTQABEiWIMo9WTbqTaMOXqnrR9U3z+gAUIp1XppgAgRrEGUpGSB\nr8xO2lfb1vehx8n2/sHCBJZOKaUSSwNEiGANoiQly26o3cy0bysc2AaDpgKiNQilVKsW1wAhIhNF\nZL2IbBSRmWGe7y0ib4nIKhH5r4jkhDznF5HPnJ8F8SxnULAGccgVIUBsfd/eHj8eMrtpDkIp1arF\nrZuriLiBR4GzgUJgqYgsMMasDdntAeAZY8zfReQM4HfAlc5zpcaYYfEqXzjBGsRB6WA31A4QW96H\n9I7QtT906KEBQinVqsWzBjES2GiM2WSMqQDmALVHlw0A3nbuLw7zfEIFaxD7qwJEra6uW5dA79Hg\nckFWD21iUkq1avEMED2AbSGPC51toVYCFzv3LwIyRaSz8zhNRJaJyEcicmG4NxCRGc4+y4qLixtd\nYK/bhUvgAM6iQaE1iANFsG8L9D7NPu6QY2sQ4Xo6KaVUK5DsJPVtwHgR+RQYDxQBfue53saYAuA7\nwEMickLtFxtjnjDGFBhjCrp27drowogI6R43+wIZIK6aASLYvbX3aHub1cP2cird1+j3VUqp5iie\nU20UAT1DHuc426oYY7bj1CBEpD1wiTFmv/NckXO7SUT+CwwHvopjeQHbzFTiMzbXENrEtHUJpGZB\nt8H2cQenMnSwCDI6xbtYSimVcPGsQSwF+ohInoh4gcuBGr2RRKSLiATL8AtglrO9o4ikBvcBRgOh\nye24Sfe6KavwQ0bnmjWILe9Dr1PBZfMUVQFC8xBKqVYqbgHCGOMDbgQWAeuAucaYNSJyj4hc4Ox2\nOrBeRL4EjgXudbb3B5aJyEps8vq+Wr2f4ibd46a0slaAOLQT9myA3NHVO2YFaxA6WE4p1TrFdTZX\nY8xCYGGtbb8KuT8PmBfmdR8Ag+NZtkhqBIi9m+3G4PiH3iEBov2x4EqBg9sTX0illEqAZCepm500\nj5uSCr/NKwRrEFvfB087OG5o9Y4uN2Qep01MSqlWSwNELRleN2WhTUzG2B5MvU4Bt6fmzjpYTinV\nimmAqCXd66Y0mKQOVML+rbBrbc3mpaCsHnBAcxBKqdZJA0QtVU1M6U7X1XX/tre5Y47euUN3m4PQ\nwXJKqVZIA0Qt6Z6QJiaAdQsgJQ265x+9c4cc8JfDkd2JLaRSSiWABohaMrwhvZgAtn0MOSMgxXv0\nzlkhg+WUUqqV0QBRS7CbqwkdHR2ueQlqjqZWSqlWRgNELWleN8ZAubdj9cZwCWqALGf5Cu3qqpRq\nhaIKECJyQsjUF6eLyI9FJDu+RUuOjOC61NIOxA1uL+QURNi5i31eR1MrpVqhaGsQLwJ+ETkReAI7\nCd9zcStVEgXXhCj1BWweokcBeNLD7+xy2Z5MWoNQSrVC0U61ETDG+ETkIuARY8wjzhTdrU5asAZR\n6YfTZ0KnvLpfoIPllFKtVLQBolJEpgFXA99ytnnq2L/FCi47WlrhhxHX1f+CDj1g20dxLpVSSiVe\ntE1M1wKjgHuNMZtFJA/4R/yKlTwZXhszSyv99ezpyOoBB3dAIBDHUimlVOJFVYNwptr+Mdi1GoBM\nY8zv41mwZEn32phZWhFlgOjQw07JcWQXZHaLY8mUUiqxou3F9F8R6SAinYAVwJMi8mB8i5YcNXIQ\n0dCurkqpViraJqYsY8xB7PKgzxhjTgHOil+xkqdGDiIaHXThIKVU6xRtgEgRkeOAS4F/x7E8SRdz\nDkKXHlVKtVLRBoh7sEuHfmWMWSoixwMb4les5Im5BpHRyU7mp11dlVKtTLRJ6heAF0IebwIuiVeh\nkiktmKSOtgYhomMhlFKtUrRJ6hwRmS8iu5yfF0UkJ96FSwav24VLYqhBgLNwkAYIpVq1gB++eBUO\nFye7JAkTbRPTU8ACoLvz8y9nW6sjImR4U6KvQYBdF0JrEEq1XluWwF/Hw5zvwDv3Jbs0CRNtgOhq\njHnKGONzfp4GusaxXEmV5kz5HbWsHnBoB/h98SuUUirx9n8Nc6+Gp8+Dsv3QtR9sfjfZpUqYaAPE\nHhGZLiJu52c6sCeeBUumdK+LsliamDp0BxOAw9/Er1BKqcSpKIHFv4M/j4AvF8Hpd8CNS2H4dNj9\npZ09oQ2Idi6m7wKPAH8EDPABcE2cypR06cF1qaPVIWSwXFarTM0o1XYc2QNPnm5rDwMvhrPvgeye\n9rm8cfZ2y3sw5NKkFTFRoqpBGGO2GmMuMMZ0NcYcY4y5kFbaiwkgPdYchC49qlTrsWGRDQ6Xz4Zv\nP1UdHACOHQxp2bD5neSVL4Eas6LcLU1WimYm3eOKMUmtAUKpVmPze5DeCU6aePRzLhfkjW0zeYjG\nBAhpslI0M+keN2WxBIi0LPC2166uLUXJXtvGrFQ4W5ZA7mgbDMLJG29rGPu2JLRYydCYAGGarBTN\nTLo3xhxE1WC5GOdjMgbWvgKHNLmdMIEA/N+Z8K+bk10S1Rzt2wIHvobccZH3CeYh2kAtos4AISKH\nRORgmJ9D2PEQrVK6JyW2gXLQsMFyq+fB3Kvg2W9DZVlsr42nJQ/B+w8nuxTxse0j2LsJ1r8Gvopk\nl0ZtWQL/+knzqdFtWWJv88ZG3qfLSdD+2MQHiLULYNcXCX3LOgOEMSbTGNMhzE+mMabeHlAiMlFE\n1ovIRhGZGeb53iLyloiscqYUzwl57moR2eD8XN2wj9cw6V5XbE1MYLu6xpKDOFAIr94KHfPgm1Xw\nxi9je794qTgC7/we/nufvd/arHZmjKk4BFuXJLcsbd2+rfD8dFj+FPzrx7ZGnWyb34OMLna8QyQi\nthax+d3EldlXDi9eB89MSWiLQ2OamOokIm7gUWASMACYJiIDau32AHb68CHYCQF/57y2E3AXcAow\nErjLWagoIdJjHSgHtqvr4V3RXZUGAvDyDRDwwZUvwWk3wdL/gzXzG1bgprT+NagsgcojdlqB1sRX\nYX/Hfc+zEyyufz3xZXjnD/DpPxP/vk1txyooP9zw11eW2dpzIAAjrreB+4NHmq58DWGM7b6aO8YG\ngbrkjYPDO+2YiET45nPwV9ixVs9PtwEjAeIWILAn9o3GmE3GmApgDjCl1j4DgLed+4tDnj8XeNMY\ns9cYsw94EwjTpSA+ggHCxHJ1kNUDMHZEdX0+/ou9+pj4O+h0PJx5F+SMgFdugj1fNbjcTeLzFyGz\nO2T1glXPJ/79v3wDFv+vreo3dRPQpsVQug/yr7KJxi9fa9wVoN9nc0jRNo+snAOLfwvvtfC1tvZt\ngb+Og0dHNvwi4vWfw47P4KLHYfIfYMCF8J+7YMN/6n9t+WE7L1JT27fZtgLkjql/30TnIbavsLfn\n3AuFS23rQwJqL/EMED2AbSGPC51toVZiFyECuAjIFJHOUb4WEZkhIstEZFlxcdNNoJXuTcEYKPfF\nsM50tF1dd62D/9wNJ02yJyoAtwemPgUuN7xwTfLyEaX7YMObMOhiGPJt+OptOLQzce9fshde/J5t\n4nr6PPh9b/jnVPjwUdi5tvH/EKtfgPSOcMIZ0Hei7Ymya13Dj/fhI/Yq+IVr6p9mZdcX8O+fQko6\n7P0KDm5v+Psm27algAFx27mJZn/HNplG69NnYfnTMOan0O88e7V+4WNwzECY993IF0m+cnj7Xvu9\neHAALPolfLO66U6Um9+zt3l1JKiDOuZCdq/EjYcoWmGbvkb9CMbeBp/+w7Y6xFk8A0Q0bgPGi8in\nwHigCIj60sAY84QxpsAYU9C1a9NNDZXuiXFdaohu6VFfBbx0PaRmwgUP16zGZveEi/7i5CPubECp\nm8C6f9n1tQddAkMus9OHfD4vce//zv02N3Ddm3D5czDsCntVt+gOeHwUPDaq4VeO5Yft1e7AiyDF\nW93H/cvXGna8A4W2uajT8XZg1b9vjnyiqjgCL1wNngz49tN225b3G/a+zUHRchvobvzEjjLetBj+\nPBI++HP9gfKb1fDqLZA7FiaEfM+97eDyZ+1F0uxpUHaw5usKl9lay7v3Q/8LoEe+rYn/ZQw8fprt\nWNHYbuZblkC7Y2wSOhp542xQCcRwIdlQ21fYzywCE35pv7+vz4z79yieAaIICBmCSI6zrYoxZrsx\n5mJjzHDgl862/dG8Np7SvXbRoJKYBss5nbrq6ur639/Zf5ALHob2xxz9fN9JMOpGWPokrHk5hhI3\nkdXz7Amv+3Do2heOG5a4ZqY9X9nPnX8V9BxpryzPewBuWg4/WW1/L8XrGn7FH8ytDP62fdyhOxw3\ntOF5iEW/BOOHK1+GcT+zeYXF/3v0fsbY5oDi9XDJ/0GfsyE1y7Z1Nxcr/gFzroj+SrxoOXQfBp50\nGH0z/PAj2yzzxi/tFBVrFxx9ggco3Q/PX2lrcVOfAnetfi4de8Olz8CejfDSDHvirSixF0x/OxvK\nD8F3XrCjm6fNhlu/hPP+nx2D9J+74I8D4f0/Nex3EEv+IShvvJ3Ab+fqyPts/RA+fKxhZQoqP2S/\nP93z7WOXCy5+wnZwmXsV7N9W9+sbIZ4BYinQR0TyRMQLXI6dMryKiHQRkWAZfgHMcu4vAs4RkY5O\ncvocZ1tCpMW6qhzYWkFqVuSrmK0fwvsPwfAr7ckvkjPvgh4FsOAm2x0zUQ7ttP8gg6ZW/4MMuQx2\nrExM17r/3A3uVDspWm3ZvWDEdfZ+4dKGHX/1C7YjQc9Tq7edNMkeL9b5/b9aDGtftlX9jr1hwh32\n7/ru/bD0bzX3/fSfsHI2jP8ZnDDBXiH3Pq26O2WybX7Pjgn54t/Rfd/8lfY70ePk6m0de8N3nrcn\n9yN7YO6VcH8ePHWezbfsWFXdMePANvj236F9hBp/3liYeJ+t2b3yI/jLaJu8zr8KfvghnHRO9b7t\nOsOI78H33oQffwq9RsFHjzesyWnPVzZ/WFf31tpynX0j5SFK9toT+KI7bPNtQ+1YCRhbgwhKy7JB\n0l8Bz18BlaUNP34d4hYgjDE+4EbsiX0dMNcYs0ZE7hGRC5zdTgfWi8iXwLHAvc5r9wK/wQaZpcA9\nzraECK5LHXNX16wIK8tt/wzmfx+yetrEdF1SvPYKSQTevCu292+MNfNtk9LgqdXbBk+17czxrkV8\n/RGsWwBjfgKZx4bfp2MeZHSGomWxH//IHvjqLRh8Sc3RsX0nAgY2vBH9sXwVsPB2W57TbrLbROD8\nh2y1f+FtsM5Ztn3nGvs4bxyM/3n1MfLGNk0eorFt7/u32aavds7J+usP63/Nzs/BX14zQID9HQyY\nAj9ZBdcstL+b8gPw1q/hr2Ph/lxYv9AmWXudUvd7jLzezpq68jnbpHjVAvjWn+xJMZJOx9sgcmiH\nTX7HKlijq2uAXG0djrPNUZECxMLb4cguwNjveEMVOQnq7vk1t3fpAxc/aQPwgvh0E45rDsIYs9AY\nc5Ix5gRjTPDk/ytjzALn/jxjTB9nn+8ZY8pDXjvLGHOi85PQxYmq1qWOuatrSICoLIPPZsOTZ8IT\n4+HIbvvHTM2s/zjZvaDvZGOZpWsAAB+9SURBVPsP29g/+r6t8IcT4YuFde/3+Tw7EVnXvtXb2h9j\nE7qrX4hfO6sxtrkm8zibgItExNasChsQINbOt12Kg81LQccNs+8bSx7io0dhzwbb88aTVr3dnWKb\nTXqcbPurb/iPXUcgLQsu+ZutOQQFe8k0tBZRfhj+82v4XU9Y1sB/jcpSe+Xpr4Sr/2WD79YP6n9d\n0XJ7WztABLk9dpqKs+6GHyyBW9fDhY/DiWfZoHHK9+t/DxE474+2pvHDD+H48dF9pj7ngLhsc2Ks\ntrwH7btB5xNie13eOPt781fW3L7uX/Z/auyt4PY2rsa4fYW9uAxX6+o7Ec640+ZAW1qAaKnSnXWp\nY5puA2wNYt9WeON/4MH+8PIPoOwATPw93LK2/iunUD1OhiPFtqdNY6yea4+z4KbITSn7ttimlsFh\nJugdcpltFtgap2TYmpdsreCMO22isi45I6D4C9uWHYvV86Brfzh2UM3tInDSubbJKJp+5cHEdL/z\nbS6hNm8GTHve/rM+e4mtJVzyt6PzTccOsoEj1jyEMbDyefhzASx50DaxvHpLdY0lluP862Z75Xnx\nk9D1JNs8E83fONibJrtXdO+V2Q2GfQemzoJzfht9+36KFwZeWP93IlS7zrYJcX09F0O1GWNP4Hlj\noy9fUN44qDgM2z+t3nZkj+2xdtxQOP0X9sKmMf8/RStsXjCScbfBWXdFnjuqETRAhNGgHATYKF+2\n33bLzB1jq8Y3LoVTfwDp2bEdK2eEvW1om3vQ5/Oh84k20fWvCD1tPn/R3g4KEyD6nWeTgPFoZvKV\n29zDsYNg6LT6988psLfBPuHR2P+1rYkNnhr+n/+kSfYfPJorvGBi+twwyeigdp1h+kt2JO7Zvwnf\npu1yQ+/RsV1VFq2Av50D82fYk+51b8INH9gTx4vXwdcfR3+sjx63f88Jv3Sa2bB5kX1b6m/2Klxm\nL15iPZEmSt9JtiNILInb3RvsoLdoxj/UVpWHCOnu+trt9iLmwsedGtUYm0cIl7ivz5E9sH9rzfxD\nAmmACKPBOYj8q2DyA/DTz+Gyf9iqcUP/kY4daLsSNqRJJah4PexaAyNnwJn/A+tftQnT2la/CD1P\nCX9V6M2A/t+yA8KaOhH2yRP2BH7Ob2o2wUTSIx+Q2H4nweAXmlsJdfx4+3v+sp7eTLUT03Xp2Bt+\n9DGcdmPkfXLH2KRwfV0zyw/bZO2TZ9gT+JRH4Xtv255e3nbwnbm2aXP2ZVAcxajeTe/YXkH9zrfN\nH0G9RtnbupqZyg7YkcPBQN0c9Z1sb+v7e4ba4uQQcmNIUAdldLJNs8E8xNoF9js3/uf2fxhsk5sJ\nwLYYgnhQsGZSO/+QIBogwmhwDqL9MTbBFuzy2hhuj+1K2JCkbNDnLwFO8vDUH0Kv0+C1n9e8utq1\nzgaRcLWHoCGXQfnB2P7p6lOyF979A5x4ts1zRCMty+ZIYgkQq16AnJF2YFM4nnQ4/nTb3TVSG264\nxHRjBa9W62t6WPJHO7DstBttl9/h02s2JbTrAtNfBFcK/PPiupfC3LfVDurr0seOuQk9TrchtqZY\nV6J6+2cc1ZumuelyInTuE1sz05YlNsh2Or5h75k3ztbgDhTZJr/jhtoOF0E5I8HlaVgeIlhb7j6s\nYWVrJA0QYQQDRMw5iKaWU2Crpg2Zd8UY276fO8Y2SbjcdrSqCcArP6xOOq+eZxN7Ay+KfKy8cTaZ\nu7KJmpkCfnj7N7bZ6+x7YnttToFtdosmIbdzjQ1+tZPTtfWdaKd43rX26Of8lbZpLlxiujGiyUNU\nltmJ7PpOtu33aR3C79cpD654wXalfHaqvdIPKj9sLxTmXg2PnWp/95c/d3RnCXeKrUXWVYMIJqiT\ndDUbtb6TbPfdaJp0gvmHWMY/1JY3zvbs+sdFNZuWgrwZNqg2JA9RtMIGvLp6cMWRBogw0pwkdcxN\nTE0tZ4Tt5/xNHQNxItm5xjYHhJ74O+XBuffa6vDSJ+0/x+fz7Bc83MC9IJfbNtFsfNP2xmqogN8G\npMdPg2WzoOC7cGzt+RvrkTMCSvdG12d/9TzbTbeu4AfVo6pr936pOGJH9a58zrbXh0tMN1Q0eYg1\nL0HJHjhlRv3H6z7cjkMo/sJO5rbqBTv47Q8nwLxr7Yl/6DT47muRe+r0HmWDZEmEHuVFy6HTCbZZ\npTnrO9nOCPDVW/XvW7zeduJoSPNSUO/T7Pds93o4PaRpqcY+o21zUSwzJBtTPYI6STRAhOF1u3C7\nJPYkdVNrTKJ6zUv2Szug1vyI+VfbZp0374JVc2279qAI7fOhhlxuu4o2ZMbZYGB4bJRNqILt1TLp\n/tiPVfU7qaeZyRj7nidMiDwoKyizmz3BhjahHdkNT59vTzLfetgOdGtqdeUhjLHJ5K797IjdaJx4\nps1RbH4XXvqePaHnX23HJdz6BZz/YPiTV1Dv0fY2Up/9ouWRu7c2Jz1H2iVDo+nuGqzBxTJArra0\nDjZI9DgZRv80/D65o+3/Tyx5iIPbbfI8iTW2etd0aItEpGFTfje1Dt3tzKqxJqqNsc0KeeNsG3Uo\nEbjgEdvc8PIPbB/t/t+q/5jdBtnJ1D56zDZ5deljf7J7H51g9lfaL/ahb+wV6QeP2NpM1/52rMCA\nCxveJa9rP9tWXrgUhl4Web+vP7LNRhPCjMwO56RJdiqUw8W2V1OwPf/y52yTRTyE5iGGXFrzuW0f\n23m5zv9jbE0fQy+3A9+87Zy27xh+z93z7fdh6/vQb3LN5w5ut4PQWkKAcLltrXD9Qjs3VO0pPUJt\nftf2Psyup+NBfa5w1hmJ9F49T7EXbFvejz7nFsw/JLEGoQEigjRPjMuOxkuwzT0WOz6zk9yNvSX8\n8x2Os3PYvHidrU1E2wV3wi9se3zo4kZur03udehhm0MO7bDrYoSuSHvMADtJXf8pje+r7XLbq/36\nkver5tjJ8aIJfmDzEP/9X3jv/9lmt4DfDiDrOaJx5a3LsYMhLdtexdYOEB//1bY7D6kjCEZy4pkN\nK48nzfbZD5eoDl6ktIQAATaor3zOriAYqftqIGCDYZ9zG99t15Ne9/OpmTbRHEseomiF7XzQbXDj\nytYIGiAiaNCqcvGQM8JOQ3G4uP6mkqDPX7JfrH7nR95n8FR7pRw6N1F9+n/L/hzZY5O2uzfYmsGe\njXYEebuu9suceZwNQpndbS3o2EFNO4gnZwR88LDtdhvuH7OyzDaF9TsfUttHd8xuQ2x5P37croVx\n5Uu2hhRPLpdt1tlcK1F9oMh2Kz71htgGijWF3qPszKjlh2v+7oqW2544STxZxeSEM+zFy/rXIgeI\n4nX2oqYh4x8aovdoOwNtpO9tbdtXwDH9o9s3TjRARJDRkHWp4yHY5l60LLqmDmPsTLAnnFF/MvHk\naxpWpnad7U+vGIJLU8oZYdtzd6wMX4YNb9iePHU1QdUmYrsof/W2nXU1s1vTlbcuuWPs+JQDhdVT\nxi+bZXubjbw+MWUI1fs0W4sq/KRmU0jRctvM2FS9uOIttb3N3XzxavgR3IEAvPuAvR/N+g9NIXeM\nvbApXFr/expjk9oDLkxM2SLQJHUEad5mkIMA26da3NE3MxUus23vAy+uf9+WKjhQK9LvZNXzdlH5\nvNNjO+7YW+CafycuOEDIvExO00No19ZIYzfiqecpttvz1pBmpoDfjoFoKc1LQX0n2abW2suCGmNH\nO695yc6enN0z/OubWq9T7e82mjUc9m6yFzlJHnOiASKCdI+redQgvBn2yi3aALHmJVu1rp1kbE3a\nH2NHfYf7nZTshS8X2bEPdSUnm4tjB1XnISC2rq3xkJppm9tCx0Ps/tIu5NTSAkRV9+Vag+b++zu7\nGttpN9lV7RIlLcs20UU75xUkfcyJBogImkUvpqCcEfYLU99qaoGAbV468eykDaxJmJwR4Xt3rZlv\n+8DXTvo2V8E8xJYlDevaGg+9R9smzeAAzaoZXJvxFBvhZPWwM/aGdnf96C92Sdvh0+1cWYmeU6r3\nGHthU9/g1+0rICXN5iCSSANEBBnelOYTIHoU2IRy8fq699v2ERzabteUbu1yRtjEeO3J5VY9b7vT\ndhuSnHI1RO4Y2xSy5iXbtfWU7yd3Mrzeo8BXVj0PUNFySO1gJ31safpOhm2f2E4eK+fA6z+3nRfO\n/1Nyfse5o+3vNhh0IylaYb/DoSOyk0ADRARpHnfzaGKC6AfMff6Sveo46dz4lynZwg2Y27vJjh8Y\nelnznW00nGAe4tVbG961tSlVTdznNIUULbddi+MwnXTc9Z0EGLt+88s/tMnhS/6WvObHXqMAqTsP\n4Xc6YDSDOa9a4F88MdK9ruZTg+h8gm2nritABPy2a2Sfc6JblKil6zbY5lpCfyerXgCk/rmXmptg\nHqJ0n126NNFdW2tr18U2c2390HbJ3Lmmec/gWpdug+1Ss5/Psx0+Ln8uuT2xMjrZ0exb65hipfgL\n8JUmPf8AGiAiSm9ONQgRJw9RR7V003/t8oZtoXkJICXVVsGDNQhj7OC43DHV3UVbimAeAklO19Zw\neo2ytbHtn9ouxS0tQR0kYtcz73kKXDGveVw89R5tm71qr0IX1AxGUAdpgIgg3clBmDgs49cgOQV2\nau5wM1RWltppvDv0sKNC24qcEfYE5q+0gWLvJjvVREs04Q64+InkdG0Np/doO8X7imfs45YaIMB2\nX77uDTt2pznIHQ2VJTVXoQtVtMLmfDrFuPxpHGiAiCA45Xe5L05rMccqpwAw4VdT+8+v7cjmKY/a\nbrFtRU6BrYrvWmtrDylp0P+CZJeqYboNal49r3o7eYjVL9gLj0SODWntgpMiRprJd/sKOy1HM8j5\nJL8EzVS6p4HrUsdL8AqudtfOze/a6SFGzrAzl7YlwUT11g9sgr7feZHXTFCxycqxY01acvNSc1WV\n4wlJVAf8sHujzSPuXNss8g+gU21ElO5t4Kpy8ZLeEbqcVDNAlB2El39kq6Jn/Tp5ZUuW7F52/qf3\nH7ZrRCS7909r03u0XRJWA0TT6z3adrt9+Ye2E0DxelsbBjuPWkMnXGxiWoOIIN1Zl7rZJKrBjocI\nXU1t0S/gYCFc9Ne21bQUFEzeH9oOGV2in0ZZRaf3afa2pfZgas76ToLKI7DxP/bir+C7ton4+sUw\nc1vi5oeqh9YgIqhal7o5BYicAjuF8b4ttivcp/+0C8/Hc0rq5i6nwE6lMHhq0gcVtTpDLrNTpgfb\nzFXT6XM2/HJns5/8UGsQEVQFiObSxATVbe4b3oAFP7brCYyfmdwyJdsJZ4KnnR0/oJpWSqoNvC1p\n0GFL0syDA2gNIqJ0Z13qZhUgjhlgr+gWOQv2XDkfUrzJLVOydR8GdxTpSUypONAaRATpnmaYg3Cn\n2CkPApVwxi9t10ilwUGpONEaRATVvZh8SS5JLcOvtP3ST/txskuilGrl4lqDEJGJIrJeRDaKyFGN\n5SLSS0QWi8inIrJKRCY723NFpFREPnN+/hLPcoZTnaRuJgPlgoZNg0uetGszK6VUHMWtBiEibuBR\n4GygEFgqIguMMWtDdrsTmGuMeVxEBgALgVznua+MMcPiVb76NLtxEEoplWDxrEGMBDYaYzYZYyqA\nOcCUWvsYIDj0NQuoNbl/8gRrEGUaIJRSbVQ8A0QPYFvI40JnW6i7gekiUoitPdwU8lye0/T0joiM\nDfcGIjJDRJaJyLLi4uImLDp43ILbJZRUNLMchFJKJUiyezFNA542xuQAk4F/iIgL2AH0MsYMB24B\nnhORoybZMcY8YYwpMMYUdO3atUkLJiLOlN/NLAehlFIJEs8AUQT0DHmc42wLdR0wF8AY8yGQBnQx\nxpQbY/Y425cDXwEnxbGsYaV7m9G61EoplWDxDBBLgT4ikiciXuByYEGtfb4GzgQQkf7YAFEsIl2d\nJDcicjzQB9gUx7KG1bmdlx0HShP9tkop1SzELUAYY3zAjcAiYB22t9IaEblHRIKT9t8KXC8iK4HZ\nwDXGrtAzDlglIp8B84AfGGP2xquskQzNyeazbfubz6JBSimVQHEdKGeMWYhNPodu+1XI/bXAUTOB\nGWNeBF6MZ9mikd87m+eXbWPz7iMc37V9soujlFIJlewkdbOW36sjACu+3p/kkiilVOJpgKjDCV3b\n0yEthRVf70t2UZRSKuE0QNTB5RKG9erIiq0aIJRSbY8GiHoM75nNlzsPcbhcB8wppdoWDRD1yO/d\nkYCBlds0D6GUals0QNRjWM9sAG1mUkq1ORog6pGV7qHPMe01Ua2UanM0QEQhv1dHPtUBc0qpNkYD\nRBTye2ezv6SSTbuPJLsoSimVMBogolA1YE7zEEqpNkQDRBSqB8xpTyalVNuhASIKwQFzn2qiWinV\nhmiAiNLwntms1wFzSqk2RANElPJ7d8TogDmlVBuiASJKOmBOKdXWaICIkg6YU0q1NRogYqAD5pRS\nbYkGiBjogDmlVFuiASIGOmBOKdWWaICIgQ6YU0q1JRogYqAD5pRSbYkGiBgFB8wdKqtMdlGUUiqu\nNEDEqHrA3IFkF0UppeJKA0SMqgbMaTOTUqqV0wARo+CAueXak0kp1cppgGiAM/odwztfFvPf9buS\nXRSllIobDRAN8JOzTqJft0x++vxn7DhQmuziKKVUXGiAaIB0r5tHr8inwhfgpuc+pdIfSHaRlFKq\nycU1QIjIRBFZLyIbRWRmmOd7ichiEflURFaJyOSQ537hvG69iJwbz3I2xAld2/O/Fw9m2dZ9PPDG\n+mQXRymlmlzcAoSIuIFHgUnAAGCaiAyotdudwFxjzHDgcuAx57UDnMcDgYnAY87xmpUpw3pwxSm9\n+Os7m3hr3c5kF0cppZpUPGsQI4GNxphNxpgKYA4wpdY+Bujg3M8Ctjv3pwBzjDHlxpjNwEbneM3O\n/5w/gIHdO3DL3JUU7itJdnGUUqrJxDNA9AC2hTwudLaFuhuYLiKFwELgphhei4jMEJFlIrKsuLi4\nqcodkzSPm0e/k48/YLjxuU+p8Gk+QinVOiQ7ST0NeNoYkwNMBv4hIlGXyRjzhDGmwBhT0LVr17gV\nsj65Xdpx/9QhfLZtP797bZ2uF6GUahXiGSCKgJ4hj3OcbaGuA+YCGGM+BNKALlG+tlmZPPg4rjkt\nl6fe38L1zyxj58GyZBdJKaUaJZ4BYinQR0TyRMSLTTovqLXP18CZACLSHxsgip39LheRVBHJA/oA\nn8SxrE3if84fwJ3n9ee9Dbs5+8F3eHF5odYmlFItVtwChDHGB9wILALWYXsrrRGRe0TkAme3W4Hr\nRWQlMBu4xlhrsDWLtcDrwI+MMf54lbWpuF3C98Yez2s3j+WkYzO59YWVXPf3ZXxzQGsTSqmWR1rL\nFW5BQYFZtmxZsotRxR8w/P2DLdy/6As8bhd3TO7PKXmdyM7w0iEthRR3stM/SikFIrLcGFMQ7rmU\nRBemrXC7hO+OyeOMfsfws3mr+MVLq2s8n5mWQnaGh+x0r73N8JKd7qFjhoesDC8dMzyke9y4XUKK\nW3C7XKS4xD52CR63C4/bhTel+n6KW3CL4Ar+uKi6H3ydyyVJ+o0opVoaDRBxltulHXNmnMpHm/aw\n61A5+0sq2F9ayf6SSg6UVrKvpIL9JZUU7itlX0kFB0oriWelToSQQOPC4xYn0Dg/zn1joKzST5nP\nT3llwLkfAAPt01LITEuhfar9yUzz0C7VjUsEARAQBBEQwBcwVPgDVPgCVPqdH59BBFI9blJTXKQ6\n75+a4ibFJQSMIWDAGIMxEDAGQ3XZ3SHB0iWCL2CcY9tbnz9AZcDgcQmpKW5SPS7SQt5LRKj0B/AH\nDJV+gz9gXysCqSlu0jyuGrcpbqHCZz9DuS9Auc9PuS9ApS+Axzlmaoo75HPY29DfcYpzCxAIGPzG\n4A/U/KkMGCp9AXyBABV+ez9gDKkeN2kp9jOke9ykeWzZ3K7qC4DqW6jwByhz/m6lFX5KK+0PQIbX\nHiPd4ybN6ybD68brtr8T589X9V0RBLdbalycpLhduMX+jfzG4PfX/Cz2O+Zy9rXfsxS3Pao/YPAF\nDD5/AF/I5470XQ1+JrfYi5vgfRGq/k/sN4Oj/m9CH7pF8KbY31ddAgFT9Z0PXmSluFzVt0LV76mp\nGOe77g8Y+z03RFXWRNAAkQAul3DaiV2i2jcQMBwsq2RfSSVllf6qf6jgP5LPOZH5nBNt8CQSPKEY\nY5yTT/CLZ/AH7AnW55wIg8cLnkwrnZN3RcitS4RU54QUPEmmelwIwuHySg6X+ThU5uNQuY+i/aWU\nVPiqvtxV/7jOSd3WcKoDkcdtTx7GwIHSSsor/VT4A5RX2pOvPxDA7RLECTi2FmSPGTwR+WqdWENP\nwB6X4HH+wfwBQ1mlPZmXVwYo8/lrnEhcAilOeYJlKnd+B/UJfiaf30S1v0o+j1tIc77LwcBfVumn\nrDJAaaU/qnFMLqFGQA4GL7D/Z4GAc8I3puoCJxzj7B/p+RSXVF3UBG9dLqn6vwoexAD9j8vksStO\njvXXUS8NEM2MyyW2uSnDm+yitErG2MAI1Nnk5g8YKnyBqlqUz2+qawkeW9MKfW3AqSXZIOevCjK+\nkCDsc2oHQNXVsEukRq0oNJgG77tcUl2LCzmZlVX68QcvCJyrz+CVqMftIj2kppDutWUHWzMsrfRT\n4tQsyipseeHoq/GAsZ/NBuSAU9uyj4Mnyho/IhioqiH4/NWvNaY6GNvPaptO7VX50X+DgKk+4fpC\nPmNoLA6+Tmo9rnreecYX/Hs6tYNyn/09+gIB0lLcpHvt3zVYO0t1atH+WjU9X8BU1f6Cv/fgfXFq\nNsHAIUJVbScS+3x1zc/lEgRbuw3+rYNlLfP5q6tFTu08WJvJ7ZwR+U0aQQOEalNEBG9K/VV3t0vs\nCdYb3RRgLpeQ5rInF/A0spRhpDX9IZWqj3alUUopFZYGCKWUUmFpgFBKKRWWBgillFJhaYBQSikV\nlgYIpZRSYWmAUEopFZYGCKWUUmG1mtlcRaQY2FrPbl2A3QkoTnPUVj+7fu62RT937HobY8Iuydlq\nAkQ0RGRZpGltW7u2+tn1c7ct+rmbljYxKaWUCksDhFJKqbDaWoB4ItkFSKK2+tn1c7ct+rmbUJvK\nQSillIpeW6tBKKWUipIGCKWUUmG1mQAhIhNFZL2IbBSRmckuT7yIyCwR2SUin4ds6yQib4rIBue2\nYzLLGA8i0lNEFovIWhFZIyI3O9tb9WcXkTQR+UREVjqf+9fO9jwR+dj5vj8vIq1yiUIRcYvIpyLy\nb+dxW/ncW0RktYh8JiLLnG1N/l1vEwFCRNzAo8AkYAAwTUQGJLdUcfM0MLHWtpnAW8aYPsBbzuPW\nxgfcaowZAJwK/Mj5G7f2z14OnGGMGQoMAyaKyKnA74E/GmNOBPYB1yWxjPF0M7Au5HFb+dwAE4wx\nw0LGPzT5d71NBAhgJLDRGLPJGFMBzAGmJLlMcWGMeRfYW2vzFODvzv2/AxcmtFAJYIzZYYxZ4dw/\nhD1p9KCVf3ZjHXYeepwfA5wBzHO2t7rPDSAiOcB5wP85j4U28Lnr0OTf9bYSIHoA20IeFzrb2opj\njTE7nPvfAMcmszDxJiK5wHDgY9rAZ3eaWT4DdgFvAl8B+40xPmeX1vp9fwj4GRBwHnembXxusBcB\nb4jIchGZ4Wxr8u96SmMPoFoWY4wRkVbbt1lE2gMvAj8xxhy0F5VWa/3sxhg/MExEsoH5QL8kFynu\nROR8YJcxZrmInJ7s8iTBGGNMkYgcA7wpIl+EPtlU3/W2UoMoAnqGPM5xtrUVO0XkOADndleSyxMX\nIuLBBodnjTEvOZvbxGcHMMbsBxYDo4BsEQleALbG7/to4AIR2YJtMj4D+BOt/3MDYIwpcm53YS8K\nRhKH73pbCRBLgT5ODwcvcDmwIMllSqQFwNXO/auBV5JYlrhw2p//BqwzxjwY8lSr/uwi0tWpOSAi\n6cDZ2PzLYmCqs1ur+9zGmF8YY3KMMbnY/+e3jTFX0Mo/N4CItBORzOB94Bzgc+LwXW8zI6lFZDK2\nzdINzDLG3JvkIsWFiMwGTsdO/7sTuAt4GZgL9MJOiX6pMaZ2IrtFE5ExwHvAaqrbpO/A5iFa7WcX\nkSHYhKQbe8E31xhzj4gcj72y7gR8Ckw3xpQnr6Tx4zQx3WaMOb8tfG7nM853HqYAzxlj7hWRzjTx\nd73NBAillFKxaStNTEoppWKkAUIppVRYGiCUUkqFpQFCKaVUWBoglFJKhaUBQql6iIjfmTUz+NNk\nE/6JSG7ozLtKNSc61YZS9Ss1xgxLdiGUSjStQSjVQM6c/Pc78/J/IiInOttzReRtEVklIm+JSC9n\n+7EiMt9Zu2GliJzmHMotIk866zm84YyIRkR+7KxvsUpE5iTpY6o2TAOEUvVLr9XEdFnIcweMMYOB\nP2NH6gM8AvzdGDMEeBZ42Nn+MPCOs3ZDPrDG2d4HeNQYMxDYD1zibJ8JDHeO84N4fTilItGR1ErV\nQ0QOG2Pah9m+BbtYzyZnosBvjDGdRWQ3cJwxptLZvsMY00VEioGc0KkfnKnJ33QWeUFEfg54jDG/\nFZHXgcPYqVJeDln3QamE0BqEUo1jItyPRehcQX6qc4PnYVdCzAeWhsxSqlRCaIBQqnEuC7n90Ln/\nAXaGUYArsJMIgl0G8gaoWuQnK9JBRcQF9DTGLAZ+DmQBR9VilIonvSJRqn7pzoptQa8bY4JdXTuK\nyCpsLWCas+0m4CkRuR0oBq51tt8MPCEi12FrCjcAOwjPDfzTCSICPOys96BUwmgOQqkGcnIQBcaY\n3ckui1LxoE1MSimlwtIahFJKqbC0BqGUUiosDRBKKaXC0gChlFIqLA0QSimlwtIAoZRSKqz/D/3A\nMK2YahylAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYeGjCnDA2zm",
        "colab_type": "text"
      },
      "source": [
        "It is quite difficult to see the plot as the scaling issues and relatively high variance. We will replace each point with an exponential moving average of the previous points to obtain a smooth curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDkFfd1ja8ou",
        "colab_type": "code",
        "outputId": "d97dd5ea-1c4d-4f2a-bacb-d1d4f1757bd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "smooth_avg_val = smooth_curve(avg_val, factor=0.4)\n",
        "plot_train_val_error(avg_train, smooth_avg_val)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSgjAwz6CBpK",
        "colab_type": "text"
      },
      "source": [
        "# **Final model**\n",
        "It seems that the validation MAE stops inproving after 10 epochs. Past that point, we start overfitting. We will build the final model, where we train it for only 10 epochs on the whole training dataset. In the end, we test the trained model on our test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nj5IaELEEAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare train, validation and test data\n",
        "(train_X, train_y), _, (test_X, test_y)  = prep(train_df, test_df, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjiSevGB6mKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "ed7d74e3-8e6d-4fd5-c4d6-bb4d57875079"
      },
      "source": [
        "# build and train the model\n",
        "model = build_model(np.shape(train_X)[1])\n",
        "model.fit(train_X, train_y, epochs=10, batch_size=512, verbose=1)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 100)               1100      \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 51,601\n",
            "Trainable params: 51,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "317952/317952 [==============================] - 5s 16us/step - loss: 3.0757 - mean_absolute_error: 1.0181\n",
            "Epoch 2/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.5153 - mean_absolute_error: 0.8301\n",
            "Epoch 3/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.2528 - mean_absolute_error: 0.8093\n",
            "Epoch 4/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.1875 - mean_absolute_error: 0.8008\n",
            "Epoch 5/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.1776 - mean_absolute_error: 0.7998\n",
            "Epoch 6/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.1751 - mean_absolute_error: 0.7995\n",
            "Epoch 7/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.1728 - mean_absolute_error: 0.7992\n",
            "Epoch 8/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.1701 - mean_absolute_error: 0.7991\n",
            "Epoch 9/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.1694 - mean_absolute_error: 0.7987\n",
            "Epoch 10/10\n",
            "317952/317952 [==============================] - 4s 13us/step - loss: 1.1697 - mean_absolute_error: 0.7995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc95b3c7ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trceXpVUC1cR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7664470c-afe5-4c79-deae-cb24b64a475a"
      },
      "source": [
        "# evaluate the trained model on test data\n",
        "test_mse_score, test_mae_score = model.evaluate(test_X, test_y)\n",
        "print(\"Test MAE Score: \", test_mae_score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144006/144006 [==============================] - 10s 73us/step\n",
            "Test MAE Score:  1.0370007480915813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSnrl2F3E09n",
        "colab_type": "text"
      },
      "source": [
        "We are still off by about £1820.00. Note that we scaled down the prices to thousands and then applied the `np.log1p` function to transform the data. To obtain the offset in price, we only need to inverse the process using the formula: $\\text{price-offset} \\approx 1000\\cdot (e^{\\text{MAE_SCORE}}-1)$."
      ]
    }
  ]
}