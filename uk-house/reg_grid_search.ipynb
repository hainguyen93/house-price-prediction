{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reg_grid_search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEnBtiXhWhWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "b78a54e3-b6c8-46a7-9b59-0f4eeb30ca94"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from keras.models import Model, Sequential\n",
        "from keras.metrics import mean_absolute_error, mae, mse\n",
        "from keras.layers import Dense, Dropout, Input\n",
        "from keras.regularizers import l2, l1_l2\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "from datetime import datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRxmx9xVW0Rg",
        "colab_type": "code",
        "outputId": "10e78cdb-3e65-47c1-fa50-9f57515e7adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD_zXvWFgn2Q",
        "colab_type": "text"
      },
      "source": [
        "# **Define some useful functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-nntaLcW2EP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(url, columns=[1, 2, 4, 6, 11]):\n",
        "  \"\"\" Load the dataset, change the column names, \n",
        "      and replace the categorical data by numeric values\n",
        "  \"\"\"\n",
        "  # load data\n",
        "  df = pd.read_csv(url, header=None, usecols=columns)\n",
        "  print('Original Data shape: ', np.shape(df))\n",
        "\n",
        "  # re-name all columns\n",
        "  column_names = ['Price', 'PurchaseDate', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  df.columns = column_names\n",
        "  \n",
        "  # resplace column values\n",
        "  df['PropertyType'] = df['PropertyType'].replace({'F':0, 'D':1, 'S':2, 'T':3, 'O':4})\n",
        "  df['LeaseDuration'] = df['LeaseDuration'].replace({'L':0, 'F':1, 'U':2})\n",
        "  df.loc[df['City']=='LONDON', 'City'] = 0\n",
        "  df.loc[df['City'] != 0, 'City'] = 1\n",
        "\n",
        "  # convert column values to appropriate dtype (to save memory)\n",
        "  df['PurchaseDate'] = pd.to_datetime(df['PurchaseDate'])\n",
        "  df['Price'] = pd.to_numeric(df[\"Price\"], downcast=\"integer\")\n",
        "  df['PropertyType'] = pd.to_numeric(df['PropertyType'], downcast='integer')\n",
        "  df['LeaseDuration'] = pd.to_numeric(df[\"LeaseDuration\"], downcast=\"integer\")\n",
        "  df['City'] = pd.to_numeric(df[\"City\"], downcast=\"integer\")\n",
        "  \n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C_FNi6BW4IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_test(df):\n",
        "  \"\"\" Split the data into training and test dataset \n",
        "  \"\"\"\n",
        "  # purchases prior to 1/1/2016 as training \n",
        "  cutoff = datetime(2016, 1, 1)\n",
        "  column_sels = ['Price', 'PropertyType', 'LeaseDuration', 'City']\n",
        "  train_df = df.loc[df['PurchaseDate'] <= cutoff][column_sels]\n",
        "  test_df = df.loc[df['PurchaseDate'] > cutoff][column_sels] \n",
        "  \n",
        "  # remove duplicates\n",
        "  train_df.drop_duplicates(keep='first', inplace=True)\n",
        "  test_df.drop_duplicates(keep='first', inplace=True)\n",
        "  print(\"Train (and val) shape: \", train_df.shape)\n",
        "  print(\"Test shape: \", test_df.shape)  \n",
        "  \n",
        "  return train_df, test_df  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycWz4gRbzi8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_val(train_df, perc=0.2):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  train_df, val_df = train_test_split(train_df, test_size=perc, random_state=2019)\n",
        "  print(\"Train shape : \", train_df.shape)\n",
        "  print(\"Test shape : \", test_df.shape)\n",
        "  \n",
        "  return train_df, val_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DMSV3QyW7_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prep(train_df, val_df, test_df):\n",
        "  \"\"\" Prepare inputs/targets pair for training, val, and testing\n",
        "      using one-hot encoding (for categorical data), and \n",
        "      down-scale the target values (prices) \n",
        "  \"\"\"\n",
        "  # training \n",
        "  train_X = train_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  train_y = train_df['Price']\n",
        "\n",
        "  # validation\n",
        "  val_X = val_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  val_y = val_df['Price']\n",
        "\n",
        "  # testing \n",
        "  test_X = test_df[['PropertyType', 'LeaseDuration', 'City']]\n",
        "  test_y = test_df['Price']\n",
        "\n",
        "  # one-hot encoding the inputs\n",
        "  ohc = OneHotEncoder(handle_unknown='ignore')\n",
        "  ohc.fit(train_X)\n",
        "  train_X = ohc.transform(train_X)\n",
        "  val_X = ohc.transform(val_X)\n",
        "  test_X = ohc.transform(test_X)\n",
        "\n",
        "  # convert the targets to smaller range\n",
        "  train_y = np.log1p(train_y * 1e-3)\n",
        "  val_y = np.log1p(val_y * 1e-3)\n",
        "  test_y = np.log1p(test_y * 1e-3)\n",
        "\n",
        "  return (train_X, train_y), (val_X, val_y), (test_X, test_y)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNEV_u4OW-MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(l_rate=0.01, dout_rate=0.5, reg_rate=0.01, num_units=60, input_size=10):\n",
        "  \"\"\" Build and compile a fully-connected neural network \n",
        "  \"\"\"\n",
        "  inp = Input(shape=(input_size,))\n",
        "  fc1 = Dense(num_units, activation='relu', kernel_regularizer=l2(reg_rate))(inp)\n",
        "  do1 = Dropout(dout_rate)(fc1)\n",
        "  fc2 = Dense(num_units, activation='relu', kernel_regularizer=l2(reg_rate))(do1)\n",
        "  do2 = Dropout(dout_rate)(fc2)\n",
        "  fc3 = Dense(num_units, activation='relu', kernel_regularizer=l2(reg_rate))(do2)\n",
        "  out = Dense(1)(fc3)\n",
        "\n",
        "  model = Model(inputs=inp, outputs=out)\n",
        "  print(model.summary())\n",
        "  optim = RMSprop(lr=l_rate)\n",
        "  model.compile(optimizer=optim, loss=mse, metrics=[mae])  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnKG-_ySXAMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs=10, batch_size=10000):\n",
        "  \"\"\" Train a compiled model\n",
        "  \"\"\"\n",
        "  history = model.fit(train_X, train_y, batch_size=batch_size, verbose=1,\n",
        "                      epochs=epochs, validation_data=(val_X, val_y))  \n",
        "  \n",
        "  train_mae = history.history['mean_absolute_error']\n",
        "  val_mae = history.history['val_mean_absolute_error']  \n",
        "\n",
        "  return train_mae, val_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dlkwMkvXB6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_cv(train_df, test_df, epochs=50, batch_size=512, k=5):\n",
        "  \"\"\" Train with k-fold cross-validation\n",
        "  \"\"\"\n",
        "  num_samples = np.shape(train_df)[0] // k\n",
        "  train_errs = []\n",
        "  val_errs = []\n",
        "\n",
        "  for i in range(k):\n",
        "    print('Processing fold {0}'.format(i))\n",
        "\n",
        "    # prepare train and val data\n",
        "    val_data = train_df.iloc[i*num_samples: (i+1)*num_samples]\n",
        "    train_data = pd.concat([train_df.iloc[:i*num_samples], \n",
        "                            train_df.iloc[(i+1)*num_samples:]])\n",
        "    (train_X, train_y), (val_X, val_y), _ = prep(train_data, val_data, test_df)\n",
        "\n",
        "    # build a new model\n",
        "    model = build_model(np.shape(train_X)[1])\n",
        "    train_mae, val_mae = train(model, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    # append train/val errs to error lists\n",
        "    train_errs.append(train_mae)\n",
        "    val_errs.append(val_mae)\n",
        "\n",
        "  return train_errs, val_errs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv198aigg3j_",
        "colab_type": "text"
      },
      "source": [
        "# **Load and pre-process data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4_fmMipXFTR",
        "colab_type": "code",
        "outputId": "b3a92a42-ef96-4dc0-c74e-5af482c7b185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# load the data\n",
        "url = '/content/drive/My Drive/pp-complete.csv'\n",
        "df = load_data(url)\n",
        "train_df, test_df = split_train_test(df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Data shape:  (24852949, 5)\n",
            "Train (and val) shape:  (317952, 4)\n",
            "Test shape:  (144006, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRVat5dUv0yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_X, train_y), (test_X, test_y), _ = prep(train_df, test_df, test_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLGCYZYEhD64",
        "colab_type": "text"
      },
      "source": [
        "# **Use GridSearch to find the best set of hyper-parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks-5BZlUv07N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c913930d-fdd8-4917-e8da-56377daa2565"
      },
      "source": [
        "model = KerasRegressor(build_fn=build_model, epochs=10, batch_size=512, verbose=0)\n",
        "\n",
        "l_rates = [0.001, 0.01]\n",
        "dout_rates = [0.1, 0.5]\n",
        "reg_rates = [0.001, 0.005, 0.01]\n",
        "num_units=[60, 80, 100]\n",
        "\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "param_grid=dict(l_rate=l_rates,\n",
        "                dout_rate=dout_rates, \n",
        "                reg_rate=reg_rates,\n",
        "                num_units=num_units)\n",
        "\n",
        "grid = GridSearchCV(estimator=model,\n",
        "                    param_grid = param_grid,\n",
        "                    cv=kf, scoring='neg_mean_squared_error')\n",
        "\n",
        "grid.fit(train_X, train_y)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
        "means = grid.cv_results_['mean_test_score']\n",
        "stds = grid.cv_results_['std_test_score']\n",
        "params = grid.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Best: -1.216376 using {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.01}\n",
            "-1.690654 (0.573621) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 60, 'reg_rate': 0.001}\n",
            "-1.628682 (0.643845) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 60, 'reg_rate': 0.005}\n",
            "-1.519647 (0.599978) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 60, 'reg_rate': 0.01}\n",
            "-1.624866 (0.675319) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 80, 'reg_rate': 0.001}\n",
            "-1.575711 (0.602384) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 80, 'reg_rate': 0.005}\n",
            "-1.568136 (0.615687) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 80, 'reg_rate': 0.01}\n",
            "-1.669899 (0.624518) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 100, 'reg_rate': 0.001}\n",
            "-1.387787 (0.457219) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 100, 'reg_rate': 0.005}\n",
            "-1.589975 (0.652277) with: {'dout_rate': 0.1, 'l_rate': 0.001, 'num_units': 100, 'reg_rate': 0.01}\n",
            "-1.524885 (0.590637) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.001}\n",
            "-1.525804 (0.575381) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.005}\n",
            "-1.518658 (0.601807) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.01}\n",
            "-1.289455 (0.447731) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 80, 'reg_rate': 0.001}\n",
            "-1.345707 (0.448835) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 80, 'reg_rate': 0.005}\n",
            "-1.635214 (0.673351) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 80, 'reg_rate': 0.01}\n",
            "-1.596944 (0.681228) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 100, 'reg_rate': 0.001}\n",
            "-1.794609 (0.682504) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 100, 'reg_rate': 0.005}\n",
            "-1.422926 (0.503215) with: {'dout_rate': 0.1, 'l_rate': 0.01, 'num_units': 100, 'reg_rate': 0.01}\n",
            "-1.549372 (0.572777) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 60, 'reg_rate': 0.001}\n",
            "-1.540923 (0.525316) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 60, 'reg_rate': 0.005}\n",
            "-1.405470 (0.493250) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 60, 'reg_rate': 0.01}\n",
            "-1.554604 (0.564770) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 80, 'reg_rate': 0.001}\n",
            "-1.543646 (0.548978) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 80, 'reg_rate': 0.005}\n",
            "-1.482768 (0.510121) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 80, 'reg_rate': 0.01}\n",
            "-1.523268 (0.564180) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 100, 'reg_rate': 0.001}\n",
            "-1.650963 (0.615083) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 100, 'reg_rate': 0.005}\n",
            "-1.607498 (0.579302) with: {'dout_rate': 0.5, 'l_rate': 0.001, 'num_units': 100, 'reg_rate': 0.01}\n",
            "-1.750947 (0.656791) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.001}\n",
            "-1.682962 (0.733912) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.005}\n",
            "-1.216376 (0.362328) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 60, 'reg_rate': 0.01}\n",
            "-1.701341 (0.683974) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 80, 'reg_rate': 0.001}\n",
            "-1.243831 (0.376622) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 80, 'reg_rate': 0.005}\n",
            "-1.585705 (0.360949) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 80, 'reg_rate': 0.01}\n",
            "-1.747268 (0.714996) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 100, 'reg_rate': 0.001}\n",
            "-1.422172 (0.476344) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 100, 'reg_rate': 0.005}\n",
            "-1.659457 (0.633585) with: {'dout_rate': 0.5, 'l_rate': 0.01, 'num_units': 100, 'reg_rate': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kW84t9ThAlq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "ee780d70-8f0d-4b34-dca8-282b48d89f6c"
      },
      "source": [
        "# Implement the best model found by GS\n",
        "model = build_model()\n",
        "model.fit(train_X, train_y, epochs=10, batch_size=512, verbose=1)\n",
        "_, test_mae = model.evaluate(test_X, test_y)\n",
        "print('Test MAE: ', test_mae)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_114\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_114 (InputLayer)       (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_453 (Dense)            (None, 60)                660       \n",
            "_________________________________________________________________\n",
            "dropout_227 (Dropout)        (None, 60)                0         \n",
            "_________________________________________________________________\n",
            "dense_454 (Dense)            (None, 60)                3660      \n",
            "_________________________________________________________________\n",
            "dropout_228 (Dropout)        (None, 60)                0         \n",
            "_________________________________________________________________\n",
            "dense_455 (Dense)            (None, 60)                3660      \n",
            "_________________________________________________________________\n",
            "dense_456 (Dense)            (None, 1)                 61        \n",
            "=================================================================\n",
            "Total params: 8,041\n",
            "Trainable params: 8,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "317952/317952 [==============================] - 13s 42us/step - loss: 1.7817 - mean_absolute_error: 0.9331\n",
            "Epoch 2/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2293 - mean_absolute_error: 0.8245\n",
            "Epoch 3/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2252 - mean_absolute_error: 0.8232\n",
            "Epoch 4/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2190 - mean_absolute_error: 0.8217\n",
            "Epoch 5/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2184 - mean_absolute_error: 0.8213\n",
            "Epoch 6/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2151 - mean_absolute_error: 0.8205\n",
            "Epoch 7/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2129 - mean_absolute_error: 0.8203\n",
            "Epoch 8/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2132 - mean_absolute_error: 0.8199\n",
            "Epoch 9/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2107 - mean_absolute_error: 0.8192\n",
            "Epoch 10/10\n",
            "317952/317952 [==============================] - 4s 14us/step - loss: 1.2084 - mean_absolute_error: 0.8188\n",
            "144006/144006 [==============================] - 18s 126us/step\n",
            "Test MAE:  0.9925624071188328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B3sX2KYhYYU",
        "colab_type": "text"
      },
      "source": [
        "We are still off by £1,691."
      ]
    }
  ]
}